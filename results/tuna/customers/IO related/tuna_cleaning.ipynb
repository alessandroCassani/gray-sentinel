{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8fe03d64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… All I/O datasets loaded successfully!\n",
      "ðŸ“Š Loaded 3 I/O metrics across 5 experiment types\n",
      "\n",
      "Dataset structure:\n",
      "  BlockLatency: ['baseline', 'cpu_stress', 'delay', 'mem_stress', 'net_loss']\n",
      "  ReadBytes: ['baseline', 'cpu_stress', 'delay', 'mem_stress', 'net_loss']\n",
      "  WriteBytes: ['baseline', 'cpu_stress', 'delay', 'mem_stress', 'net_loss']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from scipy import stats as scipy_stats\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import warnings\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load BASELINE datasets\n",
    "df_baseline_block_latency = pd.read_csv(\"baseline/block_count_latency_device.csv\")\n",
    "df_baseline_read_bytes = pd.read_csv(\"baseline/read_bytes.csv\")\n",
    "df_baseline_write_bytes = pd.read_csv(\"baseline/write_bytes.csv\")\n",
    "\n",
    "# Load CPU STRESS datasets\n",
    "df_cpustress_block_latency = pd.read_csv(\"cpu stress/block_count_latency_device.csv\")\n",
    "df_cpustress_read_bytes = pd.read_csv(\"cpu stress/read_bytes.csv\")\n",
    "df_cpustress_write_bytes = pd.read_csv(\"cpu stress/write_bytes.csv\")\n",
    "\n",
    "# Load IO datasets\n",
    "df_IO_block_latency = pd.read_csv(\"IO pressure/block_count_latency_device.csv\")\n",
    "df_IO_read_bytes = pd.read_csv(\"IO pressure/read_bytes.csv\")\n",
    "df_IO_write_bytes = pd.read_csv(\"IO pressure/write_bytes.csv\")\n",
    "\n",
    "# Load MEM STRESS datasets\n",
    "df_memstress_block_latency = pd.read_csv(\"mem stress/block_count_latency_device.csv\")\n",
    "df_memstress_read_bytes = pd.read_csv(\"mem stress/read_bytes.csv\")\n",
    "df_memstress_write_bytes = pd.read_csv(\"mem stress/write_bytes.csv\")\n",
    "\n",
    "# Load NET LOSS datasets\n",
    "df_netloss_block_latency = pd.read_csv(\"net loss/block_count_latency_device.csv\")\n",
    "df_netloss_read_bytes = pd.read_csv(\"net loss/read_bytes.csv\")\n",
    "df_netloss_write_bytes = pd.read_csv(\"net loss/write_bytes.csv\")\n",
    "\n",
    "\n",
    "# Add source labels - BASELINE\n",
    "df_baseline_block_latency[\"source\"] = \"BASELINE\"\n",
    "df_baseline_read_bytes[\"source\"] = \"BASELINE\"\n",
    "df_baseline_write_bytes[\"source\"] = \"BASELINE\"\n",
    "\n",
    "# Add source labels - CPU STRESS\n",
    "df_cpustress_block_latency[\"source\"] = \"CPU_STRESS\"\n",
    "df_cpustress_read_bytes[\"source\"] = \"CPU_STRESS\"\n",
    "df_cpustress_write_bytes[\"source\"] = \"CPU_STRESS\"\n",
    "\n",
    "# Add source labels - DELAY\n",
    "df_IO_block_latency[\"source\"] = \"DELAY\"\n",
    "df_IO_read_bytes[\"source\"] = \"DELAY\"\n",
    "df_IO_write_bytes[\"source\"] = \"DELAY\"\n",
    "\n",
    "# Add source labels - MEM STRESS\n",
    "df_memstress_block_latency[\"source\"] = \"MEM_STRESS\"\n",
    "df_memstress_read_bytes[\"source\"] = \"MEM_STRESS\"\n",
    "df_memstress_write_bytes[\"source\"] = \"MEM_STRESS\"\n",
    "\n",
    "# Add source labels - NET LOSS\n",
    "df_netloss_block_latency[\"source\"] = \"NET_LOSS\"\n",
    "df_netloss_read_bytes[\"source\"] = \"NET_LOSS\"\n",
    "df_netloss_write_bytes[\"source\"] = \"NET_LOSS\"\n",
    "\n",
    "\n",
    "# Convert to datetime - BASELINE\n",
    "df_baseline_block_latency[\"Time\"] = pd.to_datetime(df_baseline_block_latency[\"Time\"])\n",
    "df_baseline_read_bytes[\"Time\"] = pd.to_datetime(df_baseline_read_bytes[\"Time\"])\n",
    "df_baseline_write_bytes[\"Time\"] = pd.to_datetime(df_baseline_write_bytes[\"Time\"])\n",
    "\n",
    "# Convert to datetime - CPU STRESS\n",
    "df_cpustress_block_latency[\"Time\"] = pd.to_datetime(df_cpustress_block_latency[\"Time\"])\n",
    "df_cpustress_read_bytes[\"Time\"] = pd.to_datetime(df_cpustress_read_bytes[\"Time\"])\n",
    "df_cpustress_write_bytes[\"Time\"] = pd.to_datetime(df_cpustress_write_bytes[\"Time\"])\n",
    "\n",
    "# Convert to datetime - IO\n",
    "df_IO_block_latency[\"Time\"] = pd.to_datetime(df_IO_block_latency[\"Time\"])\n",
    "df_IO_read_bytes[\"Time\"] = pd.to_datetime(df_IO_read_bytes[\"Time\"])\n",
    "df_IO_write_bytes[\"Time\"] = pd.to_datetime(df_IO_write_bytes[\"Time\"])\n",
    "\n",
    "# Convert to datetime - MEM STRESS\n",
    "df_memstress_block_latency[\"Time\"] = pd.to_datetime(df_memstress_block_latency[\"Time\"])\n",
    "df_memstress_read_bytes[\"Time\"] = pd.to_datetime(df_memstress_read_bytes[\"Time\"])\n",
    "df_memstress_write_bytes[\"Time\"] = pd.to_datetime(df_memstress_write_bytes[\"Time\"])\n",
    "\n",
    "# Convert to datetime - NET LOSS\n",
    "df_netloss_block_latency[\"Time\"] = pd.to_datetime(df_netloss_block_latency[\"Time\"])\n",
    "df_netloss_read_bytes[\"Time\"] = pd.to_datetime(df_netloss_read_bytes[\"Time\"])\n",
    "df_netloss_write_bytes[\"Time\"] = pd.to_datetime(df_netloss_write_bytes[\"Time\"])\n",
    "\n",
    "\n",
    "delay = 30\n",
    "duration = 50\n",
    "\n",
    "# Synchronize all datasets with baseline timeline\n",
    "time_offset = df_baseline_block_latency[\"Time\"].min()\n",
    "\n",
    "# Synchronize CPU STRESS datasets\n",
    "cpustress_offset = time_offset - df_cpustress_block_latency[\"Time\"].min()\n",
    "df_cpustress_block_latency[\"Time\"] += cpustress_offset\n",
    "df_cpustress_read_bytes[\"Time\"] += cpustress_offset\n",
    "df_cpustress_write_bytes[\"Time\"] += cpustress_offset\n",
    "\n",
    "# Synchronize DELAY datasets\n",
    "delay_offset = time_offset - df_IO_block_latency[\"Time\"].min()\n",
    "df_IO_block_latency[\"Time\"] += delay_offset\n",
    "df_IO_read_bytes[\"Time\"] += delay_offset\n",
    "df_IO_write_bytes[\"Time\"] += delay_offset\n",
    "\n",
    "# Synchronize MEM STRESS datasets\n",
    "memstress_offset = time_offset - df_memstress_block_latency[\"Time\"].min()\n",
    "df_memstress_block_latency[\"Time\"] += memstress_offset\n",
    "df_memstress_read_bytes[\"Time\"] += memstress_offset\n",
    "df_memstress_write_bytes[\"Time\"] += memstress_offset\n",
    "\n",
    "# Synchronize NET LOSS datasets\n",
    "netloss_offset = time_offset - df_netloss_block_latency[\"Time\"].min()\n",
    "df_netloss_block_latency[\"Time\"] += netloss_offset\n",
    "df_netloss_read_bytes[\"Time\"] += netloss_offset\n",
    "df_netloss_write_bytes[\"Time\"] += netloss_offset\n",
    "\n",
    "\n",
    "# Convert timeline to minutes for ALL datasets\n",
    "all_dfs = [\n",
    "    # Baseline\n",
    "    df_baseline_block_latency, df_baseline_read_bytes, df_baseline_write_bytes,\n",
    "    # CPU Stress\n",
    "    df_cpustress_block_latency, df_cpustress_read_bytes, df_cpustress_write_bytes,\n",
    "    # Delay\n",
    "    df_IO_block_latency, df_IO_read_bytes, df_IO_write_bytes,\n",
    "    # Memory Stress\n",
    "    df_memstress_block_latency, df_memstress_read_bytes, df_memstress_write_bytes,\n",
    "    # Network Loss\n",
    "    df_netloss_block_latency, df_netloss_read_bytes, df_netloss_write_bytes\n",
    "]\n",
    "\n",
    "for df in all_dfs:\n",
    "    df[\"Minutes\"] = (df[\"Time\"] - df[\"Time\"].min()).dt.total_seconds() / 60\n",
    "\n",
    "# COMPLETE DATASETS DICTIONARY \n",
    "all_datasets = {\n",
    "    'BlockLatency': {\n",
    "        'baseline': df_baseline_block_latency,\n",
    "        'cpu_stress': df_cpustress_block_latency,\n",
    "        'delay': df_IO_block_latency,\n",
    "        'mem_stress': df_memstress_block_latency,\n",
    "        'net_loss': df_netloss_block_latency,\n",
    "    },\n",
    "    'ReadBytes': {\n",
    "        'baseline': df_baseline_read_bytes,\n",
    "        'cpu_stress': df_cpustress_read_bytes,\n",
    "        'delay': df_IO_read_bytes,\n",
    "        'mem_stress': df_memstress_read_bytes,\n",
    "        'net_loss': df_netloss_read_bytes,\n",
    "    },\n",
    "    'WriteBytes': {\n",
    "        'baseline': df_baseline_write_bytes,\n",
    "        'cpu_stress': df_cpustress_write_bytes,\n",
    "        'delay': df_IO_write_bytes,\n",
    "        'mem_stress': df_memstress_write_bytes,\n",
    "        'net_loss': df_netloss_write_bytes,\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"âœ… All I/O datasets loaded successfully!\")\n",
    "print(f\"ðŸ“Š Loaded {len(all_datasets)} I/O metrics across {len(all_datasets['BlockLatency'])} experiment types\")\n",
    "print(\"\\nDataset structure:\")\n",
    "for metric, experiments in all_datasets.items():\n",
    "    print(f\"  {metric}: {list(experiments.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48e4110",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_disk_values(df, metric_name):\n",
    "    \"\"\"Extract disk values - single column for DiskUtil, sum for ReadBytes/WriteBytes\"\"\"\n",
    "    exclude_cols = ['Time', 'Minutes', 'source']\n",
    "    value_cols = [col for col in df.columns if col not in exclude_cols]\n",
    "    \n",
    "    if metric_name == 'DiskUtil':\n",
    "        # For disk utilization, we typically have a single disk column\n",
    "        if len(value_cols) == 1:\n",
    "            return df[value_cols[0]].values\n",
    "        else:\n",
    "            # If multiple disks, take the primary one or sum (depending on use case)\n",
    "            return df[value_cols[0]].values  # Take first disk as primary\n",
    "    else:\n",
    "        # For ReadBytes/WriteBytes, sum across all partitions/devices\n",
    "        return df[value_cols].sum(axis=1).values\n",
    "\n",
    "def detect_outliers_tuna(timeseries, window_size=3, threshold=0.65, min_absolute_range=None):\n",
    "    \"\"\"\n",
    "    TUNA's relative range outlier detection with stability improvements\n",
    "    Formula: (max - min) / mean > threshold\n",
    "    Mark only the max/min values in unstable windows as outliers\n",
    "    \"\"\"\n",
    "    outlier_mask = np.zeros(len(timeseries), dtype=bool)\n",
    "    \n",
    "    # Calculate adaptive minimum absolute range if not provided\n",
    "    if min_absolute_range is None:\n",
    "        # Use 5% of the overall time series standard deviation as minimum range\n",
    "        min_absolute_range = 0.1 * np.std(timeseries)\n",
    "    \n",
    "    # Slide window across time series to detect unstable periods\n",
    "    for i in range(len(timeseries) - window_size + 1):\n",
    "        window = timeseries[i:i + window_size]\n",
    "        window_mean = np.mean(window)\n",
    "        window_range = np.max(window) - np.min(window)\n",
    "        \n",
    "        if window_mean > 0:\n",
    "            # Apply TUNA relative range criterion\n",
    "            relative_range = window_range / window_mean\n",
    "            \n",
    "            # Additional filter: require minimum absolute range to avoid hypersensitivity\n",
    "            if relative_range > threshold and window_range > min_absolute_range:\n",
    "                window_max = np.max(window)\n",
    "                window_min = np.min(window)\n",
    "                \n",
    "                # Mark extreme values within unstable window\n",
    "                for j in range(window_size):\n",
    "                    actual_idx = i + j\n",
    "                    if (timeseries[actual_idx] == window_max or \n",
    "                        timeseries[actual_idx] == window_min):\n",
    "                        outlier_mask[actual_idx] = True\n",
    "    \n",
    "    return outlier_mask\n",
    "\n",
    "def create_features_for_ml(timeseries, experiment_type, window_size=10):\n",
    "    \"\"\"Create features for RandomForest training\"\"\"\n",
    "    features = []\n",
    "    \n",
    "    # Extract temporal and statistical features from sliding windows\n",
    "    for i in range(window_size, len(timeseries)):\n",
    "        window = timeseries[i-window_size:i]\n",
    "        \n",
    "        # Statistical features from recent history\n",
    "        feature_vector = [\n",
    "            np.mean(window),                    # Rolling average\n",
    "            np.std(window),                     # Variability measure\n",
    "            np.median(window),                  # Robust central tendency\n",
    "            np.max(window) - np.min(window),    # Range indicator\n",
    "            timeseries[i-1],                    # Previous value dependency\n",
    "            i / len(timeseries),                # Temporal position\n",
    "        ]\n",
    "        \n",
    "        # Experiment context encoding for cross-experiment learning\n",
    "        exp_features = [0, 0, 0, 0, 0]\n",
    "        if experiment_type == \"baseline\":\n",
    "            exp_features[0] = 1\n",
    "        elif experiment_type == \"cpu_stress\":\n",
    "            exp_features[1] = 1\n",
    "        elif experiment_type == \"delay\":\n",
    "            exp_features[2] = 1\n",
    "        elif experiment_type == \"mem_stress\":\n",
    "            exp_features[3] = 1\n",
    "        elif experiment_type == \"net_loss\":\n",
    "            exp_features[4] = 1\n",
    "        \n",
    "        feature_vector.extend(exp_features)\n",
    "        features.append(feature_vector)\n",
    "    \n",
    "    return np.array(features)\n",
    "\n",
    "def train_random_forest_for_metric(metric_name, all_experiments):\n",
    "    \"\"\"Train RandomForest on stable periods across all experiments\"\"\"\n",
    "    print(f\"Training {metric_name}...\")\n",
    "    \n",
    "    X_stable_all = []\n",
    "    y_stable_all = []\n",
    "    \n",
    "    # Aggregate stable training data from all experimental conditions\n",
    "    for exp_name, df in all_experiments.items():\n",
    "        disk_values = extract_disk_values(df, metric_name)\n",
    "        outlier_mask = detect_outliers_tuna(disk_values)\n",
    "        stable_mask = ~outlier_mask\n",
    "        \n",
    "        features = create_features_for_ml(disk_values, exp_name)\n",
    "        # Align feature window with stability detection\n",
    "        stable_features_mask = stable_mask[10:]\n",
    "        stable_features = features[stable_features_mask]\n",
    "        stable_targets = disk_values[10:][stable_features_mask]\n",
    "        \n",
    "        if len(stable_features) > 0:\n",
    "            # Apply local smoothing to targets for better generalization\n",
    "            smoothed_targets = []\n",
    "            for j, target in enumerate(stable_targets):\n",
    "                start_idx = max(0, j-2)\n",
    "                end_idx = min(len(stable_targets), j+3)\n",
    "                local_values = stable_targets[start_idx:end_idx]\n",
    "                smoothed_targets.append(np.median(local_values))\n",
    "            \n",
    "            X_stable_all.extend(stable_features)\n",
    "            y_stable_all.extend(smoothed_targets)\n",
    "    \n",
    "    # Ensure sufficient training data\n",
    "    if len(X_stable_all) < 10:\n",
    "        return None, None\n",
    "    \n",
    "    X_stable_all = np.array(X_stable_all)\n",
    "    y_stable_all = np.array(y_stable_all)\n",
    "    \n",
    "    # Standardize features for optimal model performance\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X_stable_all)\n",
    "\n",
    "    # Hyperparameter grid for model optimization\n",
    "    param_grid = {\n",
    "        'n_estimators': [100],\n",
    "        'max_depth': [5, 10, 15],\n",
    "        'min_samples_split': [2, 5],\n",
    "        'min_samples_leaf': [1, 2, 4],\n",
    "        'max_features': ['sqrt', 'log2']\n",
    "    }\n",
    "    \n",
    "    model = RandomForestRegressor(\n",
    "        criterion='squared_error',\n",
    "        bootstrap=True,\n",
    "        n_jobs=-1,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # Grid search with cross-validation for robust parameter selection\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=model,\n",
    "        param_grid=param_grid,\n",
    "        scoring='neg_mean_squared_error',\n",
    "        cv=3,\n",
    "        n_jobs=-1,\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    grid_search.fit(X_scaled, y_stable_all)\n",
    "    model = grid_search.best_estimator_\n",
    "    \n",
    "    return model, scaler\n",
    "\n",
    "def apply_penalty(timeseries, outlier_mask, penalty_factor=0.5):\n",
    "    \"\"\"Apply penalty to outliers by reducing their deviation from baseline\"\"\"\n",
    "    cleaned_series = timeseries.copy()\n",
    "    \n",
    "    # Check if this looks like WriteBytes/ReadBytes data (high variance, burst patterns)\n",
    "    is_burst_metric = (np.max(timeseries) > 10 * np.median(timeseries)) and (len(np.unique(timeseries)) > 20)\n",
    "    \n",
    "    if is_burst_metric:\n",
    "        # For burst-type metrics like WriteBytes: use state-aware baseline\n",
    "        # Separate low-activity and high-activity periods\n",
    "        low_threshold = np.percentile(timeseries, 40)  # 40th percentile as threshold\n",
    "        \n",
    "        for i in range(len(timeseries)):\n",
    "            if outlier_mask[i]:\n",
    "                original_value = timeseries[i]\n",
    "                \n",
    "                # Determine baseline based on local activity level\n",
    "                if original_value <= low_threshold:\n",
    "                    # Low activity region - use low activity baseline\n",
    "                    low_values = timeseries[timeseries <= low_threshold]\n",
    "                    baseline = np.median(low_values)\n",
    "                else:\n",
    "                    # High activity region - use high activity baseline  \n",
    "                    high_values = timeseries[timeseries > low_threshold]\n",
    "                    baseline = np.median(high_values)\n",
    "                \n",
    "                # Apply gentler penalty (reduce penalty factor)\n",
    "                gentle_penalty = penalty_factor * 0.3  # Much gentler for burst metrics\n",
    "                cleaned_series[i] = baseline + (original_value - baseline) * gentle_penalty\n",
    "    else:\n",
    "        # Original TUNA logic for non-burst metrics\n",
    "        stable_values = timeseries[~outlier_mask]\n",
    "        if len(stable_values) > 0:\n",
    "            baseline = np.median(stable_values)\n",
    "        else:\n",
    "            baseline = np.median(timeseries)\n",
    "        \n",
    "        # Reduce outlier magnitude toward baseline\n",
    "        for i in range(len(timeseries)):\n",
    "            if outlier_mask[i]:\n",
    "                original_value = timeseries[i]\n",
    "                # Linear penalty: reduce distance from baseline by factor\n",
    "                cleaned_series[i] = baseline + (original_value - baseline) * penalty_factor\n",
    "    \n",
    "    return cleaned_series\n",
    "\n",
    "def apply_tuna_to_single_series(disk_values, exp_name, model, scaler, penalty_factor=0.5, blend_weight=1.0, metric_name=\"\"):\n",
    "    \"\"\"Apply TUNA cleaning to a single disk time series\"\"\"\n",
    "    \n",
    "    # Phase 1: Identify unstable measurements\n",
    "    outlier_mask = detect_outliers_tuna(disk_values)\n",
    "    stable_mask = ~outlier_mask\n",
    "    outliers_count = np.sum(outlier_mask)\n",
    "    \n",
    "    # Phase 2: Apply penalty to unstable measurements\n",
    "    cleaned_series = apply_penalty(disk_values, outlier_mask, penalty_factor)\n",
    "    \n",
    "    # Phase 3: Smart ML application based on data characteristics\n",
    "    unique_values = len(np.unique(disk_values))\n",
    "    data_range = np.max(disk_values) - np.min(disk_values)\n",
    "    \n",
    "    # Check if data is too discrete/limited for ML enhancement\n",
    "    is_discrete_limited = unique_values <= 5 and data_range <= 5\n",
    "    \n",
    "    if model is not None and scaler is not None and not is_discrete_limited:\n",
    "        # Apply 100% ML for continuous/high-variance data\n",
    "        features = create_features_for_ml(disk_values, exp_name)\n",
    "        features_scaled = scaler.transform(features)\n",
    "        ml_predictions = model.predict(features_scaled)\n",
    "        \n",
    "        # Replace stable periods with 100% ML predictions\n",
    "        for i, prediction in enumerate(ml_predictions):\n",
    "            actual_idx = i + 10  # Account for feature window offset\n",
    "            if actual_idx < len(cleaned_series) and stable_mask[actual_idx]:\n",
    "                cleaned_series[actual_idx] = prediction\n",
    "                \n",
    "        print(f\"  Applied 100% ML enhancement for {metric_name} - {exp_name}\")\n",
    "    else:\n",
    "        # For discrete/limited data, keep penalty-only approach\n",
    "        print(f\"  Skipped ML enhancement for {metric_name} - {exp_name} (discrete data: {unique_values} unique values)\")\n",
    "    \n",
    "    # Compute improvement metrics\n",
    "    original_std = np.std(disk_values)\n",
    "    cleaned_std = np.std(cleaned_series)\n",
    "    noise_reduction = (original_std - cleaned_std) / original_std * 100 if original_std > 0 else 0\n",
    "    correlation = np.corrcoef(disk_values, cleaned_series)[0, 1] if len(disk_values) > 1 else 1.0\n",
    "    \n",
    "    return cleaned_series, outlier_mask, {\n",
    "        'outliers': outliers_count,\n",
    "        'noise_reduction': noise_reduction,\n",
    "        'correlation': correlation\n",
    "    }\n",
    "\n",
    "def run_tuna_for_all_metrics():\n",
    "    \"\"\"Run TUNA for all disk metrics\"\"\"\n",
    "    print(\"Running TUNA analysis on disk I/O datasets...\")\n",
    "    \n",
    "    tuna_results = {}\n",
    "    \n",
    "    # Process each disk metric independently\n",
    "    for metric_name, experiments in all_datasets.items():\n",
    "        print(f\"Processing {metric_name}\")\n",
    "        \n",
    "        # Train metric-specific model on cross-experiment stable data\n",
    "        model, scaler = train_random_forest_for_metric(metric_name, experiments)\n",
    "        \n",
    "        # Apply cleaning to each experimental condition\n",
    "        metric_results = {}\n",
    "        for exp_name, df in experiments.items():\n",
    "            disk_values = extract_disk_values(df, metric_name)\n",
    "            cleaned_series, outlier_mask, stats = apply_tuna_to_single_series(\n",
    "                disk_values, exp_name, model, scaler, metric_name=metric_name\n",
    "            )\n",
    "            \n",
    "            metric_results[exp_name] = {\n",
    "                'original': disk_values,\n",
    "                'cleaned': cleaned_series,\n",
    "                'outliers': outlier_mask,\n",
    "                'stats': stats\n",
    "            }\n",
    "        \n",
    "        tuna_results[metric_name] = metric_results\n",
    "    \n",
    "    return tuna_results\n",
    "\n",
    "def plot_tuna_results(tuna_results):\n",
    "    \"\"\"Plot TUNA results with original vs cleaned comparison using Minutes (0-120) x-axis\"\"\"\n",
    "    \n",
    "    for metric_name, metric_results in tuna_results.items():\n",
    "        n_experiments = len(metric_results)\n",
    "        fig, axes = plt.subplots(n_experiments, 1, figsize=(15, 4*n_experiments))\n",
    "        \n",
    "        if n_experiments == 1:\n",
    "            axes = [axes]\n",
    "        \n",
    "        # Visualize cleaning results for each experiment\n",
    "        for i, (exp_name, results) in enumerate(metric_results.items()):\n",
    "            original = results['original']\n",
    "            cleaned = results['cleaned']\n",
    "            outliers = results['outliers']\n",
    "            stats = results['stats']\n",
    "            \n",
    "            # Get the corresponding dataframe to extract Minutes column\n",
    "            df = all_datasets[metric_name][exp_name]\n",
    "            minutes = df['Minutes'].values\n",
    "            \n",
    "            # Plot time series with cleaning overlay using Minutes as x-axis\n",
    "            axes[i].plot(minutes, original, 'b-', alpha=0.7, label='Original', linewidth=1)\n",
    "            axes[i].plot(minutes, cleaned, 'r-', alpha=0.8, label='TUNA Cleaned', linewidth=1.5)\n",
    "            \n",
    "            # Highlight detected outliers\n",
    "            outlier_points = np.where(outliers)[0]\n",
    "            if len(outlier_points) > 0:\n",
    "                axes[i].scatter(minutes[outlier_points], original[outlier_points], \n",
    "                               c='orange', s=20, alpha=0.7, label='Outliers', zorder=5)\n",
    "            \n",
    "            # Set appropriate y-axis label based on metric\n",
    "            if 'DiskUtil' in metric_name:\n",
    "                y_label = 'Disk Utilization (%)'\n",
    "            elif 'ReadBytes' in metric_name:\n",
    "                y_label = 'Total Read Bytes'\n",
    "            elif 'WriteBytes' in metric_name:\n",
    "                y_label = 'Total Write Bytes'\n",
    "            else:\n",
    "                y_label = 'Disk I/O Value'\n",
    "            \n",
    "            axes[i].set_title(f'{metric_name} - {exp_name} (Noise Reduction: {stats[\"noise_reduction\"]:.1f}%)')\n",
    "            axes[i].set_xlabel('Minutes')\n",
    "            axes[i].set_ylabel(y_label)\n",
    "            axes[i].set_xlim(0, 120)  # Set x-axis to match experiment duration (0-120 minutes)\n",
    "            axes[i].legend()\n",
    "            axes[i].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.suptitle(f'TUNA Results: {metric_name}', fontsize=16)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "def print_tuna_summary(tuna_results):\n",
    "    \"\"\"Print summary table of cleaning effectiveness\"\"\"\n",
    "    print(\"\\nDisk I/O TUNA Results Summary:\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"{'Metric':<15} {'Experiment':<12} {'Outliers':<10} {'Noise Red%':<12} {'Correlation':<12}\")\n",
    "    print(\"-\"*60)\n",
    "    \n",
    "    # Tabulate results across all metrics and experiments\n",
    "    for metric_name, metric_results in tuna_results.items():\n",
    "        for exp_name, results in metric_results.items():\n",
    "            stats = results['stats']\n",
    "            print(f\"{metric_name:<15} {exp_name:<12} {stats['outliers']:<10} \"\n",
    "                  f\"{stats['noise_reduction']:>10.1f}% {stats['correlation']:>11.3f}\")\n",
    "\n",
    "# Execute complete TUNA analysis pipeline\n",
    "tuna_results = run_tuna_for_all_metrics()\n",
    "plot_tuna_results(tuna_results)\n",
    "print_tuna_summary(tuna_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

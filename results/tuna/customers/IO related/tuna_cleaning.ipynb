{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8fe03d64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… All I/O datasets loaded successfully!\n",
      "ðŸ“Š Loaded 3 I/O metrics across 5 experiment types\n",
      "\n",
      "Dataset structure:\n",
      "  BlockLatency: ['baseline', 'cpu_stress', 'delay', 'mem_stress', 'net_loss']\n",
      "  ReadBytes: ['baseline', 'cpu_stress', 'delay', 'mem_stress', 'net_loss']\n",
      "  WriteBytes: ['baseline', 'cpu_stress', 'delay', 'mem_stress', 'net_loss']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from scipy import stats as scipy_stats\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import warnings\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load BASELINE datasets\n",
    "df_baseline_block_latency = pd.read_csv(\"baseline/block_count_latency_device.csv\")\n",
    "df_baseline_read_bytes = pd.read_csv(\"baseline/read_bytes.csv\")\n",
    "df_baseline_write_bytes = pd.read_csv(\"baseline/write_bytes.csv\")\n",
    "\n",
    "# Load CPU STRESS datasets\n",
    "df_cpustress_block_latency = pd.read_csv(\"cpu stress/block_count_latency_device.csv\")\n",
    "df_cpustress_read_bytes = pd.read_csv(\"cpu stress/read_bytes.csv\")\n",
    "df_cpustress_write_bytes = pd.read_csv(\"cpu stress/write_bytes.csv\")\n",
    "\n",
    "# Load IO datasets\n",
    "df_IO_block_latency = pd.read_csv(\"IO pressure/block_count_latency_device.csv\")\n",
    "df_IO_read_bytes = pd.read_csv(\"IO pressure/read_bytes.csv\")\n",
    "df_IO_write_bytes = pd.read_csv(\"IO pressure/write_bytes.csv\")\n",
    "\n",
    "# Load MEM STRESS datasets\n",
    "df_memstress_block_latency = pd.read_csv(\"mem stress/block_count_latency_device.csv\")\n",
    "df_memstress_read_bytes = pd.read_csv(\"mem stress/read_bytes.csv\")\n",
    "df_memstress_write_bytes = pd.read_csv(\"mem stress/write_bytes.csv\")\n",
    "\n",
    "# Load NET LOSS datasets\n",
    "df_netloss_block_latency = pd.read_csv(\"net loss/block_count_latency_device.csv\")\n",
    "df_netloss_read_bytes = pd.read_csv(\"net loss/read_bytes.csv\")\n",
    "df_netloss_write_bytes = pd.read_csv(\"net loss/write_bytes.csv\")\n",
    "\n",
    "\n",
    "# Add source labels - BASELINE\n",
    "df_baseline_block_latency[\"source\"] = \"BASELINE\"\n",
    "df_baseline_read_bytes[\"source\"] = \"BASELINE\"\n",
    "df_baseline_write_bytes[\"source\"] = \"BASELINE\"\n",
    "\n",
    "# Add source labels - CPU STRESS\n",
    "df_cpustress_block_latency[\"source\"] = \"CPU_STRESS\"\n",
    "df_cpustress_read_bytes[\"source\"] = \"CPU_STRESS\"\n",
    "df_cpustress_write_bytes[\"source\"] = \"CPU_STRESS\"\n",
    "\n",
    "# Add source labels - DELAY\n",
    "df_IO_block_latency[\"source\"] = \"DELAY\"\n",
    "df_IO_read_bytes[\"source\"] = \"DELAY\"\n",
    "df_IO_write_bytes[\"source\"] = \"DELAY\"\n",
    "\n",
    "# Add source labels - MEM STRESS\n",
    "df_memstress_block_latency[\"source\"] = \"MEM_STRESS\"\n",
    "df_memstress_read_bytes[\"source\"] = \"MEM_STRESS\"\n",
    "df_memstress_write_bytes[\"source\"] = \"MEM_STRESS\"\n",
    "\n",
    "# Add source labels - NET LOSS\n",
    "df_netloss_block_latency[\"source\"] = \"NET_LOSS\"\n",
    "df_netloss_read_bytes[\"source\"] = \"NET_LOSS\"\n",
    "df_netloss_write_bytes[\"source\"] = \"NET_LOSS\"\n",
    "\n",
    "\n",
    "# Convert to datetime - BASELINE\n",
    "df_baseline_block_latency[\"Time\"] = pd.to_datetime(df_baseline_block_latency[\"Time\"])\n",
    "df_baseline_read_bytes[\"Time\"] = pd.to_datetime(df_baseline_read_bytes[\"Time\"])\n",
    "df_baseline_write_bytes[\"Time\"] = pd.to_datetime(df_baseline_write_bytes[\"Time\"])\n",
    "\n",
    "# Convert to datetime - CPU STRESS\n",
    "df_cpustress_block_latency[\"Time\"] = pd.to_datetime(df_cpustress_block_latency[\"Time\"])\n",
    "df_cpustress_read_bytes[\"Time\"] = pd.to_datetime(df_cpustress_read_bytes[\"Time\"])\n",
    "df_cpustress_write_bytes[\"Time\"] = pd.to_datetime(df_cpustress_write_bytes[\"Time\"])\n",
    "\n",
    "# Convert to datetime - IO\n",
    "df_IO_block_latency[\"Time\"] = pd.to_datetime(df_IO_block_latency[\"Time\"])\n",
    "df_IO_read_bytes[\"Time\"] = pd.to_datetime(df_IO_read_bytes[\"Time\"])\n",
    "df_IO_write_bytes[\"Time\"] = pd.to_datetime(df_IO_write_bytes[\"Time\"])\n",
    "\n",
    "# Convert to datetime - MEM STRESS\n",
    "df_memstress_block_latency[\"Time\"] = pd.to_datetime(df_memstress_block_latency[\"Time\"])\n",
    "df_memstress_read_bytes[\"Time\"] = pd.to_datetime(df_memstress_read_bytes[\"Time\"])\n",
    "df_memstress_write_bytes[\"Time\"] = pd.to_datetime(df_memstress_write_bytes[\"Time\"])\n",
    "\n",
    "# Convert to datetime - NET LOSS\n",
    "df_netloss_block_latency[\"Time\"] = pd.to_datetime(df_netloss_block_latency[\"Time\"])\n",
    "df_netloss_read_bytes[\"Time\"] = pd.to_datetime(df_netloss_read_bytes[\"Time\"])\n",
    "df_netloss_write_bytes[\"Time\"] = pd.to_datetime(df_netloss_write_bytes[\"Time\"])\n",
    "\n",
    "\n",
    "delay = 30\n",
    "duration = 50\n",
    "\n",
    "# Synchronize all datasets with baseline timeline\n",
    "time_offset = df_baseline_block_latency[\"Time\"].min()\n",
    "\n",
    "# Synchronize CPU STRESS datasets\n",
    "cpustress_offset = time_offset - df_cpustress_block_latency[\"Time\"].min()\n",
    "df_cpustress_block_latency[\"Time\"] += cpustress_offset\n",
    "df_cpustress_read_bytes[\"Time\"] += cpustress_offset\n",
    "df_cpustress_write_bytes[\"Time\"] += cpustress_offset\n",
    "\n",
    "# Synchronize DELAY datasets\n",
    "delay_offset = time_offset - df_IO_block_latency[\"Time\"].min()\n",
    "df_IO_block_latency[\"Time\"] += delay_offset\n",
    "df_IO_read_bytes[\"Time\"] += delay_offset\n",
    "df_IO_write_bytes[\"Time\"] += delay_offset\n",
    "\n",
    "# Synchronize MEM STRESS datasets\n",
    "memstress_offset = time_offset - df_memstress_block_latency[\"Time\"].min()\n",
    "df_memstress_block_latency[\"Time\"] += memstress_offset\n",
    "df_memstress_read_bytes[\"Time\"] += memstress_offset\n",
    "df_memstress_write_bytes[\"Time\"] += memstress_offset\n",
    "\n",
    "# Synchronize NET LOSS datasets\n",
    "netloss_offset = time_offset - df_netloss_block_latency[\"Time\"].min()\n",
    "df_netloss_block_latency[\"Time\"] += netloss_offset\n",
    "df_netloss_read_bytes[\"Time\"] += netloss_offset\n",
    "df_netloss_write_bytes[\"Time\"] += netloss_offset\n",
    "\n",
    "\n",
    "# Convert timeline to minutes for ALL datasets\n",
    "all_dfs = [\n",
    "    # Baseline\n",
    "    df_baseline_block_latency, df_baseline_read_bytes, df_baseline_write_bytes,\n",
    "    # CPU Stress\n",
    "    df_cpustress_block_latency, df_cpustress_read_bytes, df_cpustress_write_bytes,\n",
    "    # Delay\n",
    "    df_IO_block_latency, df_IO_read_bytes, df_IO_write_bytes,\n",
    "    # Memory Stress\n",
    "    df_memstress_block_latency, df_memstress_read_bytes, df_memstress_write_bytes,\n",
    "    # Network Loss\n",
    "    df_netloss_block_latency, df_netloss_read_bytes, df_netloss_write_bytes\n",
    "]\n",
    "\n",
    "for df in all_dfs:\n",
    "    df[\"Minutes\"] = (df[\"Time\"] - df[\"Time\"].min()).dt.total_seconds() / 60\n",
    "\n",
    "# COMPLETE DATASETS DICTIONARY \n",
    "all_datasets = {\n",
    "    'BlockLatency': {\n",
    "        'baseline': df_baseline_block_latency,\n",
    "        'cpu_stress': df_cpustress_block_latency,\n",
    "        'delay': df_IO_block_latency,\n",
    "        'mem_stress': df_memstress_block_latency,\n",
    "        'net_loss': df_netloss_block_latency,\n",
    "    },\n",
    "    'ReadBytes': {\n",
    "        'baseline': df_baseline_read_bytes,\n",
    "        'cpu_stress': df_cpustress_read_bytes,\n",
    "        'delay': df_IO_read_bytes,\n",
    "        'mem_stress': df_memstress_read_bytes,\n",
    "        'net_loss': df_netloss_read_bytes,\n",
    "    },\n",
    "    'WriteBytes': {\n",
    "        'baseline': df_baseline_write_bytes,\n",
    "        'cpu_stress': df_cpustress_write_bytes,\n",
    "        'delay': df_IO_write_bytes,\n",
    "        'mem_stress': df_memstress_write_bytes,\n",
    "        'net_loss': df_netloss_write_bytes,\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"âœ… All I/O datasets loaded successfully!\")\n",
    "print(f\"ðŸ“Š Loaded {len(all_datasets)} I/O metrics across {len(all_datasets['BlockLatency'])} experiment types\")\n",
    "print(\"\\nDataset structure:\")\n",
    "for metric, experiments in all_datasets.items():\n",
    "    print(f\"  {metric}: {list(experiments.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48e4110",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running TUNA analysis on TCP datasets...\n",
      "Processing BlockLatency\n",
      "Training BlockLatency...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "create_features_for_ml() missing 1 required positional argument: 'experiment_type'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[32]\u001b[39m\u001b[32m, line 378\u001b[39m\n\u001b[32m    374\u001b[39m             \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_name\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m<15\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexp_name\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m<12\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstats[\u001b[33m'\u001b[39m\u001b[33moutliers\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m<10\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    375\u001b[39m                   \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstats[\u001b[33m'\u001b[39m\u001b[33mnoise_reduction\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m>10.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m% \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstats[\u001b[33m'\u001b[39m\u001b[33mcorrelation\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m>11.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    377\u001b[39m \u001b[38;5;66;03m# Execute complete TUNA analysis pipeline\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m378\u001b[39m tuna_results = \u001b[43mrun_tuna_for_all_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    379\u001b[39m plot_tuna_results(tuna_results)\n\u001b[32m    380\u001b[39m print_tuna_summary(tuna_results)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[32]\u001b[39m\u001b[32m, line 292\u001b[39m, in \u001b[36mrun_tuna_for_all_metrics\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    289\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mProcessing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    291\u001b[39m \u001b[38;5;66;03m# Train metric-specific model on cross-experiment stable data\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m292\u001b[39m model, scaler = \u001b[43mtrain_random_forest_for_metric\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmetric_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexperiments\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    294\u001b[39m \u001b[38;5;66;03m# Apply cleaning to each experimental condition\u001b[39;00m\n\u001b[32m    295\u001b[39m metric_results = {}\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[32]\u001b[39m\u001b[32m, line 130\u001b[39m, in \u001b[36mtrain_random_forest_for_metric\u001b[39m\u001b[34m(metric_name, all_experiments)\u001b[39m\n\u001b[32m    127\u001b[39m outlier_mask = detect_outliers_tuna(tcp_values)\n\u001b[32m    128\u001b[39m stable_mask = ~outlier_mask\n\u001b[32m--> \u001b[39m\u001b[32m130\u001b[39m features = \u001b[43mcreate_features_for_ml\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtcp_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexp_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    131\u001b[39m \u001b[38;5;66;03m# Align feature window with stability detection\u001b[39;00m\n\u001b[32m    132\u001b[39m stable_features_mask = stable_mask[\u001b[32m10\u001b[39m:]\n",
      "\u001b[31mTypeError\u001b[39m: create_features_for_ml() missing 1 required positional argument: 'experiment_type'"
     ]
    }
   ],
   "source": [
    "def extract_disk_values(df, metric_name):\n",
    "    \"\"\"Extract disk values - single column for DiskUtil, sum for ReadBytes/WriteBytes\"\"\"\n",
    "    exclude_cols = ['Time', 'Minutes', 'source']\n",
    "    value_cols = [col for col in df.columns if col not in exclude_cols]\n",
    "    \n",
    "    if metric_name == 'DiskUtil':\n",
    "        # For disk utilization, we typically have a single disk column\n",
    "        if len(value_cols) == 1:\n",
    "            return df[value_cols[0]].values\n",
    "        else:\n",
    "            # If multiple disks, take the primary one or sum (depending on use case)\n",
    "            return df[value_cols[0]].values  # Take first disk as primary\n",
    "    else:\n",
    "        # For ReadBytes/WriteBytes, sum across all partitions/devices\n",
    "        return df[value_cols].sum(axis=1).values\n",
    "\n",
    "def detect_outliers_tuna(timeseries, window_size=5, threshold=0.65, min_absolute_range=None):\n",
    "    \"\"\"\n",
    "    TUNA's relative range outlier detection with stability improvements\n",
    "    Formula: (max - min) / mean > threshold\n",
    "    Mark all values in unstable windows as outliers\n",
    "    \"\"\"\n",
    "    outlier_mask = np.zeros(len(timeseries), dtype=bool)\n",
    "    \n",
    "    # Calculate adaptive minimum absolute range if not provided\n",
    "    if min_absolute_range is None:\n",
    "        # Use 5% of the overall time series standard deviation as minimum range\n",
    "        min_absolute_range = 0.1 * np.std(timeseries)\n",
    "    \n",
    "    # Slide window across time series to detect unstable periods\n",
    "    for i in range(len(timeseries) - window_size + 1):\n",
    "        window = timeseries[i:i + window_size]\n",
    "        window_mean = np.mean(window)\n",
    "        window_range = np.max(window) - np.min(window)\n",
    "        \n",
    "        if window_mean > 0:\n",
    "            # Apply TUNA relative range criterion\n",
    "            relative_range = window_range / window_mean\n",
    "            \n",
    "            # Additional filter: require minimum absolute range to avoid hypersensitivity\n",
    "            if relative_range > threshold and window_range > min_absolute_range:\n",
    "                # Mark all values within unstable window as outliers\n",
    "                for j in range(window_size):\n",
    "                    actual_idx = i + j\n",
    "                    outlier_mask[actual_idx] = True\n",
    "    \n",
    "    return outlier_mask\n",
    "\n",
    "def create_features_for_ml(timeseries, outlier_mask, experiment_type, window_size=10):\n",
    "    \"\"\"Create features for ML training using only stable values in history\"\"\"\n",
    "    features = []\n",
    "    \n",
    "    # Extract temporal and statistical features from sliding windows of STABLE values only\n",
    "    for i in range(window_size, len(timeseries)):\n",
    "        # Get the last window_size values, but only use stable ones\n",
    "        stable_history = []\n",
    "        for j in range(i - window_size, i):\n",
    "            if j >= 0 and not outlier_mask[j]:  # Only include stable values\n",
    "                stable_history.append(timeseries[j])\n",
    "        \n",
    "        # If we don't have enough stable history, skip this prediction\n",
    "        if len(stable_history) < 3:\n",
    "            continue\n",
    "            \n",
    "        # Pad with last stable value if needed\n",
    "        while len(stable_history) < window_size:\n",
    "            stable_history.append(stable_history[-1])\n",
    "        \n",
    "        # Statistical features from stable history only\n",
    "        stable_window = np.array(stable_history)\n",
    "        feature_vector = [\n",
    "            np.mean(stable_window),                    # Rolling average of stable values\n",
    "            np.std(stable_window),                     # Variability of stable values\n",
    "            np.median(stable_window),                  # Robust central tendency\n",
    "            np.max(stable_window) - np.min(stable_window),  # Range of stable values\n",
    "            stable_history[-1],                        # Last stable value\n",
    "            i / len(timeseries),                       # Temporal position\n",
    "        ]\n",
    "        \n",
    "        # Experiment context encoding\n",
    "        exp_features = [0, 0, 0, 0, 0]\n",
    "        if experiment_type == \"baseline\":\n",
    "            exp_features[0] = 1\n",
    "        elif experiment_type == \"cpu_stress\":\n",
    "            exp_features[1] = 1\n",
    "        elif experiment_type == \"delay\":\n",
    "            exp_features[2] = 1\n",
    "        elif experiment_type == \"mem_stress\":\n",
    "            exp_features[3] = 1\n",
    "        elif experiment_type == \"net_loss\":\n",
    "            exp_features[4] = 1\n",
    "        \n",
    "        feature_vector.extend(exp_features)\n",
    "        features.append((i, feature_vector))  # Store index with features\n",
    "    \n",
    "    return features\n",
    "\n",
    "def train_random_forest_for_metric(metric_name, all_experiments):\n",
    "    \"\"\"Train RandomForest on stable periods across all experiments\"\"\"\n",
    "    print(f\"Training {metric_name}...\")\n",
    "    \n",
    "    X_stable_all = []\n",
    "    y_stable_all = []\n",
    "    \n",
    "    # Aggregate stable training data from all experimental conditions\n",
    "    for exp_name, df in all_experiments.items():\n",
    "        tcp_values = extract_tcp_values(df, metric_name)\n",
    "        outlier_mask = detect_outliers_tuna(tcp_values)\n",
    "        stable_mask = ~outlier_mask\n",
    "        \n",
    "        features = create_features_for_ml(tcp_values, exp_name)\n",
    "        # Align feature window with stability detection\n",
    "        stable_features_mask = stable_mask[10:]\n",
    "        stable_features = features[stable_features_mask]\n",
    "        stable_targets = tcp_values[10:][stable_features_mask]\n",
    "        \n",
    "        if len(stable_features) > 0:\n",
    "            # Apply local smoothing to targets for better generalization\n",
    "            smoothed_targets = []\n",
    "            for j, target in enumerate(stable_targets):\n",
    "                start_idx = max(0, j-2)\n",
    "                end_idx = min(len(stable_targets), j+3)\n",
    "                local_values = stable_targets[start_idx:end_idx]\n",
    "                smoothed_targets.append(np.median(local_values))\n",
    "            \n",
    "            X_stable_all.extend(stable_features)\n",
    "            y_stable_all.extend(smoothed_targets)\n",
    "    \n",
    "    # Ensure sufficient training data\n",
    "    if len(X_stable_all) < 10:\n",
    "        return None, None\n",
    "    \n",
    "    X_stable_all = np.array(X_stable_all)\n",
    "    y_stable_all = np.array(y_stable_all)\n",
    "    \n",
    "    # Standardize features for optimal model performance\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X_stable_all)\n",
    "\n",
    "    # Hyperparameter grid for model optimization\n",
    "    param_grid = {\n",
    "        'n_estimators': [100],\n",
    "        'max_depth': [5, 10, 15],\n",
    "        'min_samples_split': [2, 5],\n",
    "        'min_samples_leaf': [1, 2, 4],\n",
    "        'max_features': ['sqrt', 'log2']\n",
    "    }\n",
    "    \n",
    "    model = RandomForestRegressor(\n",
    "        criterion='squared_error',\n",
    "        bootstrap=True,\n",
    "        n_jobs=-1,\n",
    "        random_state=42\n",
    "    )\n",
    "\n",
    "    # Grid search with cross-validation for robust parameter selection\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=model,\n",
    "        param_grid=param_grid,\n",
    "        scoring='neg_mean_squared_error',\n",
    "        cv=3,\n",
    "        n_jobs=-1,\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    grid_search.fit(X_scaled, y_stable_all)\n",
    "    model = grid_search.best_estimator_\n",
    "    \n",
    "    return model, scaler\n",
    "\n",
    "def apply_penalty(timeseries, outlier_mask, penalty_factor=0.75):\n",
    "    \"\"\"Apply penalty to outliers by reducing their magnitude\"\"\"\n",
    "    cleaned_series = timeseries.copy()\n",
    "    \n",
    "    # Apply penalty exactly once to each outlier\n",
    "    outlier_indices = np.where(outlier_mask)[0]\n",
    "    for i in outlier_indices:\n",
    "        original_value = timeseries[i]\n",
    "        if original_value != 0:  # Only apply penalty to non-zero values\n",
    "            # Simple penalty: reduce magnitude by factor\n",
    "            cleaned_series[i] = original_value * penalty_factor\n",
    "        # Zero values remain zero (no penalty needed)\n",
    "    \n",
    "    return cleaned_series\n",
    "\n",
    "def apply_tuna_to_single_series(tcp_values, exp_name, model, scaler, penalty_factor=0.75, blend_weight=1.0, metric_name=\"\"):\n",
    "    \"\"\"Apply TUNA cleaning to a single TCP time series with ML + penalty\"\"\"\n",
    "    \n",
    "    # Check data characteristics for smart ML application\n",
    "    unique_values = len(np.unique(tcp_values))\n",
    "    data_range = np.max(tcp_values) - np.min(tcp_values)\n",
    "    zero_percentage = np.sum(tcp_values == 0) / len(tcp_values) * 100\n",
    "    \n",
    "    # Enhanced discrete data detection for TCP retransmission patterns\n",
    "    is_discrete_data = (\n",
    "        unique_values <= 10 and data_range <= 50  # Low variety and range\n",
    "        or zero_percentage > 60  # High percentage of zeros (common in retrans data)\n",
    "        or (unique_values <= 20 and all(x == int(x) for x in np.unique(tcp_values) if not np.isnan(x)))  # All integers\n",
    "    )\n",
    "    \n",
    "    outlier_mask = detect_outliers_tuna(tcp_values, window_size=5, threshold=0.65)\n",
    "    \n",
    "    stable_mask = ~outlier_mask\n",
    "    outliers_count = np.sum(outlier_mask)\n",
    "    \n",
    "    # Phase 2: Apply penalty to outliers FIRST\n",
    "    cleaned_series = apply_penalty(tcp_values, outlier_mask, penalty_factor)\n",
    "    \n",
    "    # Phase 3: Apply ML enhancement only for appropriate data types\n",
    "    if not is_discrete_data and model is not None and scaler is not None:\n",
    "        try:\n",
    "            # Create features for ML prediction\n",
    "            features = create_features_for_ml(tcp_values, exp_name)\n",
    "            features_scaled = scaler.transform(features)\n",
    "            ml_predictions = model.predict(features_scaled)\n",
    "            \n",
    "            # Apply 100% ML for stable periods only (excluding outliers)\n",
    "            for i, prediction in enumerate(ml_predictions):\n",
    "                actual_idx = i + 10  # Account for feature window offset\n",
    "                if (actual_idx < len(cleaned_series) and \n",
    "                    stable_mask[actual_idx] and \n",
    "                    not outlier_mask[actual_idx]):  # Ensure outliers don't get ML override\n",
    "                    cleaned_series[actual_idx] = prediction\n",
    "            \n",
    "            # Ensure non-negative values for count data\n",
    "            if any(service in metric_name.lower() for service in ['apigateway', 'customers', 'vets', 'visits']):\n",
    "                cleaned_series = np.maximum(0, cleaned_series)\n",
    "            \n",
    "            print(f\"  âœ“ Applied ML + penalty (factor: {penalty_factor}) for {metric_name} - {exp_name}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  âš  ML enhancement failed for {metric_name} - {exp_name}: {str(e)}\")\n",
    "    else:\n",
    "        if is_discrete_data:\n",
    "            print(f\"  â„¹ Applied penalty-only (factor: {penalty_factor}) for {metric_name} - {exp_name} (discrete data)\")\n",
    "        else:\n",
    "            print(f\"  â„¹ Applied penalty-only (factor: {penalty_factor}) for {metric_name} - {exp_name} (no model)\")\n",
    "    \n",
    "    # Calculate cleaning statistics\n",
    "    original_std = np.std(tcp_values)\n",
    "    cleaned_std = np.std(cleaned_series)\n",
    "    noise_reduction = (original_std - cleaned_std) / original_std * 100 if original_std > 0 else 0\n",
    "    correlation = np.corrcoef(tcp_values, cleaned_series)[0, 1] if len(tcp_values) > 1 else 1.0\n",
    "    \n",
    "    cleaning_stats = {\n",
    "        'outliers': outliers_count,\n",
    "        'outlier_percentage': (outliers_count / len(tcp_values)) * 100,\n",
    "        'noise_reduction': noise_reduction,\n",
    "        'correlation': correlation,\n",
    "        'mean_before': np.mean(tcp_values),\n",
    "        'mean_after': np.mean(cleaned_series),\n",
    "        'std_before': original_std,\n",
    "        'std_after': cleaned_std,\n",
    "        'zero_percentage': zero_percentage,\n",
    "        'unique_values': unique_values,\n",
    "        'data_type': 'discrete' if is_discrete_data else 'continuous',\n",
    "        'penalty_factor': penalty_factor\n",
    "    }\n",
    "    \n",
    "    return cleaned_series, outlier_mask, cleaning_stats\n",
    "\n",
    "def run_tuna_for_all_metrics():\n",
    "    \"\"\"Run TUNA for all TCP metrics\"\"\"\n",
    "    print(\"Running TUNA analysis on TCP datasets...\")\n",
    "    \n",
    "    tuna_results = {}\n",
    "    \n",
    "    # Process each TCP metric independently\n",
    "    for metric_name, experiments in all_datasets.items():\n",
    "        print(f\"Processing {metric_name}\")\n",
    "        \n",
    "        # Train metric-specific model on cross-experiment stable data\n",
    "        model, scaler = train_random_forest_for_metric(metric_name, experiments)\n",
    "        \n",
    "        # Apply cleaning to each experimental condition\n",
    "        metric_results = {}\n",
    "        for exp_name, df in experiments.items():\n",
    "            tcp_values = extract_tcp_values(df, metric_name)\n",
    "            cleaned_series, outlier_mask, stats = apply_tuna_to_single_series(\n",
    "                tcp_values, exp_name, model, scaler, metric_name=metric_name\n",
    "            )\n",
    "            \n",
    "            metric_results[exp_name] = {\n",
    "                'original': tcp_values,\n",
    "                'cleaned': cleaned_series,\n",
    "                'outliers': outlier_mask,\n",
    "                'stats': stats\n",
    "            }\n",
    "        \n",
    "        tuna_results[metric_name] = metric_results\n",
    "    \n",
    "    return tuna_results\n",
    "\n",
    "def plot_tuna_results(tuna_results):\n",
    "    \"\"\"Plot TUNA results with original vs cleaned comparison using Minutes (0-120) x-axis\"\"\"\n",
    "    \n",
    "    for metric_name, metric_results in tuna_results.items():\n",
    "        n_experiments = len(metric_results)\n",
    "        fig, axes = plt.subplots(n_experiments, 1, figsize=(15, 4*n_experiments))\n",
    "        \n",
    "        if n_experiments == 1:\n",
    "            axes = [axes]\n",
    "        \n",
    "        # Visualize cleaning results for each experiment\n",
    "        for i, (exp_name, results) in enumerate(metric_results.items()):\n",
    "            original = results['original']\n",
    "            cleaned = results['cleaned']\n",
    "            outliers = results['outliers']\n",
    "            stats = results['stats']\n",
    "            \n",
    "            # Get the corresponding dataframe to extract Minutes column\n",
    "            df = all_datasets[metric_name][exp_name]\n",
    "            minutes = df['Minutes'].values\n",
    "            \n",
    "            # Plot time series with cleaning overlay using Minutes as x-axis\n",
    "            axes[i].plot(minutes, original, 'b-', alpha=0.7, label='Original', linewidth=1)\n",
    "            axes[i].plot(minutes, cleaned, 'r-', alpha=0.8, label='TUNA Cleaned', linewidth=1.5)\n",
    "            \n",
    "            # Highlight detected outliers\n",
    "            outlier_points = np.where(outliers)[0]\n",
    "            if len(outlier_points) > 0:\n",
    "                axes[i].scatter(minutes[outlier_points], original[outlier_points], \n",
    "                               c='orange', s=20, alpha=0.7, label='Outliers', zorder=5)\n",
    "            \n",
    "            # Set appropriate y-axis label based on metric\n",
    "            if 'srtt' in metric_name.lower():\n",
    "                y_label = 'SRTT Values'\n",
    "            elif any(service in metric_name.lower() for service in ['apigateway', 'customers', 'visits', 'vets']):\n",
    "                y_label = 'Retransmission Packets'\n",
    "            else:\n",
    "                y_label = 'TCP Values'\n",
    "            \n",
    "            axes[i].set_title(f'{metric_name} - {exp_name} (Noise Reduction: {stats[\"noise_reduction\"]:.1f}%)')\n",
    "            axes[i].set_xlabel('Minutes')\n",
    "            axes[i].set_ylabel(y_label)\n",
    "            axes[i].set_xlim(0, 120)  # Set x-axis to match your experiment duration\n",
    "            axes[i].legend()\n",
    "            axes[i].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.suptitle(f'TUNA Results: {metric_name}', fontsize=16, y=0.98)\n",
    "        plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "        plt.show()\n",
    "\n",
    "def print_tuna_summary(tuna_results):\n",
    "    \"\"\"Print summary table of cleaning effectiveness\"\"\"\n",
    "    print(\"\\nTCP TUNA Results Summary:\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"{'Metric':<15} {'Experiment':<12} {'Outliers':<10} {'Noise Red%':<12} {'Correlation':<12}\")\n",
    "    print(\"-\"*60)\n",
    "    \n",
    "    # Tabulate results across all metrics and experiments\n",
    "    for metric_name, metric_results in tuna_results.items():\n",
    "        for exp_name, results in metric_results.items():\n",
    "            stats = results['stats']\n",
    "            print(f\"{metric_name:<15} {exp_name:<12} {stats['outliers']:<10} \"\n",
    "                  f\"{stats['noise_reduction']:>10.1f}% {stats['correlation']:>11.3f}\")\n",
    "\n",
    "# Execute complete TUNA analysis pipeline\n",
    "tuna_results = run_tuna_for_all_metrics()\n",
    "plot_tuna_results(tuna_results)\n",
    "print_tuna_summary(tuna_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

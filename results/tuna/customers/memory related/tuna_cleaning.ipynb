{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a69175",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import os\n",
    "from scipy import stats as scipy_stats\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import warnings\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# Load BASELINE datasets\n",
    "df_baseline_mem_available = pd.read_csv(\"baseline/mem_available.csv\")\n",
    "df_baseline_mem_cache = pd.read_csv(\"baseline/mem_cache.csv\")\n",
    "df_baseline_mem_util = pd.read_csv(\"baseline/mem_util.csv\")\n",
    "\n",
    "# Load CPU STRESS datasets\n",
    "df_cpustress_mem_available = pd.read_csv(\"cpu stress/mem_available.csv\")\n",
    "df_cpustress_mem_cache = pd.read_csv(\"cpu stress/mem_cache.csv\")\n",
    "df_cpustress_mem_util = pd.read_csv(\"cpu stress/mem_util.csv\")\n",
    "\n",
    "# Load IO datasets\n",
    "df_IO_mem_available = pd.read_csv(\"IO pressure/mem_available.csv\")\n",
    "df_IO_mem_cache = pd.read_csv(\"IO pressure/mem_cache.csv\")\n",
    "df_IO_mem_util = pd.read_csv(\"IO pressure/mem_util.csv\")\n",
    "\n",
    "# Load MEM STRESS datasets\n",
    "df_memstress_mem_available = pd.read_csv(\"mem stress/mem_available.csv\")\n",
    "df_memstress_mem_cache = pd.read_csv(\"mem stress/mem_cache.csv\")\n",
    "df_memstress_mem_util = pd.read_csv(\"mem stress/mem_util.csv\")\n",
    "\n",
    "# Load NET LOSS datasets\n",
    "df_netloss_mem_available = pd.read_csv(\"net loss/mem_available.csv\")\n",
    "df_netloss_mem_cache = pd.read_csv(\"net loss/mem_cache.csv\")\n",
    "df_netloss_mem_util = pd.read_csv(\"net loss/mem_util.csv\")\n",
    "\n",
    "\n",
    "# Add source labels - BASELINE\n",
    "df_baseline_mem_available[\"source\"] = \"BASELINE\"\n",
    "df_baseline_mem_cache[\"source\"] = \"BASELINE\"\n",
    "df_baseline_mem_util[\"source\"] = \"BASELINE\"\n",
    "\n",
    "# Add source labels - CPU STRESS\n",
    "df_cpustress_mem_available[\"source\"] = \"CPU_STRESS\"\n",
    "df_cpustress_mem_cache[\"source\"] = \"CPU_STRESS\"\n",
    "df_cpustress_mem_util[\"source\"] = \"CPU_STRESS\"\n",
    "\n",
    "# Add source labels - DELAY\n",
    "df_IO_mem_available[\"source\"] = \"DELAY\"\n",
    "df_IO_mem_cache[\"source\"] = \"DELAY\"\n",
    "df_IO_mem_util[\"source\"] = \"DELAY\"\n",
    "\n",
    "# Add source labels - MEM STRESS\n",
    "df_memstress_mem_available[\"source\"] = \"MEM_STRESS\"\n",
    "df_memstress_mem_cache[\"source\"] = \"MEM_STRESS\"\n",
    "df_memstress_mem_util[\"source\"] = \"MEM_STRESS\"\n",
    "\n",
    "# Add source labels - NET LOSS\n",
    "df_netloss_mem_available[\"source\"] = \"NET_LOSS\"\n",
    "df_netloss_mem_cache[\"source\"] = \"NET_LOSS\"\n",
    "df_netloss_mem_util[\"source\"] = \"NET_LOSS\"\n",
    "\n",
    "\n",
    "# Convert to datetime - BASELINE\n",
    "df_baseline_mem_available[\"Time\"] = pd.to_datetime(df_baseline_mem_available[\"Time\"])\n",
    "df_baseline_mem_cache[\"Time\"] = pd.to_datetime(df_baseline_mem_cache[\"Time\"])\n",
    "df_baseline_mem_util[\"Time\"] = pd.to_datetime(df_baseline_mem_util[\"Time\"])\n",
    "\n",
    "# Convert to datetime - CPU STRESS\n",
    "df_cpustress_mem_available[\"Time\"] = pd.to_datetime(df_cpustress_mem_available[\"Time\"])\n",
    "df_cpustress_mem_cache[\"Time\"] = pd.to_datetime(df_cpustress_mem_cache[\"Time\"])\n",
    "df_cpustress_mem_util[\"Time\"] = pd.to_datetime(df_cpustress_mem_util[\"Time\"])\n",
    "\n",
    "# Convert to datetime - DELAY\n",
    "df_IO_mem_available[\"Time\"] = pd.to_datetime(df_IO_mem_available[\"Time\"])\n",
    "df_IO_mem_cache[\"Time\"] = pd.to_datetime(df_IO_mem_cache[\"Time\"])\n",
    "df_IO_mem_util[\"Time\"] = pd.to_datetime(df_IO_mem_util[\"Time\"])\n",
    "\n",
    "# Convert to datetime - MEM STRESS\n",
    "df_memstress_mem_available[\"Time\"] = pd.to_datetime(df_memstress_mem_available[\"Time\"])\n",
    "df_memstress_mem_cache[\"Time\"] = pd.to_datetime(df_memstress_mem_cache[\"Time\"])\n",
    "df_memstress_mem_util[\"Time\"] = pd.to_datetime(df_memstress_mem_util[\"Time\"])\n",
    "\n",
    "# Convert to datetime - NET LOSS\n",
    "df_netloss_mem_available[\"Time\"] = pd.to_datetime(df_netloss_mem_available[\"Time\"])\n",
    "df_netloss_mem_cache[\"Time\"] = pd.to_datetime(df_netloss_mem_cache[\"Time\"])\n",
    "df_netloss_mem_util[\"Time\"] = pd.to_datetime(df_netloss_mem_util[\"Time\"])\n",
    "\n",
    "\n",
    "delay = 30\n",
    "duration = 50\n",
    "\n",
    "# Synchronize all datasets with baseline timeline\n",
    "time_offset = df_baseline_mem_available[\"Time\"].min()\n",
    "\n",
    "# Synchronize CPU STRESS datasets\n",
    "cpustress_offset = time_offset - df_cpustress_mem_available[\"Time\"].min()\n",
    "df_cpustress_mem_available[\"Time\"] += cpustress_offset\n",
    "df_cpustress_mem_cache[\"Time\"] += cpustress_offset\n",
    "df_cpustress_mem_util[\"Time\"] += cpustress_offset\n",
    "\n",
    "# Synchronize IO pressure datasets\n",
    "delay_offset = time_offset - df_IO_mem_available[\"Time\"].min()\n",
    "df_IO_mem_available[\"Time\"] += delay_offset\n",
    "df_IO_mem_cache[\"Time\"] += delay_offset\n",
    "df_IO_mem_util[\"Time\"] += delay_offset\n",
    "\n",
    "# Synchronize MEM STRESS datasets\n",
    "memstress_offset = time_offset - df_memstress_mem_available[\"Time\"].min()\n",
    "df_memstress_mem_available[\"Time\"] += memstress_offset\n",
    "df_memstress_mem_cache[\"Time\"] += memstress_offset\n",
    "df_memstress_mem_util[\"Time\"] += memstress_offset\n",
    "\n",
    "# Synchronize NET LOSS datasets\n",
    "netloss_offset = time_offset - df_netloss_mem_available[\"Time\"].min()\n",
    "df_netloss_mem_available[\"Time\"] += netloss_offset\n",
    "df_netloss_mem_cache[\"Time\"] += netloss_offset\n",
    "df_netloss_mem_util[\"Time\"] += netloss_offset\n",
    "\n",
    "\n",
    "# Convert timeline to minutes for ALL datasets\n",
    "all_dfs = [\n",
    "    # Baseline\n",
    "    df_baseline_mem_available, df_baseline_mem_cache, df_baseline_mem_util,\n",
    "    # CPU Stress\n",
    "    df_cpustress_mem_available, df_cpustress_mem_cache, df_cpustress_mem_util,\n",
    "    # IO pressure\n",
    "    df_IO_mem_available, df_IO_mem_cache, df_IO_mem_util,\n",
    "    # Memory Stress\n",
    "    df_memstress_mem_available, df_memstress_mem_cache, df_memstress_mem_util,\n",
    "    # Network Loss\n",
    "    df_netloss_mem_available, df_netloss_mem_cache, df_netloss_mem_util\n",
    "]\n",
    "\n",
    "for df in all_dfs:\n",
    "    df[\"Minutes\"] = (df[\"Time\"] - df[\"Time\"].min()).dt.total_seconds() / 60\n",
    "\n",
    "# COMPLETE DATASETS DICTIONARY \n",
    "all_datasets = {\n",
    "    'MemAvailable': {\n",
    "        'baseline': df_baseline_mem_available,\n",
    "        'cpu_stress': df_cpustress_mem_available,\n",
    "        'IO': df_IO_mem_available,\n",
    "        'mem_stress': df_memstress_mem_available,\n",
    "        'net_loss': df_netloss_mem_available,\n",
    "    },\n",
    "    'MemCache': {\n",
    "        'baseline': df_baseline_mem_cache,\n",
    "        'cpu_stress': df_cpustress_mem_cache,\n",
    "        'IO': df_IO_mem_cache,\n",
    "        'mem_stress': df_memstress_mem_cache,\n",
    "        'net_loss': df_netloss_mem_cache,\n",
    "    },\n",
    "    'MemUtil': {\n",
    "        'baseline': df_baseline_mem_util,\n",
    "        'cpu_stress': df_cpustress_mem_util,\n",
    "        'IO': df_IO_mem_util,\n",
    "        'mem_stress': df_memstress_mem_util,\n",
    "        'net_loss': df_netloss_mem_util,\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"✅ All memory datasets loaded successfully!\")\n",
    "print(f\"📊 Loaded {len(all_datasets)} memory metrics across {len(all_datasets['MemAvailable'])} experiment types\")\n",
    "print(\"\\nDataset structure:\")\n",
    "for metric, experiments in all_datasets.items():\n",
    "    print(f\"  {metric}: {list(experiments.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb39d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_convert_to_float(series):\n",
    "    try:\n",
    "        return pd.to_numeric(series, errors='coerce').fillna(0.0).astype(float)\n",
    "    except:\n",
    "        return series.fillna(0.0).astype(float)\n",
    "\n",
    "def safe_array_conversion(data):\n",
    "    data = np.array(data, dtype=float)\n",
    "    if np.any(np.isnan(data)):\n",
    "        median_val = np.nanmedian(data)\n",
    "        if np.isnan(median_val):\n",
    "            median_val = 0.0\n",
    "        data = np.nan_to_num(data, nan=median_val)\n",
    "    return data\n",
    "\n",
    "def get_data_columns(df):\n",
    "    \"\"\"Get columns that contain actual data (excluding time/metadata columns)\"\"\"\n",
    "    exclude_cols = ['Time', 'Minutes', 'source']\n",
    "    return [col for col in df.columns if col not in exclude_cols]\n",
    "\n",
    "def detect_outliers_tuna(timeseries, window_size=3, threshold=0.65, min_absolute_range=None):\n",
    "    timeseries = safe_array_conversion(timeseries)\n",
    "    outlier_mask = np.zeros(len(timeseries), dtype=bool)\n",
    "    \n",
    "    if min_absolute_range is None:\n",
    "        std_val = np.std(timeseries)\n",
    "        if std_val == 0 or np.isnan(std_val):\n",
    "            min_absolute_range = 0.01 * np.max(timeseries)\n",
    "        else:\n",
    "            min_absolute_range = 0.1 * std_val\n",
    "    \n",
    "    if min_absolute_range <= 0:\n",
    "        min_absolute_range = 1e-6\n",
    "    \n",
    "    for i in range(len(timeseries) - window_size + 1):\n",
    "        window = timeseries[i:i + window_size]\n",
    "        window_mean = np.mean(window)\n",
    "        window_range = np.max(window) - np.min(window)\n",
    "        \n",
    "        if window_mean > 0 and window_range > 0:\n",
    "            relative_range = window_range / window_mean\n",
    "            \n",
    "            if relative_range > threshold and window_range > min_absolute_range:\n",
    "                window_max = np.max(window)\n",
    "                window_min = np.min(window)\n",
    "                \n",
    "                for j in range(window_size):\n",
    "                    actual_idx = i + j\n",
    "                    if (timeseries[actual_idx] == window_max or \n",
    "                        timeseries[actual_idx] == window_min):\n",
    "                        outlier_mask[actual_idx] = True\n",
    "    \n",
    "    return outlier_mask\n",
    "\n",
    "def create_features_for_ml(timeseries, experiment_type, window_size=10):\n",
    "    timeseries = safe_array_conversion(timeseries)\n",
    "    features = []\n",
    "    \n",
    "    for i in range(window_size, len(timeseries)):\n",
    "        window = timeseries[i-window_size:i]\n",
    "        \n",
    "        feature_vector = [\n",
    "            np.mean(window),\n",
    "            np.std(window),\n",
    "            np.median(window),\n",
    "            np.max(window) - np.min(window),\n",
    "            timeseries[i-1],\n",
    "            i / len(timeseries),\n",
    "        ]\n",
    "        \n",
    "        feature_vector = [0.0 if np.isnan(val) or np.isinf(val) else float(val) for val in feature_vector]\n",
    "        \n",
    "        exp_features = [0, 0, 0, 0, 0]\n",
    "        if experiment_type == \"baseline\":\n",
    "            exp_features[0] = 1\n",
    "        elif experiment_type == \"cpu_stress\":\n",
    "            exp_features[1] = 1\n",
    "        elif experiment_type == \"delay\":\n",
    "            exp_features[2] = 1\n",
    "        elif experiment_type == \"mem_stress\":\n",
    "            exp_features[3] = 1\n",
    "        elif experiment_type == \"net_loss\":\n",
    "            exp_features[4] = 1\n",
    "        \n",
    "        feature_vector.extend(exp_features)\n",
    "        features.append(feature_vector)\n",
    "    \n",
    "    return np.array(features)\n",
    "\n",
    "def train_random_forest_for_column(column_name, column_data_dict, experiments_dict):\n",
    "    \"\"\"Train a random forest model for a specific column across all experiments\"\"\"\n",
    "    X_stable_all = []\n",
    "    y_stable_all = []\n",
    "    \n",
    "    for exp_name, column_values in column_data_dict.items():\n",
    "        column_values = safe_array_conversion(column_values)\n",
    "        \n",
    "        outlier_mask = detect_outliers_tuna(column_values)\n",
    "        stable_mask = ~outlier_mask\n",
    "        \n",
    "        features = create_features_for_ml(column_values, exp_name)\n",
    "        if len(features) > 0:\n",
    "            stable_features_mask = stable_mask[10:10+len(features)]\n",
    "            stable_features = features[stable_features_mask]\n",
    "            stable_targets = column_values[10:10+len(features)][stable_features_mask]\n",
    "            \n",
    "            valid_mask = ~(np.isnan(stable_targets) | np.isinf(stable_targets))\n",
    "            stable_features = stable_features[valid_mask]\n",
    "            stable_targets = stable_targets[valid_mask]\n",
    "            \n",
    "            if len(stable_features) > 0:\n",
    "                smoothed_targets = []\n",
    "                for j, target in enumerate(stable_targets):\n",
    "                    start_idx = max(0, j-2)\n",
    "                    end_idx = min(len(stable_targets), j+3)\n",
    "                    local_values = stable_targets[start_idx:end_idx]\n",
    "                    smoothed_targets.append(np.median(local_values))\n",
    "                \n",
    "                X_stable_all.extend(stable_features)\n",
    "                y_stable_all.extend(smoothed_targets)\n",
    "    \n",
    "    if len(X_stable_all) < 10:\n",
    "        return None, None\n",
    "    \n",
    "    X_stable_all = np.array(X_stable_all)\n",
    "    y_stable_all = np.array(y_stable_all)\n",
    "    \n",
    "    # Clean NaN values\n",
    "    nan_mask = np.isnan(y_stable_all) | np.isinf(y_stable_all)\n",
    "    if np.any(nan_mask):\n",
    "        X_stable_all = X_stable_all[~nan_mask]\n",
    "        y_stable_all = y_stable_all[~nan_mask]\n",
    "    \n",
    "    feature_nan_mask = np.isnan(X_stable_all).any(axis=1) | np.isinf(X_stable_all).any(axis=1)\n",
    "    if np.any(feature_nan_mask):\n",
    "        X_stable_all = X_stable_all[~feature_nan_mask]\n",
    "        y_stable_all = y_stable_all[~feature_nan_mask]\n",
    "    \n",
    "    if len(X_stable_all) < 10:\n",
    "        return None, None\n",
    "    \n",
    "    try:\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(X_stable_all)\n",
    "\n",
    "        model = RandomForestRegressor(\n",
    "            n_estimators=50,\n",
    "            max_depth=10,\n",
    "            min_samples_split=5,\n",
    "            min_samples_leaf=2,\n",
    "            max_features='sqrt',\n",
    "            random_state=42,\n",
    "            n_jobs=1\n",
    "        )\n",
    "        \n",
    "        model.fit(X_scaled, y_stable_all)\n",
    "        return model, scaler\n",
    "    except Exception:\n",
    "        return None, None\n",
    "\n",
    "def apply_penalty(timeseries, outlier_mask, penalty_factor=0.5):\n",
    "    timeseries = safe_array_conversion(timeseries)\n",
    "    cleaned_series = timeseries.copy()\n",
    "    \n",
    "    stable_values = timeseries[~outlier_mask]\n",
    "    if len(stable_values) > 0:\n",
    "        baseline = np.median(stable_values)\n",
    "    else:\n",
    "        baseline = np.median(timeseries)\n",
    "    \n",
    "    if np.isnan(baseline):\n",
    "        baseline = 0.0\n",
    "    \n",
    "    for i in range(len(timeseries)):\n",
    "        if outlier_mask[i]:\n",
    "            original_value = timeseries[i]\n",
    "            if np.isnan(original_value):\n",
    "                cleaned_series[i] = baseline\n",
    "            else:\n",
    "                cleaned_series[i] = baseline + (original_value - baseline) * penalty_factor\n",
    "    \n",
    "    return cleaned_series\n",
    "\n",
    "def apply_tuna_to_single_column(column_values, exp_name, model, scaler, penalty_factor=0.5):\n",
    "    \"\"\"Apply TUNA to a single column\"\"\"\n",
    "    column_values = safe_array_conversion(column_values)\n",
    "    \n",
    "    outlier_mask = detect_outliers_tuna(column_values)\n",
    "    stable_mask = ~outlier_mask\n",
    "    outliers_count = np.sum(outlier_mask)\n",
    "    \n",
    "    cleaned_series = apply_penalty(column_values, outlier_mask, penalty_factor)\n",
    "    \n",
    "    if model is not None and scaler is not None:\n",
    "        try:\n",
    "            features = create_features_for_ml(column_values, exp_name)\n",
    "            if len(features) > 0:\n",
    "                features_scaled = scaler.transform(features)\n",
    "                ml_predictions = model.predict(features_scaled)\n",
    "                \n",
    "                for i, prediction in enumerate(ml_predictions):\n",
    "                    actual_idx = i + 10\n",
    "                    if actual_idx < len(cleaned_series) and stable_mask[actual_idx]:\n",
    "                        if not np.isnan(prediction) and not np.isinf(prediction):\n",
    "                            cleaned_series[actual_idx] = prediction\n",
    "        except Exception:\n",
    "            pass\n",
    "    \n",
    "    original_std = np.std(column_values)\n",
    "    cleaned_std = np.std(cleaned_series)\n",
    "    \n",
    "    if original_std > 0 and not np.isnan(original_std):\n",
    "        noise_reduction = (original_std - cleaned_std) / original_std * 100\n",
    "    else:\n",
    "        noise_reduction = 0.0\n",
    "    \n",
    "    if len(column_values) > 1:\n",
    "        correlation = np.corrcoef(column_values, cleaned_series)[0, 1]\n",
    "        if np.isnan(correlation):\n",
    "            correlation = 1.0\n",
    "    else:\n",
    "        correlation = 1.0\n",
    "    \n",
    "    cleaning_stats = {\n",
    "        'outliers': outliers_count,\n",
    "        'outlier_percentage': (outliers_count / len(column_values)) * 100,\n",
    "        'noise_reduction': noise_reduction,\n",
    "        'correlation': correlation\n",
    "    }\n",
    "    \n",
    "    return cleaned_series, outlier_mask, cleaning_stats\n",
    "\n",
    "def run_tuna_for_all_metrics_column_by_column(all_datasets):\n",
    "    \"\"\"Run TUNA processing column by column for all metrics\"\"\"\n",
    "    tuna_results = {}\n",
    "    \n",
    "    for metric_name, experiments in all_datasets.items():\n",
    "        print(f\"\\n🔄 Processing metric: {metric_name}\")\n",
    "        \n",
    "        # Get all data columns for this metric\n",
    "        sample_df = next(iter(experiments.values()))\n",
    "        data_columns = get_data_columns(sample_df)\n",
    "        \n",
    "        print(f\"  📊 Data columns: {data_columns}\")\n",
    "        \n",
    "        # Train models for each column\n",
    "        column_models = {}\n",
    "        for col in data_columns:\n",
    "            print(f\"    🤖 Training model for column: {col}\")\n",
    "            \n",
    "            # Collect column data across all experiments\n",
    "            column_data_dict = {}\n",
    "            for exp_name, df in experiments.items():\n",
    "                if col in df.columns:\n",
    "                    column_data_dict[exp_name] = safe_convert_to_float(df[col])\n",
    "            \n",
    "            if column_data_dict:\n",
    "                model, scaler = train_random_forest_for_column(col, column_data_dict, experiments)\n",
    "                column_models[col] = (model, scaler)\n",
    "            else:\n",
    "                column_models[col] = (None, None)\n",
    "        \n",
    "        # Process each experiment\n",
    "        metric_results = {}\n",
    "        for exp_name, df in experiments.items():\n",
    "            print(f\"  🔄 Processing experiment: {exp_name}\")\n",
    "            \n",
    "            column_results = {}\n",
    "            overall_stats = {'outliers': 0, 'noise_reduction': 0.0, 'correlation': 0.0}\n",
    "            \n",
    "            # Process each column\n",
    "            for col in data_columns:\n",
    "                if col in df.columns:\n",
    "                    column_values = safe_convert_to_float(df[col])\n",
    "                    model, scaler = column_models[col]\n",
    "                    \n",
    "                    cleaned_series, outlier_mask, stats = apply_tuna_to_single_column(\n",
    "                        column_values, exp_name, model, scaler\n",
    "                    )\n",
    "                    \n",
    "                    column_results[col] = {\n",
    "                        'original': column_values.values,\n",
    "                        'cleaned': cleaned_series,\n",
    "                        'outliers': outlier_mask,\n",
    "                        'stats': stats\n",
    "                    }\n",
    "                    \n",
    "                    # Accumulate overall stats\n",
    "                    overall_stats['outliers'] += stats['outliers']\n",
    "                    overall_stats['noise_reduction'] += stats['noise_reduction']\n",
    "                    overall_stats['correlation'] += stats['correlation']\n",
    "            \n",
    "            # Average the overall stats\n",
    "            if len(column_results) > 0:\n",
    "                overall_stats['noise_reduction'] /= len(column_results)\n",
    "                overall_stats['correlation'] /= len(column_results)\n",
    "            \n",
    "            metric_results[exp_name] = {\n",
    "                'column_results': column_results,\n",
    "                'stats': overall_stats\n",
    "            }\n",
    "        \n",
    "        tuna_results[metric_name] = metric_results\n",
    "    \n",
    "    return tuna_results\n",
    "\n",
    "def plot_tuna_results_column_by_column(tuna_results, all_datasets):\n",
    "    \"\"\"Plot results for column-by-column processing - using same format as original\"\"\"\n",
    "    for metric_name, metric_results in tuna_results.items():\n",
    "        n_experiments = len(metric_results)\n",
    "        fig, axes = plt.subplots(n_experiments, 1, figsize=(15, 4*n_experiments))\n",
    "        \n",
    "        if n_experiments == 1:\n",
    "            axes = [axes]\n",
    "        \n",
    "        for i, (exp_name, results) in enumerate(metric_results.items()):\n",
    "            df = all_datasets[metric_name][exp_name]\n",
    "            minutes = df['Minutes'].values\n",
    "            \n",
    "            # For column-by-column, we'll plot the first/main column or aggregate if multiple\n",
    "            column_results = results['column_results']\n",
    "            \n",
    "            if len(column_results) == 1:\n",
    "                # Single column - plot directly\n",
    "                col_name = list(column_results.keys())[0]\n",
    "                col_results = column_results[col_name]\n",
    "                original = col_results['original']\n",
    "                cleaned = col_results['cleaned']\n",
    "                outliers = col_results['outliers']\n",
    "                stats = col_results['stats']\n",
    "            else:\n",
    "                # Multiple columns - aggregate for visualization\n",
    "                # Take the first column as representative, or could sum/average\n",
    "                col_name = list(column_results.keys())[0]\n",
    "                col_results = column_results[col_name]\n",
    "                original = col_results['original']\n",
    "                cleaned = col_results['cleaned']\n",
    "                outliers = col_results['outliers']\n",
    "                stats = results['stats']  # Use overall stats for multiple columns\n",
    "            \n",
    "            axes[i].plot(minutes, original, 'b-', alpha=0.7, label='Original', linewidth=1)\n",
    "            axes[i].plot(minutes, cleaned, 'r-', alpha=0.8, label='TUNA Cleaned', linewidth=1.5)\n",
    "            \n",
    "            outlier_points = np.where(outliers)[0]\n",
    "            if len(outlier_points) > 0:\n",
    "                axes[i].scatter(minutes[outlier_points], original[outlier_points], \n",
    "                               c='orange', s=20, alpha=0.7, label='Outliers', zorder=5)\n",
    "            \n",
    "            # Determine y-label based on metric type\n",
    "            if 'srtt' in metric_name.lower():\n",
    "                y_label = 'SRTT Values'\n",
    "            elif any(service in metric_name.lower() for service in ['apigateway', 'customers', 'visits', 'vets']):\n",
    "                y_label = 'Retransmission Packets'\n",
    "            elif 'mem' in metric_name.lower():\n",
    "                y_label = 'Memory Values'\n",
    "            else:\n",
    "                y_label = 'Values'\n",
    "            \n",
    "            axes[i].set_title(f'{metric_name} - {exp_name} (Noise Reduction: {stats[\"noise_reduction\"]:.1f}%)')\n",
    "            axes[i].set_xlabel('Minutes')\n",
    "            axes[i].set_ylabel(y_label)\n",
    "            axes[i].set_xlim(0, 120) \n",
    "            axes[i].legend()\n",
    "            axes[i].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.suptitle(f'TUNA Results: {metric_name}', fontsize=16, y=0.98)\n",
    "        plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "        plt.show()\n",
    "\n",
    "def print_tuna_summary_column_by_column(tuna_results):\n",
    "    \"\"\"Print summary for column-by-column processing\"\"\"\n",
    "    print(\"\\nColumn-by-Column TUNA Results Summary:\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"{'Metric':<15} {'Experiment':<12} {'Column':<15} {'Outliers':<10} {'Noise Red%':<12} {'Correlation':<12}\")\n",
    "    print(\"-\"*80)\n",
    "    \n",
    "    for metric_name, metric_results in tuna_results.items():\n",
    "        for exp_name, results in metric_results.items():\n",
    "            for col_name, col_results in results['column_results'].items():\n",
    "                stats = col_results['stats']\n",
    "                print(f\"{metric_name:<15} {exp_name:<12} {col_name:<15} {stats['outliers']:<10} \"\n",
    "                      f\"{stats['noise_reduction']:>10.1f}% {stats['correlation']:>11.3f}\")\n",
    "\n",
    "def create_cleaned_csv_from_tuna_columns(original_df, column_results, metric_name, experiment_name, include_metadata=False):\n",
    "    \"\"\"Create cleaned CSV from column-by-column results\"\"\"\n",
    "    cleaned_df = original_df.copy()\n",
    "    \n",
    "    # Replace each column with its cleaned version\n",
    "    for col, col_results in column_results.items():\n",
    "        if col in cleaned_df.columns:\n",
    "            cleaned_series = col_results['cleaned']\n",
    "            cleaned_df[col] = cleaned_series[:len(cleaned_df)]\n",
    "    \n",
    "    if include_metadata:\n",
    "        # Add overall metadata\n",
    "        total_outliers = sum(col_results['stats']['outliers'] for col_results in column_results.values())\n",
    "        avg_noise_reduction = sum(col_results['stats']['noise_reduction'] for col_results in column_results.values()) / len(column_results)\n",
    "        avg_correlation = sum(col_results['stats']['correlation'] for col_results in column_results.values()) / len(column_results)\n",
    "        \n",
    "        cleaned_df['tuna_processed'] = True\n",
    "        cleaned_df['tuna_total_outliers'] = total_outliers\n",
    "        cleaned_df['tuna_avg_noise_reduction'] = avg_noise_reduction\n",
    "        cleaned_df['tuna_avg_correlation'] = avg_correlation\n",
    "        cleaned_df['tuna_metric_name'] = metric_name\n",
    "        cleaned_df['tuna_experiment'] = experiment_name\n",
    "        \n",
    "        # Add column-specific metadata\n",
    "        for col, col_results in column_results.items():\n",
    "            stats = col_results['stats']\n",
    "            outlier_mask = col_results['outliers']\n",
    "            cleaned_df[f'tuna_{col}_outliers'] = stats['outliers']\n",
    "            cleaned_df[f'tuna_{col}_noise_reduction'] = stats['noise_reduction']\n",
    "            cleaned_df[f'tuna_{col}_correlation'] = stats['correlation']\n",
    "            cleaned_df[f'tuna_{col}_outlier_flag'] = outlier_mask[:len(cleaned_df)]\n",
    "    \n",
    "    return cleaned_df\n",
    "\n",
    "def export_all_cleaned_csvs(tuna_results, all_datasets, output_base_path=\"cleaned_memory_data\", include_metadata=False, preserve_structure=True):\n",
    "    \"\"\"Export all TUNA-cleaned datasets organized by experiment type\"\"\"\n",
    "    output_base = Path(output_base_path)\n",
    "    output_base.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    created_files = {}\n",
    "    total_files = 0\n",
    "    \n",
    "    # Get all experiment types from all metrics\n",
    "    experiment_types = set()\n",
    "    for metric_results in tuna_results.values():\n",
    "        experiment_types.update(metric_results.keys())\n",
    "    \n",
    "    print(f\"📁 Creating directories for experiments: {sorted(experiment_types)}\")\n",
    "    \n",
    "    # Create directory structure by experiment type\n",
    "    for exp_name in experiment_types:\n",
    "        exp_dir = output_base / exp_name\n",
    "        exp_dir.mkdir(exist_ok=True)\n",
    "        created_files[exp_name] = []\n",
    "    \n",
    "    # Process each metric and experiment\n",
    "    for metric_name, metric_results in tuna_results.items():\n",
    "        print(f\"\\n📊 Processing {metric_name}...\")\n",
    "        \n",
    "        for exp_name, results in metric_results.items():\n",
    "            print(f\"  🔄 Exporting {exp_name}...\")\n",
    "            \n",
    "            # Get original dataframe and column results\n",
    "            original_df = all_datasets[metric_name][exp_name]\n",
    "            column_results = results['column_results']\n",
    "            \n",
    "            # Create cleaned dataframe\n",
    "            cleaned_df = create_cleaned_csv_from_tuna_columns(\n",
    "                original_df=original_df,\n",
    "                column_results=column_results,\n",
    "                metric_name=metric_name,\n",
    "                experiment_name=exp_name,\n",
    "                include_metadata=include_metadata\n",
    "            )\n",
    "            \n",
    "            # Print detailed column statistics\n",
    "            for col, col_result in column_results.items():\n",
    "                col_stats = col_result['stats']\n",
    "                print(f\"    📈 {col}: {col_stats['outliers']} outliers, {col_stats['noise_reduction']:.1f}% noise reduction\")\n",
    "            \n",
    "            csv_filename = f\"{metric_name.lower()}_{exp_name}.csv\"\n",
    "            csv_path = output_base / exp_name / csv_filename\n",
    "            \n",
    "            # Export to CSV\n",
    "            cleaned_df.to_csv(csv_path, index=False)\n",
    "            created_files[exp_name].append(str(csv_path))\n",
    "            total_files += 1\n",
    "            \n",
    "            # Print overall summary\n",
    "            total_outliers = sum(col_result['stats']['outliers'] for col_result in column_results.values())\n",
    "            overall_stats = results['stats']\n",
    "            outliers_pct = (total_outliers / len(cleaned_df)) * 100\n",
    "            print(f\"    ✅ {csv_filename}\")\n",
    "            print(f\"       📈 Total outliers: {total_outliers} ({outliers_pct:.1f}%)\")\n",
    "            print(f\"       🔧 Overall noise reduction: {overall_stats['noise_reduction']:.1f}%\")\n",
    "            print(f\"       📊 Overall correlation: {overall_stats['correlation']:.3f}\")\n",
    "    \n",
    "    print(f\"\\n✅ Export completed!\")\n",
    "    print(f\"📁 Total files created: {total_files}\")\n",
    "    print(f\"📂 Directory structure:\")\n",
    "    for exp_name in sorted(experiment_types):\n",
    "        print(f\"  {exp_name}/\")\n",
    "        for file_path in created_files[exp_name]:\n",
    "            print(f\"    📄 {Path(file_path).name}\")\n",
    "    \n",
    "    return created_files\n",
    "\n",
    "def export_tuna_data_to_csv(tuna_results, export_dir=\"../../noise_reduction_data\"):    \n",
    "    export_data = []\n",
    "    \n",
    "    for metric_name, metric_data in tuna_results.items():\n",
    "        for experiment_name, experiment_data in metric_data.items():\n",
    "            if 'stats' in experiment_data:\n",
    "                stats = experiment_data['stats']\n",
    "                \n",
    "                export_data.append({\n",
    "                    'experiment_name': experiment_name,\n",
    "                    'metric': metric_name,\n",
    "                    'noise_reduction_pct': stats.get('noise_reduction', 0.0),\n",
    "                    'correlation': stats.get('correlation', 1.0),\n",
    "                    'outliers_removed': stats.get('outliers', 0),\n",
    "                })\n",
    "    \n",
    "    csv_filename = f\"customers_tuna_memory.csv\"\n",
    "    csv_path = os.path.join(export_dir, csv_filename)\n",
    "    \n",
    "    df_simple = pd.DataFrame(export_data)\n",
    "    df_simple.to_csv(csv_path, index=False)\n",
    "\n",
    "    return csv_path\n",
    "\n",
    "def main(all_datasets):    \n",
    "    tuna_results = run_tuna_for_all_metrics_column_by_column(all_datasets)\n",
    "    csv_path = export_tuna_data_to_csv(tuna_results)\n",
    "    plot_tuna_results_column_by_column(tuna_results, all_datasets)\n",
    "    print_tuna_summary_column_by_column(tuna_results)\n",
    "    \n",
    "    created_files = export_all_cleaned_csvs(\n",
    "        tuna_results=tuna_results,\n",
    "        all_datasets=all_datasets,\n",
    "        output_base_path=\"cleaned_memory_data\",\n",
    "        include_metadata=False\n",
    "    )\n",
    "    \n",
    "    return tuna_results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    tuna_results = main(all_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b21d874",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_series(series):\n",
    "    if len(series) == 0:\n",
    "        return series\n",
    "    \n",
    "    min_val = series.min()\n",
    "    max_val = series.max()\n",
    "    if max_val > min_val:\n",
    "        return (series - min_val) / (max_val - min_val)\n",
    "    else:\n",
    "        return pd.Series(np.zeros(len(series)), index=series.index if hasattr(series, 'index') else None)\n",
    "\n",
    "def get_memory_y_axis_label(metric_name):\n",
    "    labels = {\n",
    "        'MemAvailable': 'Memory Available (KB)',\n",
    "        'MemCache': 'Memory Cache (KB)', \n",
    "        'MemUtil': 'Memory Utilization (%)',\n",
    "        'Memory Utilization': 'Memory Utilization (%)',\n",
    "        'Memory Cache': 'Memory Cache (KB)',\n",
    "        'Memory Available': 'Memory Available (KB)',\n",
    "        'Memory Usage': 'Memory Usage (KB)',\n",
    "        'Memory Buffer': 'Memory Buffer (KB)',\n",
    "        'Memory Free': 'Memory Free (KB)',\n",
    "    }\n",
    "    return labels.get(metric_name, 'Memory Value')\n",
    "\n",
    "def analyze_and_plot_memory_comparison(metric_name, experiment_name, experiment_df, baseline_df, \n",
    "                                     tuna_results=None, delay_minutes=30, duration_minutes=50):\n",
    "    \n",
    "    experiment_values = extract_memory_values(experiment_df)\n",
    "    baseline_values = extract_memory_values(baseline_df)\n",
    "    \n",
    "    experiment_cleaned = None\n",
    "    baseline_cleaned = None\n",
    "    experiment_outliers = None\n",
    "    baseline_outliers = None\n",
    "    \n",
    "    if tuna_results and metric_name in tuna_results:\n",
    "        if experiment_name in tuna_results[metric_name]:\n",
    "            experiment_cleaned = tuna_results[metric_name][experiment_name]['cleaned']\n",
    "            experiment_outliers = tuna_results[metric_name][experiment_name]['outliers']\n",
    "        if 'baseline' in tuna_results[metric_name]:\n",
    "            baseline_cleaned = tuna_results[metric_name]['baseline']['cleaned']\n",
    "            baseline_outliers = tuna_results[metric_name]['baseline']['outliers']\n",
    "    \n",
    "    experiment_series = experiment_cleaned if experiment_cleaned is not None else experiment_values\n",
    "    baseline_series = baseline_cleaned if baseline_cleaned is not None else baseline_values\n",
    "    \n",
    "    experiment_norm = normalize_series(pd.Series(experiment_series))\n",
    "    baseline_norm = normalize_series(pd.Series(baseline_series))\n",
    "    \n",
    "    min_length = min(len(experiment_norm), len(baseline_norm), \n",
    "                    len(experiment_df['Minutes']), len(baseline_df['Minutes']))\n",
    "    \n",
    "    df_combined = pd.DataFrame({\n",
    "        'Baseline': baseline_norm.iloc[:min_length],\n",
    "        'Experiment': experiment_norm.iloc[:min_length],\n",
    "        'Minutes': experiment_df['Minutes'].iloc[:min_length],\n",
    "        'Baseline_Minutes': baseline_df['Minutes'].iloc[:min_length]\n",
    "    })\n",
    "    \n",
    "    df_combined['difference'] = df_combined['Experiment'] - df_combined['Baseline']\n",
    "    \n",
    "    df_combined['phase'] = 'before'\n",
    "    df_combined.loc[(df_combined['Minutes'] >= delay_minutes) & \n",
    "                   (df_combined['Minutes'] <= delay_minutes + duration_minutes), 'phase'] = 'during'\n",
    "    df_combined.loc[df_combined['Minutes'] > delay_minutes + duration_minutes, 'phase'] = 'after'\n",
    "    \n",
    "    stats_baseline = df_combined.groupby('phase')['Baseline'].agg(['mean', 'std', 'min', 'max'])\n",
    "    stats_experiment = df_combined.groupby('phase')['Experiment'].agg(['mean', 'std', 'min', 'max'])\n",
    "    \n",
    "    t_tests = {}\n",
    "    available_phases = stats_baseline.index.tolist()\n",
    "    \n",
    "    for phase in ['before', 'during', 'after']:\n",
    "        if phase in available_phases:\n",
    "            data = df_combined[df_combined['phase'] == phase]\n",
    "            if len(data) > 1:\n",
    "                t_stat, p_value = scipy_stats.ttest_ind(data['Baseline'].dropna(), data['Experiment'].dropna())\n",
    "                t_tests[phase] = {'t': t_stat, 'p': p_value}\n",
    "            else:\n",
    "                t_tests[phase] = {'t': 0, 'p': 1}\n",
    "        else:\n",
    "            t_tests[phase] = {'t': 0, 'p': 1, 'note': 'Phase not present in data'}\n",
    "    \n",
    "    impact = {}\n",
    "    for phase in ['before', 'during', 'after']:\n",
    "        if phase in available_phases:\n",
    "            baseline_mean = stats_baseline.loc[phase, 'mean']\n",
    "            experiment_mean = stats_experiment.loc[phase, 'mean']\n",
    "            if baseline_mean != 0:\n",
    "                impact[phase] = ((experiment_mean - baseline_mean) / baseline_mean) * 100\n",
    "            else:\n",
    "                impact[phase] = 0\n",
    "        else:\n",
    "            impact[phase] = 0\n",
    "    \n",
    "    fig = plt.figure(figsize=(18, 26))\n",
    "    \n",
    "    y_label = get_memory_y_axis_label(metric_name)\n",
    "    \n",
    "    plt.subplot(4, 1, 1)\n",
    "    plt.plot(baseline_df['Minutes'][:len(baseline_values)], baseline_values, \n",
    "             label='Baseline Original', color='orange', alpha=0.7, linewidth=2)\n",
    "    \n",
    "    if baseline_cleaned is not None:\n",
    "        plt.plot(baseline_df['Minutes'][:len(baseline_cleaned)], baseline_cleaned, \n",
    "                 label='Baseline TUNA Cleaned', linewidth=2, color='blue')\n",
    "        \n",
    "        if baseline_outliers is not None:\n",
    "            outlier_points = np.where(baseline_outliers)[0]\n",
    "            if len(outlier_points) > 0:\n",
    "                plt.scatter(baseline_df['Minutes'].iloc[outlier_points], \n",
    "                           baseline_values[outlier_points], \n",
    "                           c='red', s=15, alpha=0.7, label='Outliers', zorder=5)\n",
    "        \n",
    "        title_suffix = \" (with TUNA cleaning)\"\n",
    "    else:\n",
    "        plt.plot(baseline_df['Minutes'][:len(baseline_values)], baseline_values, \n",
    "                 label='Baseline', linewidth=2, color='blue')\n",
    "        title_suffix = \"\"\n",
    "    \n",
    "    plt.axvspan(delay_minutes, delay_minutes + duration_minutes, \n",
    "                color='red', alpha=0.2, label=\"Stress/Delay Period\")\n",
    "    plt.title(f'{metric_name}: Baseline Data{title_suffix}', fontsize=16, fontweight='bold')\n",
    "    plt.xlabel('Minutes', fontsize=14)\n",
    "    plt.ylabel(y_label, fontsize=14)\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tick_params(axis='both', which='major', labelsize=12)\n",
    "    \n",
    "    plt.subplot(4, 1, 2)\n",
    "    plt.plot(experiment_df['Minutes'][:len(experiment_values)], experiment_values, \n",
    "             label=f'{experiment_name.replace(\"_\", \" \").title()} Original', \n",
    "             alpha=0.7, color='lightcoral', linewidth=2)\n",
    "    \n",
    "    if experiment_cleaned is not None:\n",
    "        plt.plot(experiment_df['Minutes'][:len(experiment_cleaned)], experiment_cleaned, \n",
    "                 label=f'{experiment_name.replace(\"_\", \" \").title()} TUNA Cleaned', \n",
    "                 linewidth=2, color='darkred')\n",
    "        \n",
    "        if experiment_outliers is not None:\n",
    "            outlier_points = np.where(experiment_outliers)[0]\n",
    "            if len(outlier_points) > 0:\n",
    "                plt.scatter(experiment_df['Minutes'].iloc[outlier_points], \n",
    "                           experiment_values[outlier_points], \n",
    "                           c='red', s=15, alpha=0.7, label='Outliers', zorder=5)\n",
    "        \n",
    "        title_suffix = \" (with TUNA cleaning)\"\n",
    "    else:\n",
    "        plt.plot(experiment_df['Minutes'][:len(experiment_values)], experiment_values, \n",
    "                 label=f'{experiment_name.replace(\"_\", \" \").title()}', \n",
    "                 linewidth=2, color='darkred')\n",
    "        title_suffix = \"\"\n",
    "    \n",
    "    plt.axvspan(delay_minutes, delay_minutes + duration_minutes, \n",
    "                color='red', alpha=0.2, label=\"Stress/Delay Period\")\n",
    "    plt.title(f'{metric_name}: {experiment_name.replace(\"_\", \" \").title()} Data{title_suffix}', \n",
    "              fontsize=16, fontweight='bold')\n",
    "    plt.xlabel('Minutes', fontsize=14)\n",
    "    plt.ylabel(y_label, fontsize=14)\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tick_params(axis='both', which='major', labelsize=12)\n",
    "    \n",
    "    plt.subplot(4, 1, 3)\n",
    "    plt.plot(df_combined['Minutes'], df_combined['Baseline'], \n",
    "             label='Baseline', linewidth=3, color='blue', alpha=0.9)\n",
    "    plt.plot(df_combined['Minutes'], df_combined['Experiment'], \n",
    "             label=f'{experiment_name.replace(\"_\", \" \").title()}', linewidth=3, color='red', alpha=0.9)\n",
    "    plt.axvspan(delay_minutes, delay_minutes + duration_minutes, \n",
    "                color='red', alpha=0.2, label=\"Stress/Delay Period\")\n",
    "    plt.title(f\"{metric_name}: Baseline vs {experiment_name.replace('_', ' ').title()}\", \n",
    "              fontsize=16, fontweight='bold')\n",
    "    plt.xlabel(\"Minutes\", fontsize=14)\n",
    "    plt.ylabel(f\"Normalized {metric_name}\", fontsize=14)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.tick_params(axis='both', which='major', labelsize=12)\n",
    "    \n",
    "    plt.subplot(4, 3, 10)\n",
    "    box_data = pd.melt(df_combined[['Baseline', 'Experiment', 'phase']], \n",
    "                      id_vars=['phase'], var_name='source', value_name='value')\n",
    "    sns.boxplot(x='phase', y='value', hue='source', data=box_data, ax=plt.gca())\n",
    "    plt.title(f'{metric_name}: Distribution by Phase', fontsize=16, fontweight='bold')\n",
    "    plt.xlabel('Phase', fontsize=14)\n",
    "    plt.ylabel('Normalized Values', fontsize=14)\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.tick_params(axis='both', which='major', labelsize=12)\n",
    "    \n",
    "    plt.subplot(4, 3, 11)\n",
    "    phases_to_plot = [p for p in ['before', 'during', 'after'] if p in available_phases]\n",
    "    \n",
    "    if len(phases_to_plot) > 0:\n",
    "        x = np.arange(len(phases_to_plot))\n",
    "        width = 0.35\n",
    "        \n",
    "        baseline_means = [stats_baseline.loc[p, 'mean'] for p in phases_to_plot]\n",
    "        baseline_stds = [stats_baseline.loc[p, 'std'] for p in phases_to_plot]\n",
    "        experiment_means = [stats_experiment.loc[p, 'mean'] for p in phases_to_plot]\n",
    "        experiment_stds = [stats_experiment.loc[p, 'std'] for p in phases_to_plot]\n",
    "        \n",
    "        bars1 = plt.bar(x - width/2, baseline_means, width, \n",
    "                       label='Baseline', alpha=0.8, color='blue')\n",
    "        bars2 = plt.bar(x + width/2, experiment_means, width, \n",
    "                       label=f'{experiment_name.replace(\"_\", \" \").title()}', alpha=0.8, color='red')\n",
    "        \n",
    "        plt.errorbar(x - width/2, baseline_means, yerr=baseline_stds, \n",
    "                    fmt='none', ecolor='black', capsize=8, linewidth=3)\n",
    "        plt.errorbar(x + width/2, experiment_means, yerr=experiment_stds, \n",
    "                    fmt='none', ecolor='black', capsize=8, linewidth=3)\n",
    "        \n",
    "        plt.xticks(x, phases_to_plot)\n",
    "    else:\n",
    "        plt.text(0.5, 0.5, 'No phase data available', ha='center', va='center', \n",
    "                transform=plt.gca().transAxes, fontsize=14)\n",
    "    \n",
    "    plt.title(f'{metric_name}: Average by Phase', fontsize=16, fontweight='bold')\n",
    "    plt.xlabel('Phase', fontsize=14)\n",
    "    plt.ylabel('Mean Normalized Values', fontsize=14)\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    plt.tick_params(axis='both', which='major', labelsize=12)\n",
    "    \n",
    "    plt.subplot(4, 3, 12)\n",
    "    plt.plot(df_combined['Minutes'], df_combined['difference'], \n",
    "             color='purple', linewidth=2, alpha=0.8)\n",
    "    plt.axhline(y=0, color='black', linestyle='-', alpha=0.5, linewidth=1)\n",
    "    plt.axvspan(delay_minutes, delay_minutes + duration_minutes, \n",
    "                color='red', alpha=0.2)\n",
    "    plt.title(f'{metric_name}: {experiment_name.replace(\"_\", \" \").title()} - Baseline Difference', \n",
    "              fontsize=16, fontweight='bold')\n",
    "    plt.xlabel('Minutes', fontsize=14)\n",
    "    plt.ylabel('Difference', fontsize=14)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tick_params(axis='both', which='major', labelsize=12)\n",
    "    \n",
    "    plt.suptitle(f'{metric_name} - {experiment_name.replace(\"_\", \" \").title()} vs Baseline Analysis', \n",
    "                fontsize=22, fontweight='bold', y=0.98)\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=0.95, hspace=0.45, wspace=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"{metric_name.upper()} - {experiment_name.upper().replace('_', ' ')} VS BASELINE ANALYSIS\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    if tuna_results and metric_name in tuna_results:\n",
    "        print(f\"\\nTUNA CLEANING SUMMARY:\")\n",
    "        if experiment_name in tuna_results[metric_name]:\n",
    "            exp_stats = tuna_results[metric_name][experiment_name]['stats']\n",
    "            print(f\"  {experiment_name.upper().replace('_', ' ')}: {exp_stats['outliers']} outliers removed\")\n",
    "            print(f\"    Noise Reduction: {exp_stats['noise_reduction']:.1f}%, Correlation: {exp_stats['correlation']:.3f}\")\n",
    "        \n",
    "        if 'baseline' in tuna_results[metric_name]:\n",
    "            base_stats = tuna_results[metric_name]['baseline']['stats']\n",
    "            print(f\"  BASELINE: {base_stats['outliers']} outliers removed\")\n",
    "            print(f\"    Noise Reduction: {base_stats['noise_reduction']:.1f}%, Correlation: {base_stats['correlation']:.3f}\")\n",
    "    \n",
    "    print(f\"\\nBASELINE STATISTICS BY PHASE:\")\n",
    "    if len(stats_baseline) > 0:\n",
    "        print(stats_baseline.round(4))\n",
    "    else:\n",
    "        print(\"  No phase data available\")\n",
    "    \n",
    "    print(f\"\\n{experiment_name.upper().replace('_', ' ')} STATISTICS BY PHASE:\")\n",
    "    if len(stats_experiment) > 0:\n",
    "        print(stats_experiment.round(4))\n",
    "    else:\n",
    "        print(\"  No phase data available\")\n",
    "    \n",
    "    print(f\"\\nSTATISTICAL SIGNIFICANCE TESTS:\")\n",
    "    for phase, test in t_tests.items():\n",
    "        if 'note' in test:\n",
    "            print(f\"  {phase.upper():8}: {test['note']}\")\n",
    "        else:\n",
    "            sig = \"SIGNIFICANT\" if test['p'] < 0.05 else \"NOT SIGNIFICANT\"\n",
    "            print(f\"  {phase.upper():8}: t={test['t']:6.2f}, p={test['p']:8.4f} ({sig})\")\n",
    "    \n",
    "    print(f\"\\n{experiment_name.upper().replace('_', ' ')} IMPACT:\")\n",
    "    for phase, change in impact.items():\n",
    "        if change == 0 and phase not in available_phases:\n",
    "            print(f\"  {phase.upper():8}: Phase not present in data\")\n",
    "        else:\n",
    "            direction = \"INCREASE\" if change > 0 else \"DECREASE\" if change < 0 else \"NO CHANGE\"\n",
    "            print(f\"  {phase.upper():8}: {change:+7.2f}% ({direction})\")\n",
    "    \n",
    "    print(f\"\\nMEMORY METRICS DETAILS:\")\n",
    "    print(f\"  Metric: {metric_name}\")\n",
    "    print(f\"  Metric type: {y_label}\")\n",
    "    print(f\"  Data points analyzed: {len(df_combined)}\")\n",
    "    \n",
    "    return {\n",
    "        'stats_baseline': stats_baseline,\n",
    "        'stats_experiment': stats_experiment,\n",
    "        't_tests': t_tests,\n",
    "        'impact': impact,\n",
    "        'combined_data': df_combined,\n",
    "        'tuna_info': {\n",
    "            'experiment_cleaned': experiment_cleaned is not None,\n",
    "            'baseline_cleaned': baseline_cleaned is not None\n",
    "        }\n",
    "    }\n",
    "\n",
    "def run_comprehensive_memory_analysis(all_datasets, tuna_results=None, delay_minutes=30, duration_minutes=50):\n",
    "    analysis_results = {}\n",
    "    \n",
    "    for metric_name, experiments in all_datasets.items():\n",
    "        print(f\"\\n🔍 Analyzing {metric_name}...\")\n",
    "        analysis_results[metric_name] = {}\n",
    "        \n",
    "        baseline_df = experiments['baseline']\n",
    "        \n",
    "        for experiment_name, experiment_df in experiments.items():\n",
    "            if experiment_name != 'baseline':\n",
    "                print(f\"\\n📊 Comparing {metric_name}: {experiment_name} stress vs baseline\")\n",
    "                \n",
    "                result = analyze_and_plot_memory_comparison(\n",
    "                    metric_name=metric_name,\n",
    "                    experiment_name=experiment_name,\n",
    "                    experiment_df=experiment_df,\n",
    "                    baseline_df=baseline_df,\n",
    "                    tuna_results=tuna_results,\n",
    "                    delay_minutes=delay_minutes,\n",
    "                    duration_minutes=duration_minutes\n",
    "                )\n",
    "                \n",
    "                analysis_results[metric_name][experiment_name] = result\n",
    "    \n",
    "    return analysis_results\n",
    "\n",
    "memory_results = run_comprehensive_memory_analysis(all_datasets, tuna_results, delay_minutes=30, duration_minutes=50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

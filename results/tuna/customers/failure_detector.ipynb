{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a6b574",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_auc_score, roc_curve, precision_recall_curve\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "root = Path(\"unified\")\n",
    "\n",
    "# Dizionario dove salviamo tutti i dataset\n",
    "all_datasets = {}\n",
    "\n",
    "for csv_file in root.rglob(\"all_metrics_combined*.csv\"):\n",
    "    experiment_name = csv_file.parent.name  # nome della cartella esperimento\n",
    "    df = pd.read_csv(csv_file)\n",
    "    df[\"source\"] = experiment_name.upper()  # aggiungiamo la colonna \"source\"\n",
    "    all_datasets[experiment_name] = df\n",
    "    print(f\"Caricato cutomers {experiment_name} experiment ({len(df)} righe)\")\n",
    "\n",
    "# Se vuoi un unico DataFrame con tutto insieme\n",
    "df_all = pd.concat(all_datasets.values(), ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfde99dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_simple_nan_histogram(df, title=\"NaN Count Histogram\"):\n",
    "\n",
    "    nan_counts = df.isnull().sum()\n",
    "    \n",
    "    has_nan = nan_counts[nan_counts > 0]\n",
    "    \n",
    "    if len(has_nan) == 0:\n",
    "        print(\"Nessun valore NaN trovato nel dataset\")\n",
    "        return\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.bar(range(len(has_nan)), has_nan.values, color='red', alpha=0.7)\n",
    "    \n",
    "    plt.title(title, fontsize=16, fontweight='bold')\n",
    "    plt.xlabel('Features', fontsize=12)\n",
    "    plt.ylabel('NaN Count', fontsize=12)\n",
    "    plt.xticks(range(len(has_nan)), has_nan.index, rotation=45, ha='right')\n",
    "    \n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Stampa anche un summary\n",
    "    print(f\"Totale features con NaN: {len(has_nan)}\")\n",
    "    print(f\"Totale NaN nel dataset: {has_nan.sum()}\")\n",
    "\n",
    "def plot_nan_distribution_histogram(df, title=\"NaN Distribution per Row\"):\n",
    "    \"\"\"\n",
    "    Plotta la distribuzione del numero di NaN per riga\n",
    "    \"\"\"\n",
    "    # Calcola quanti NaN ci sono per ogni riga\n",
    "    nan_per_row = df.isnull().sum(axis=1)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Istogramma vero e proprio\n",
    "    plt.hist(nan_per_row, bins=30, color='purple', alpha=0.7, edgecolor='black')\n",
    "    \n",
    "    plt.title(title, fontsize=16, fontweight='bold')\n",
    "    plt.xlabel('Numero di NaN per riga', fontsize=12)\n",
    "    plt.ylabel('Frequenza', fontsize=12)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Statistics\n",
    "    print(f\"Statistiche NaN per riga:\")\n",
    "    print(f\"Media: {nan_per_row.mean():.2f}\")\n",
    "    print(f\"Mediana: {nan_per_row.median():.2f}\")\n",
    "    print(f\"Massimo: {nan_per_row.max()}\")\n",
    "    print(f\"Righe senza NaN: {(nan_per_row == 0).sum()}\")\n",
    "\n",
    "def plot_nan_histogram_all_experiments(all_datasets):\n",
    "    \"\"\"\n",
    "    Plotta l'istogramma dei NaN per tutti gli esperimenti\n",
    "    \"\"\"\n",
    "    experiments = list(all_datasets.keys())\n",
    "    n_experiments = len(experiments)\n",
    "    \n",
    "    # Calcola dimensioni griglia\n",
    "    if n_experiments <= 4:\n",
    "        rows, cols = 2, 2\n",
    "        figsize = (16, 10)\n",
    "    else:\n",
    "        rows = int(np.ceil(n_experiments / 2))\n",
    "        cols = 2\n",
    "        figsize = (16, 5*rows)\n",
    "    \n",
    "    fig, axes = plt.subplots(rows, cols, figsize=figsize)\n",
    "    \n",
    "    # Se c'è solo un subplot, converti in lista\n",
    "    if n_experiments == 1:\n",
    "        axes = [axes]\n",
    "    else:\n",
    "        axes = axes.flatten()\n",
    "    \n",
    "    for i, exp_name in enumerate(experiments):\n",
    "        if i < len(axes):\n",
    "            ax = axes[i]\n",
    "            df = all_datasets[exp_name]\n",
    "            \n",
    "            # Calcola NaN count\n",
    "            nan_counts = df.isnull().sum()\n",
    "            has_nan = nan_counts[nan_counts > 0]\n",
    "            \n",
    "            if len(has_nan) > 0:\n",
    "                # Plot bar chart\n",
    "                bars = ax.bar(range(len(has_nan)), has_nan.values, color='red', alpha=0.7)\n",
    "                \n",
    "                ax.set_title(f'NaN Count - {exp_name.upper()}', fontsize=14, fontweight='bold')\n",
    "                ax.set_xlabel('Features', fontsize=10)\n",
    "                ax.set_ylabel('NaN Count', fontsize=10)\n",
    "                \n",
    "                # Abbrevia i nomi delle features per leggibilità\n",
    "                feature_names = []\n",
    "                for name in has_nan.index:\n",
    "                    if len(name) > 12:\n",
    "                        # Prendi prime 12 caratteri\n",
    "                        short_name = name[:12] + \"...\"\n",
    "                    else:\n",
    "                        short_name = name\n",
    "                    feature_names.append(short_name)\n",
    "                \n",
    "                ax.set_xticks(range(len(has_nan)))\n",
    "                ax.set_xticklabels(feature_names, rotation=45, ha='right', fontsize=8)\n",
    "                \n",
    "                ax.grid(True, alpha=0.3)\n",
    "                \n",
    "                # Aggiungi valori sopra le barre se sono poche\n",
    "                if len(has_nan) <= 8:\n",
    "                    for bar, value in zip(bars, has_nan.values):\n",
    "                        ax.text(bar.get_x() + bar.get_width()/2, \n",
    "                               bar.get_height() + max(has_nan.values)*0.01,\n",
    "                               f'{int(value)}', ha='center', va='bottom', fontsize=8)\n",
    "                \n",
    "                # Summary testuale nell'angolo\n",
    "                total_nan = has_nan.sum()\n",
    "                ax.text(0.02, 0.98, f'Total NaN: {total_nan}', transform=ax.transAxes, \n",
    "                       fontsize=10, verticalalignment='top', \n",
    "                       bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"yellow\", alpha=0.7))\n",
    "            else:\n",
    "                # Nessun NaN trovato\n",
    "                ax.text(0.5, 0.5, f'No NaN\\nin {exp_name.upper()}', \n",
    "                       ha='center', va='center', transform=ax.transAxes, \n",
    "                       fontsize=16, bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightgreen\", alpha=0.7))\n",
    "                ax.set_title(f'NaN Count - {exp_name.upper()}', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # Nascondi subplot vuoti\n",
    "    for j in range(len(experiments), len(axes)):\n",
    "        axes[j].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Summary generale\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"SUMMARY NaN PER ESPERIMENTO\")\n",
    "    print(f\"{'='*60}\")\n",
    "    for exp_name, df in all_datasets.items():\n",
    "        nan_counts = df.isnull().sum()\n",
    "        has_nan = nan_counts[nan_counts > 0]\n",
    "        total_nan = has_nan.sum() if len(has_nan) > 0 else 0\n",
    "        features_with_nan = len(has_nan)\n",
    "        \n",
    "        print(f\"{exp_name.upper():<15}: {total_nan:>6} NaN totali, {features_with_nan:>3} features con NaN\")\n",
    "\n",
    "# Esempi di utilizzo:\n",
    "\n",
    "# Tutti gli esperimenti in una dashboard\n",
    "plot_nan_histogram_all_experiments(all_datasets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78e7234",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def normalize_0_100(series: pd.Series) -> pd.Series:\n",
    "    min_val = series.min()\n",
    "    max_val = series.max()\n",
    "    if pd.isna(min_val) or pd.isna(max_val) or min_val == max_val:\n",
    "        return pd.Series([0] * len(series), index=series.index)\n",
    "    return (series - min_val) / (max_val - min_val) * 100\n",
    "\n",
    "def process_series_for_plot(series: pd.Series, normalize: bool = True):\n",
    "    if normalize:\n",
    "        return normalize_0_100(series)\n",
    "    else:\n",
    "        return series\n",
    "\n",
    "def calculate_cpu_cumulative(df: pd.DataFrame, metric_type: str):\n",
    "    cpu_cols = [col for col in df.columns if metric_type.lower() in col.lower() \n",
    "                and 'cpu' in col.lower() and 'minutes' not in col.lower()]\n",
    "    \n",
    "    if not cpu_cols:\n",
    "        return None\n",
    "    \n",
    "    return df[cpu_cols].sum(axis=1)\n",
    "\n",
    "def calculate_tcp_non_srtt_cumulative(df: pd.DataFrame):\n",
    "    tcp_cols = []\n",
    "    for col in df.columns:\n",
    "        col_lower = col.lower()\n",
    "        if any(tcp_term in col_lower for tcp_term in ['api', 'service', 'gateway', 'customer', 'vet', 'visit']) and 'srtt' not in col_lower and 'minutes' not in col_lower:\n",
    "            tcp_cols.append(col)\n",
    "    \n",
    "    if not tcp_cols:\n",
    "        return None\n",
    "    \n",
    "    return df[tcp_cols].sum(axis=1)\n",
    "\n",
    "def calculate_total_network_traffic(df: pd.DataFrame):\n",
    "    srtt_cols = [col for col in df.columns if 'srtt' in col.lower() \n",
    "                 and 'minutes' not in col.lower()]\n",
    "    \n",
    "    if not srtt_cols:\n",
    "        return None\n",
    "    \n",
    "    return df[srtt_cols].sum(axis=1)\n",
    "\n",
    "def plot_cpu_cumulative_thin(df: pd.DataFrame, time_col='minutes', normalize=True):\n",
    "    if time_col not in df.columns:\n",
    "        return\n",
    "    \n",
    "    cpu_metrics = ['iowait', 'irq', 'system', 'user', 'utilization']\n",
    "    colors = ['red', 'blue', 'green', 'orange', 'purple']\n",
    "    \n",
    "    plt.figure(figsize=(14, 8))\n",
    "    plt.axvspan(30, 80, alpha=0.2, color='red', label='Stress')\n",
    "    \n",
    "    for i, metric in enumerate(cpu_metrics):\n",
    "        cumulative = calculate_cpu_cumulative(df, metric)\n",
    "        if cumulative is not None:\n",
    "            processed_series = process_series_for_plot(cumulative, normalize)\n",
    "            plt.plot(df[time_col], processed_series, \n",
    "                    label=f'CPU {metric} (cumulativo)', \n",
    "                    linewidth=1.5,\n",
    "                    color=colors[i % len(colors)])\n",
    "    \n",
    "    title_suffix = \"Normalizzate (0-100)\" if normalize else \"Valori Originali\"\n",
    "    ylabel = \"Valore normalizzato (0-100)\" if normalize else \"Valore\"\n",
    "    \n",
    "    plt.title(f\"Metriche CPU Cumulative - {title_suffix}\", fontsize=16, fontweight='bold')\n",
    "    plt.xlabel(\"Tempo (minuti)\", fontsize=12)\n",
    "    plt.ylabel(ylabel, fontsize=12)\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=10)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    if normalize:\n",
    "        plt.ylim(0, 100)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_memory_metrics(df: pd.DataFrame, time_col='minutes', normalize=True):\n",
    "    if time_col not in df.columns:\n",
    "        return\n",
    "    \n",
    "    mem_available = [col for col in df.columns if 'memavailable' in col.lower() \n",
    "                     and 'minutes' not in col.lower()]\n",
    "    mem_util = [col for col in df.columns if 'memutil' in col.lower() \n",
    "                and 'minutes' not in col.lower()]\n",
    "    mem_cache = [col for col in df.columns if 'memcache' in col.lower() \n",
    "                 and 'minutes' not in col.lower()]\n",
    "    \n",
    "    plt.figure(figsize=(14, 8))\n",
    "    plt.axvspan(30, 80, alpha=0.2, color='red', label='Stress')\n",
    "    \n",
    "    colors = ['blue', 'green', 'orange']\n",
    "    labels = ['Memory Available', 'Memory Util', 'Memory Cache']\n",
    "    metric_groups = [mem_available, mem_util, mem_cache]\n",
    "    \n",
    "    for i, (metrics, label, color) in enumerate(zip(metric_groups, labels, colors)):\n",
    "        for col in metrics:\n",
    "            if col in df.columns:\n",
    "                processed_series = process_series_for_plot(df[col], normalize)\n",
    "                plt.plot(df[time_col], processed_series, \n",
    "                        label=label, \n",
    "                        linewidth=1.5,\n",
    "                        color=color)\n",
    "                break\n",
    "    \n",
    "    title_suffix = \"Normalizzate (0-100)\" if normalize else \"Valori Originali\"\n",
    "    ylabel = \"Valore normalizzato (0-100)\" if normalize else \"Valore\"\n",
    "    \n",
    "    plt.title(f\"Metriche Memory - {title_suffix}\", fontsize=16, fontweight='bold')\n",
    "    plt.xlabel(\"Tempo (minuti)\", fontsize=12)\n",
    "    plt.ylabel(ylabel, fontsize=12)\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=10)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    if normalize:\n",
    "        plt.ylim(0, 100)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_tcp_network_traffic(df: pd.DataFrame, time_col='minutes', normalize=True):\n",
    "    if time_col not in df.columns:\n",
    "        return\n",
    "    \n",
    "    total_network_traffic = calculate_total_network_traffic(df)\n",
    "    retransmitted_packets = calculate_tcp_non_srtt_cumulative(df)\n",
    "    \n",
    "    if total_network_traffic is None and retransmitted_packets is None:\n",
    "        return\n",
    "    \n",
    "    plt.figure(figsize=(14, 8))\n",
    "    plt.axvspan(30, 80, alpha=0.2, color='red', label='Stress')\n",
    "    \n",
    "    if total_network_traffic is not None:\n",
    "        processed_traffic = process_series_for_plot(total_network_traffic, normalize)\n",
    "        plt.plot(df[time_col], processed_traffic, \n",
    "                label='Total Network Traffic', \n",
    "                linewidth=1.5,\n",
    "                color='orange')\n",
    "    \n",
    "    if retransmitted_packets is not None:\n",
    "        processed_packets = process_series_for_plot(retransmitted_packets, normalize)\n",
    "        plt.plot(df[time_col], processed_packets, \n",
    "                label='Retransmitted Packets', \n",
    "                linewidth=1.5,\n",
    "                color='purple')\n",
    "    \n",
    "    title_suffix = \"Normalized (0-100)\" if normalize else \"Original Values\"\n",
    "    ylabel = \"Normalized Value (0-100)\" if normalize else \"Value\"\n",
    "    \n",
    "    plt.title(f\"Network Traffic Metrics - {title_suffix}\", fontsize=16, fontweight='bold')\n",
    "    plt.xlabel(\"Time (minutes)\", fontsize=12)\n",
    "    plt.ylabel(ylabel, fontsize=12)\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=10)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    if normalize:\n",
    "        plt.ylim(0, 100)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_io_readwrite(df: pd.DataFrame, time_col='minutes', normalize=True):\n",
    "    if time_col not in df.columns:\n",
    "        return\n",
    "    \n",
    "    read_cols = [col for col in df.columns if 'readbytes' in col.lower() \n",
    "                 and 'minutes' not in col.lower()]\n",
    "    write_cols = [col for col in df.columns if 'writebytes' in col.lower() \n",
    "                  and 'minutes' not in col.lower()]\n",
    "    \n",
    "    if not read_cols and not write_cols:\n",
    "        return\n",
    "    \n",
    "    plt.figure(figsize=(14, 8))\n",
    "    plt.axvspan(30, 80, alpha=0.2, color='red', label='Stress')\n",
    "    \n",
    "    if read_cols:\n",
    "        read_cumulative = df[read_cols].sum(axis=1)\n",
    "        processed_read = process_series_for_plot(read_cumulative, normalize)\n",
    "        plt.plot(df[time_col], processed_read, \n",
    "                label='Read Bytes (cumulativo)', \n",
    "                linewidth=1.5,\n",
    "                color='green')\n",
    "    \n",
    "    if write_cols:\n",
    "        write_cumulative = df[write_cols].sum(axis=1)\n",
    "        processed_write = process_series_for_plot(write_cumulative, normalize)\n",
    "        plt.plot(df[time_col], processed_write, \n",
    "                label='Write Bytes (cumulativo)', \n",
    "                linewidth=1.5,\n",
    "                color='red')\n",
    "    \n",
    "    title_suffix = \"Normalizzate (0-100)\" if normalize else \"Valori Originali\"\n",
    "    ylabel = \"Valore normalizzato (0-100)\" if normalize else \"Valore\"\n",
    "    \n",
    "    plt.title(f\"Metriche IO - Read/Write Bytes Cumulative - {title_suffix}\", \n",
    "              fontsize=16, fontweight='bold')\n",
    "    plt.xlabel(\"Tempo (minuti)\", fontsize=12)\n",
    "    plt.ylabel(ylabel, fontsize=12)\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=10)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    if normalize:\n",
    "        plt.ylim(0, 100)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_dashboard(df: pd.DataFrame, time_col='minutes', normalize=True):\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(20, 12))\n",
    "    \n",
    "    title_suffix = \"Normalized (0-100)\" if normalize else \"Original Values\"\n",
    "    ylabel = \"Normalized Value (0-100)\" if normalize else \"Value\"\n",
    "    \n",
    "    ax1 = axes[0, 0]\n",
    "    ax1.axvspan(30, 80, alpha=0.2, color='red', label='Stress')\n",
    "    \n",
    "    cpu_metrics = ['iowait', 'irq', 'system', 'user', 'utilization']\n",
    "    colors = ['red', 'blue', 'green', 'orange', 'purple']\n",
    "    \n",
    "    for i, metric in enumerate(cpu_metrics):\n",
    "        cumulative = calculate_cpu_cumulative(df, metric)\n",
    "        if cumulative is not None:\n",
    "            processed_series = process_series_for_plot(cumulative, normalize)\n",
    "            ax1.plot(df[time_col], processed_series, \n",
    "                    label=f'CPU {metric}', \n",
    "                    linewidth=1.5,\n",
    "                    color=colors[i % len(colors)])\n",
    "    \n",
    "    ax1.set_title(f\"CPU Cumulative - {title_suffix}\", fontsize=14, fontweight='bold')\n",
    "    ax1.set_xlabel(\"Minutes\", fontsize=10)\n",
    "    ax1.set_ylabel(ylabel, fontsize=10)\n",
    "    ax1.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=8)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    if normalize:\n",
    "        ax1.set_ylim(0, 100)\n",
    "    \n",
    "    ax2 = axes[0, 1]\n",
    "    ax2.axvspan(30, 80, alpha=0.2, color='red', label='Stress')\n",
    "    \n",
    "    mem_available = [col for col in df.columns if 'memavailable' in col.lower() \n",
    "                     and 'minutes' not in col.lower()]\n",
    "    mem_util = [col for col in df.columns if 'memutil' in col.lower() \n",
    "                and 'minutes' not in col.lower()]\n",
    "    mem_cache = [col for col in df.columns if 'memcache' in col.lower() \n",
    "                 and 'minutes' not in col.lower()]\n",
    "    \n",
    "    colors = ['blue', 'green', 'orange']\n",
    "    labels = ['Memory Available', 'Memory Util', 'Memory Cache']\n",
    "    metric_groups = [mem_available, mem_util, mem_cache]\n",
    "    \n",
    "    for i, (metrics, label, color) in enumerate(zip(metric_groups, labels, colors)):\n",
    "        for col in metrics:\n",
    "            if col in df.columns:\n",
    "                processed_series = process_series_for_plot(df[col], normalize)\n",
    "                ax2.plot(df[time_col], processed_series, \n",
    "                        label=label, \n",
    "                        linewidth=1.5,\n",
    "                        color=color)\n",
    "                break\n",
    "    \n",
    "    ax2.set_title(f\"Memory - {title_suffix}\", fontsize=14, fontweight='bold')\n",
    "    ax2.set_xlabel(\"Minutes\", fontsize=10)\n",
    "    ax2.set_ylabel(ylabel, fontsize=10)\n",
    "    ax2.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=8)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    if normalize:\n",
    "        ax2.set_ylim(0, 100)\n",
    "    \n",
    "    ax3 = axes[1, 0]\n",
    "    ax3.axvspan(30, 80, alpha=0.2, color='red', label='Stress')\n",
    "    \n",
    "    total_network_traffic = calculate_total_network_traffic(df)\n",
    "    retransmitted_packets = calculate_tcp_non_srtt_cumulative(df)\n",
    "    \n",
    "    if total_network_traffic is not None:\n",
    "        processed_traffic = process_series_for_plot(total_network_traffic, normalize)\n",
    "        ax3.plot(df[time_col], processed_traffic, \n",
    "                label='Total Network Traffic', \n",
    "                linewidth=1.5,\n",
    "                color='orange')\n",
    "    \n",
    "    if retransmitted_packets is not None:\n",
    "        processed_packets = process_series_for_plot(retransmitted_packets, normalize)\n",
    "        ax3.plot(df[time_col], processed_packets, \n",
    "                label='Retransmitted Packets', \n",
    "                linewidth=1.5,\n",
    "                color='purple')\n",
    "    \n",
    "    ax3.set_title(f\"Network Traffic - {title_suffix}\", fontsize=14, fontweight='bold')\n",
    "    ax3.set_xlabel(\"Minutes\", fontsize=10)\n",
    "    ax3.set_ylabel(ylabel, fontsize=10)\n",
    "    ax3.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=8)\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    if normalize:\n",
    "        ax3.set_ylim(0, 100)\n",
    "    \n",
    "    ax4 = axes[1, 1]\n",
    "    ax4.axvspan(30, 80, alpha=0.2, color='red', label='Stress')\n",
    "    \n",
    "    read_cols = [col for col in df.columns if 'readbytes' in col.lower() \n",
    "                 and 'minutes' not in col.lower()]\n",
    "    write_cols = [col for col in df.columns if 'writebytes' in col.lower() \n",
    "                  and 'minutes' not in col.lower()]\n",
    "    \n",
    "    if read_cols:\n",
    "        read_cumulative = df[read_cols].sum(axis=1)\n",
    "        processed_read = process_series_for_plot(read_cumulative, normalize)\n",
    "        ax4.plot(df[time_col], processed_read, \n",
    "                label='Read Bytes', \n",
    "                linewidth=1.5,\n",
    "                color='green')\n",
    "    \n",
    "    if write_cols:\n",
    "        write_cumulative = df[write_cols].sum(axis=1)\n",
    "        processed_write = process_series_for_plot(write_cumulative, normalize)\n",
    "        ax4.plot(df[time_col], processed_write, \n",
    "                label='Write Bytes', \n",
    "                linewidth=1.5,\n",
    "                color='red')\n",
    "    \n",
    "    ax4.set_title(f\"IO Read/Write - {title_suffix}\", fontsize=14, fontweight='bold')\n",
    "    ax4.set_xlabel(\"Minutes\", fontsize=10)\n",
    "    ax4.set_ylabel(ylabel, fontsize=10)\n",
    "    ax4.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=8)\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    if normalize:\n",
    "        ax4.set_ylim(0, 100)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_all_simplified_metrics(df: pd.DataFrame, time_col='minutes', normalize=True):\n",
    "    plot_cpu_cumulative_thin(df, time_col, normalize)\n",
    "    plot_memory_metrics(df, time_col, normalize)\n",
    "    plot_tcp_network_traffic(df, time_col, normalize)\n",
    "    plot_io_readwrite(df, time_col, normalize)\n",
    "\n",
    "def compare_normalized_vs_original(df, time_col='minutes'):\n",
    "    plot_dashboard(df, time_col, normalize=True)\n",
    "    plot_dashboard(df, time_col, normalize=False)\n",
    "plot_dashboard(all_datasets['baseline'])\n",
    "\n",
    "# Dashboard con valori originali  \n",
    "plot_dashboard(all_datasets['baseline'], normalize=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4f755d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def clean_dataset(df):\n",
    "    df = df.copy()\n",
    "    \n",
    "    cpu_8_9_cols = [col for col in df.columns if 'cpu_8' in col or 'cpu_9' in col or 'cpu 8' in col or 'cpu 9' in col]\n",
    "    if cpu_8_9_cols:\n",
    "        df = df.drop(columns=cpu_8_9_cols)\n",
    "    \n",
    "    tcp_cols = [col for col in df.columns if any(term in col.lower() for term in ['api', 'service', 'gateway', 'customer', 'vet', 'visit']) and 'srtt' not in col.lower()]\n",
    "    for col in tcp_cols:\n",
    "        df[col] = df[col].fillna(0)\n",
    "    \n",
    "    other_cols = [col for col in df.columns if col not in tcp_cols and df[col].isnull().sum() > 0]\n",
    "    for col in other_cols:\n",
    "        df[col] = df[col].interpolate(method='linear').fillna(method='bfill').fillna(method='ffill')\n",
    "    \n",
    "    return df\n",
    "\n",
    "def clean_all_datasets(all_datasets):\n",
    "    processed = {}\n",
    "    for name, df in all_datasets.items():\n",
    "        original_shape = df.shape\n",
    "        processed_df = clean_dataset(df)\n",
    "        processed[name] = processed_df\n",
    "        \n",
    "        # Show summary\n",
    "        dropped_cols = original_shape[1] - processed_df.shape[1]\n",
    "        nan_count = processed_df.isnull().sum().sum()\n",
    "        print(f\"  {name}: {original_shape} → {processed_df.shape} | Dropped: {dropped_cols} | NaN: {nan_count}\")\n",
    "    \n",
    "    print(\"✅ Done!\")\n",
    "    return processed\n",
    "\n",
    "\n",
    "all_datasets = clean_all_datasets(all_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff86bb0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_0_100(series: pd.Series) -> pd.Series:\n",
    "    \"\"\"Normalizza una serie tra 0 e 100\"\"\"\n",
    "    min_val = series.min()\n",
    "    max_val = series.max()\n",
    "    if pd.isna(min_val) or pd.isna(max_val) or min_val == max_val:\n",
    "        return pd.Series([0] * len(series), index=series.index)\n",
    "    return (series - min_val) / (max_val - min_val) * 100\n",
    "\n",
    "def process_series_for_plot(series: pd.Series, normalize: bool = True):\n",
    "    \"\"\"Processa una serie per il plot in base alle opzioni scelte\"\"\"\n",
    "    if normalize:\n",
    "        return normalize_0_100(series)\n",
    "    else:\n",
    "        return series\n",
    "\n",
    "def calculate_cpu_cumulative(df: pd.DataFrame, metric_type: str):\n",
    "    \"\"\"Calcola il cumulativo di una specifica metrica CPU attraverso tutti i core\"\"\"\n",
    "    cpu_cols = [col for col in df.columns if metric_type.lower() in col.lower() \n",
    "                and 'cpu' in col.lower() and 'minutes' not in col.lower()]\n",
    "    \n",
    "    if not cpu_cols:\n",
    "        return None\n",
    "    \n",
    "    return df[cpu_cols].sum(axis=1)\n",
    "\n",
    "def calculate_tcp_non_srtt_cumulative(df: pd.DataFrame):\n",
    "    \"\"\"Calcola il cumulativo delle metriche TCP non-SRTT (retransmitted packets)\"\"\"\n",
    "    tcp_cols = []\n",
    "    for col in df.columns:\n",
    "        col_lower = col.lower()\n",
    "        # Cerca metriche TCP/network ma escludi SRTT e Minutes\n",
    "        if any(tcp_term in col_lower for tcp_term in ['api', 'service', 'gateway', 'customer', 'vet', 'visit']) and 'srtt' not in col_lower and 'minutes' not in col_lower:\n",
    "            tcp_cols.append(col)\n",
    "    \n",
    "    if not tcp_cols:\n",
    "        return None\n",
    "    \n",
    "    return df[tcp_cols].sum(axis=1)\n",
    "\n",
    "def calculate_total_network_traffic(df: pd.DataFrame):\n",
    "    \"\"\"Calcola il Total Network Traffic (solo SRTT)\"\"\"\n",
    "    srtt_cols = [col for col in df.columns if 'srtt' in col.lower() \n",
    "                 and 'minutes' not in col.lower()]\n",
    "    \n",
    "    if not srtt_cols:\n",
    "        return None\n",
    "    \n",
    "    return df[srtt_cols].sum(axis=1)\n",
    "\n",
    "def plot_cpu_cumulative_thin(df: pd.DataFrame, time_col='minutes', normalize=True):\n",
    "    \"\"\"Plot CPU cumulative con linee sottili\"\"\"\n",
    "    if time_col not in df.columns:\n",
    "        print(f\"Attenzione: colonna '{time_col}' non trovata nel DataFrame\")\n",
    "        return\n",
    "    \n",
    "    cpu_metrics = ['iowait', 'irq', 'system', 'user', 'utilization']\n",
    "    colors = ['red', 'blue', 'green', 'orange', 'purple']\n",
    "    \n",
    "    plt.figure(figsize=(14, 8))\n",
    "    plt.axvspan(30, 80, alpha=0.2, color='red', label='Stress')\n",
    "    \n",
    "    for i, metric in enumerate(cpu_metrics):\n",
    "        cumulative = calculate_cpu_cumulative(df, metric)\n",
    "        if cumulative is not None:\n",
    "            processed_series = process_series_for_plot(cumulative, normalize)\n",
    "            plt.plot(df[time_col], processed_series, \n",
    "                    label=f'CPU {metric} (cumulativo)', \n",
    "                    linewidth=1.5,\n",
    "                    color=colors[i % len(colors)])\n",
    "    \n",
    "    title_suffix = \"Normalizzate (0-100)\" if normalize else \"Valori Originali\"\n",
    "    ylabel = \"Valore normalizzato (0-100)\" if normalize else \"Valore\"\n",
    "    \n",
    "    plt.title(f\"Metriche CPU Cumulative - {title_suffix}\", fontsize=16, fontweight='bold')\n",
    "    plt.xlabel(\"Tempo (minuti)\", fontsize=12)\n",
    "    plt.ylabel(ylabel, fontsize=12)\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=10)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    if normalize:\n",
    "        plt.ylim(0, 100)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_memory_metrics(df: pd.DataFrame, time_col='minutes', normalize=True):\n",
    "    \"\"\"Plot metriche Memory: mem_available, mem_util, mem_cache\"\"\"\n",
    "    if time_col not in df.columns:\n",
    "        print(f\"Attenzione: colonna '{time_col}' non trovata nel DataFrame\")\n",
    "        return\n",
    "    \n",
    "    mem_available = [col for col in df.columns if 'memavailable' in col.lower() \n",
    "                     and 'minutes' not in col.lower()]\n",
    "    mem_util = [col for col in df.columns if 'memutil' in col.lower() \n",
    "                and 'minutes' not in col.lower()]\n",
    "    mem_cache = [col for col in df.columns if 'memcache' in col.lower() \n",
    "                 and 'minutes' not in col.lower()]\n",
    "    \n",
    "    plt.figure(figsize=(14, 8))\n",
    "    plt.axvspan(30, 80, alpha=0.2, color='red', label='Stress')\n",
    "    \n",
    "    colors = ['blue', 'green', 'orange']\n",
    "    labels = ['Memory Available', 'Memory Util', 'Memory Cache']\n",
    "    metric_groups = [mem_available, mem_util, mem_cache]\n",
    "    \n",
    "    for i, (metrics, label, color) in enumerate(zip(metric_groups, labels, colors)):\n",
    "        for col in metrics:\n",
    "            if col in df.columns:\n",
    "                processed_series = process_series_for_plot(df[col], normalize)\n",
    "                plt.plot(df[time_col], processed_series, \n",
    "                        label=label, \n",
    "                        linewidth=1.5,\n",
    "                        color=color)\n",
    "                break\n",
    "    \n",
    "    title_suffix = \"Normalizzate (0-100)\" if normalize else \"Valori Originali\"\n",
    "    ylabel = \"Valore normalizzato (0-100)\" if normalize else \"Valore\"\n",
    "    \n",
    "    plt.title(f\"Metriche Memory - {title_suffix}\", fontsize=16, fontweight='bold')\n",
    "    plt.xlabel(\"Tempo (minuti)\", fontsize=12)\n",
    "    plt.ylabel(ylabel, fontsize=12)\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=10)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    if normalize:\n",
    "        plt.ylim(0, 100)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_tcp_network_traffic(df: pd.DataFrame, time_col='minutes', normalize=True):\n",
    "    \"\"\"Plot Network Traffic: Total Network Traffic (SRTT) e Retransmitted Packets\"\"\"\n",
    "    if time_col not in df.columns:\n",
    "        print(f\"Attenzione: colonna '{time_col}' non trovata nel DataFrame\")\n",
    "        return\n",
    "    \n",
    "    # Calcola Total Network Traffic (SRTT)\n",
    "    total_network_traffic = calculate_total_network_traffic(df)\n",
    "    \n",
    "    # Calcola Retransmitted Packets (TCP non-SRTT)\n",
    "    retransmitted_packets = calculate_tcp_non_srtt_cumulative(df)\n",
    "    \n",
    "    if total_network_traffic is None and retransmitted_packets is None:\n",
    "        print(\"Nessuna metrica TCP/Network trovata\")\n",
    "        return\n",
    "    \n",
    "    plt.figure(figsize=(14, 8))\n",
    "    plt.axvspan(30, 80, alpha=0.2, color='red', label='Stress')\n",
    "    \n",
    "    # Plot Total Network Traffic (SRTT) se disponibile\n",
    "    if total_network_traffic is not None:\n",
    "        processed_traffic = process_series_for_plot(total_network_traffic, normalize)\n",
    "        plt.plot(df[time_col], processed_traffic, \n",
    "                label='Total Network Traffic', \n",
    "                linewidth=1.5,\n",
    "                color='orange')\n",
    "    \n",
    "    # Plot Retransmitted Packets se disponibile\n",
    "    if retransmitted_packets is not None:\n",
    "        processed_packets = process_series_for_plot(retransmitted_packets, normalize)\n",
    "        plt.plot(df[time_col], processed_packets, \n",
    "                label='Retransmitted Packets', \n",
    "                linewidth=1.5,\n",
    "                color='purple')\n",
    "    \n",
    "    title_suffix = \"Normalized (0-100)\" if normalize else \"Original Values\"\n",
    "    ylabel = \"Normalized Value (0-100)\" if normalize else \"Value\"\n",
    "    \n",
    "    plt.title(f\"Network Traffic Metrics - {title_suffix}\", fontsize=16, fontweight='bold')\n",
    "    plt.xlabel(\"Time (minutes)\", fontsize=12)\n",
    "    plt.ylabel(ylabel, fontsize=12)\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=10)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    if normalize:\n",
    "        plt.ylim(0, 100)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_io_readwrite(df: pd.DataFrame, time_col='minutes', normalize=True):\n",
    "    \"\"\"Plot IO: readbytes e writebytes insieme\"\"\"\n",
    "    if time_col not in df.columns:\n",
    "        print(f\"Attenzione: colonna '{time_col}' non trovata nel DataFrame\")\n",
    "        return\n",
    "    \n",
    "    read_cols = [col for col in df.columns if 'readbytes' in col.lower() \n",
    "                 and 'minutes' not in col.lower()]\n",
    "    write_cols = [col for col in df.columns if 'writebytes' in col.lower() \n",
    "                  and 'minutes' not in col.lower()]\n",
    "    \n",
    "    if not read_cols and not write_cols:\n",
    "        print(\"Nessuna metrica readbytes/writebytes trovata\")\n",
    "        return\n",
    "    \n",
    "    plt.figure(figsize=(14, 8))\n",
    "    plt.axvspan(30, 80, alpha=0.2, color='red', label='Stress')\n",
    "    \n",
    "    if read_cols:\n",
    "        read_cumulative = df[read_cols].sum(axis=1)\n",
    "        processed_read = process_series_for_plot(read_cumulative, normalize)\n",
    "        plt.plot(df[time_col], processed_read, \n",
    "                label='Read Bytes (cumulativo)', \n",
    "                linewidth=1.5,\n",
    "                color='green')\n",
    "    \n",
    "    if write_cols:\n",
    "        write_cumulative = df[write_cols].sum(axis=1)\n",
    "        processed_write = process_series_for_plot(write_cumulative, normalize)\n",
    "        plt.plot(df[time_col], processed_write, \n",
    "                label='Write Bytes (cumulativo)', \n",
    "                linewidth=1.5,\n",
    "                color='red')\n",
    "    \n",
    "    title_suffix = \"Normalizzate (0-100)\" if normalize else \"Valori Originali\"\n",
    "    ylabel = \"Valore normalizzato (0-100)\" if normalize else \"Valore\"\n",
    "    \n",
    "    plt.title(f\"Metriche IO - Read/Write Bytes Cumulative - {title_suffix}\", \n",
    "              fontsize=16, fontweight='bold')\n",
    "    plt.xlabel(\"Tempo (minuti)\", fontsize=12)\n",
    "    plt.ylabel(ylabel, fontsize=12)\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=10)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    if normalize:\n",
    "        plt.ylim(0, 100)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_dashboard(df: pd.DataFrame, time_col='minutes', normalize=True):\n",
    "    \"\"\"Plot dashboard 2x2 con opzione di normalizzazione\"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(20, 12))\n",
    "    \n",
    "    title_suffix = \"Normalized (0-100)\" if normalize else \"Original Values\"\n",
    "    ylabel = \"Normalized Value (0-100)\" if normalize else \"Value\"\n",
    "    \n",
    "    # CPU Plot\n",
    "    ax1 = axes[0, 0]\n",
    "    ax1.axvspan(30, 80, alpha=0.2, color='red', label='Stress')\n",
    "    \n",
    "    cpu_metrics = ['iowait', 'irq', 'system', 'user', 'utilization']\n",
    "    colors = ['red', 'blue', 'green', 'orange', 'purple']\n",
    "    \n",
    "    for i, metric in enumerate(cpu_metrics):\n",
    "        cumulative = calculate_cpu_cumulative(df, metric)\n",
    "        if cumulative is not None:\n",
    "            processed_series = process_series_for_plot(cumulative, normalize)\n",
    "            ax1.plot(df[time_col], processed_series, \n",
    "                    label=f'CPU {metric}', \n",
    "                    linewidth=1.5,\n",
    "                    color=colors[i % len(colors)])\n",
    "    \n",
    "    ax1.set_title(f\"CPU Cumulative - {title_suffix}\", fontsize=14, fontweight='bold')\n",
    "    ax1.set_xlabel(\"Minutes\", fontsize=10)\n",
    "    ax1.set_ylabel(ylabel, fontsize=10)\n",
    "    ax1.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=8)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    if normalize:\n",
    "        ax1.set_ylim(0, 100)\n",
    "    \n",
    "    # Memory Plot\n",
    "    ax2 = axes[0, 1]\n",
    "    ax2.axvspan(30, 80, alpha=0.2, color='red', label='Stress')\n",
    "    \n",
    "    mem_available = [col for col in df.columns if 'memavailable' in col.lower() \n",
    "                     and 'minutes' not in col.lower()]\n",
    "    mem_util = [col for col in df.columns if 'memutil' in col.lower() \n",
    "                and 'minutes' not in col.lower()]\n",
    "    mem_cache = [col for col in df.columns if 'memcache' in col.lower() \n",
    "                 and 'minutes' not in col.lower()]\n",
    "    \n",
    "    colors = ['blue', 'green', 'orange']\n",
    "    labels = ['Memory Available', 'Memory Util', 'Memory Cache']\n",
    "    metric_groups = [mem_available, mem_util, mem_cache]\n",
    "    \n",
    "    for i, (metrics, label, color) in enumerate(zip(metric_groups, labels, colors)):\n",
    "        for col in metrics:\n",
    "            if col in df.columns:\n",
    "                processed_series = process_series_for_plot(df[col], normalize)\n",
    "                ax2.plot(df[time_col], processed_series, \n",
    "                        label=label, \n",
    "                        linewidth=1.5,\n",
    "                        color=color)\n",
    "                break\n",
    "    \n",
    "    ax2.set_title(f\"Memory - {title_suffix}\", fontsize=14, fontweight='bold')\n",
    "    ax2.set_xlabel(\"Minutes\", fontsize=10)\n",
    "    ax2.set_ylabel(ylabel, fontsize=10)\n",
    "    ax2.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=8)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    if normalize:\n",
    "        ax2.set_ylim(0, 100)\n",
    "    \n",
    "    # Network Traffic Plot\n",
    "    ax3 = axes[1, 0]\n",
    "    ax3.axvspan(30, 80, alpha=0.2, color='red', label='Stress')\n",
    "    \n",
    "    # Total Network Traffic (SRTT)\n",
    "    total_network_traffic = calculate_total_network_traffic(df)\n",
    "    \n",
    "    # Retransmitted Packets (TCP non-SRTT)\n",
    "    retransmitted_packets = calculate_tcp_non_srtt_cumulative(df)\n",
    "    \n",
    "    if total_network_traffic is not None:\n",
    "        processed_traffic = process_series_for_plot(total_network_traffic, normalize)\n",
    "        ax3.plot(df[time_col], processed_traffic, \n",
    "                label='Total Network Traffic', \n",
    "                linewidth=1.5,\n",
    "                color='orange')\n",
    "    \n",
    "    if retransmitted_packets is not None:\n",
    "        processed_packets = process_series_for_plot(retransmitted_packets, normalize)\n",
    "        ax3.plot(df[time_col], processed_packets, \n",
    "                label='Retransmitted Packets', \n",
    "                linewidth=1.5,\n",
    "                color='purple')\n",
    "    \n",
    "    ax3.set_title(f\"Network Traffic - {title_suffix}\", fontsize=14, fontweight='bold')\n",
    "    ax3.set_xlabel(\"Minutes\", fontsize=10)\n",
    "    ax3.set_ylabel(ylabel, fontsize=10)\n",
    "    ax3.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=8)\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    if normalize:\n",
    "        ax3.set_ylim(0, 100)\n",
    "    \n",
    "    # IO Plot\n",
    "    ax4 = axes[1, 1]\n",
    "    ax4.axvspan(30, 80, alpha=0.2, color='red', label='Stress')\n",
    "    \n",
    "    read_cols = [col for col in df.columns if 'readbytes' in col.lower() \n",
    "                 and 'minutes' not in col.lower()]\n",
    "    write_cols = [col for col in df.columns if 'writebytes' in col.lower() \n",
    "                  and 'minutes' not in col.lower()]\n",
    "    \n",
    "    if read_cols:\n",
    "        read_cumulative = df[read_cols].sum(axis=1)\n",
    "        processed_read = process_series_for_plot(read_cumulative, normalize)\n",
    "        ax4.plot(df[time_col], processed_read, \n",
    "                label='Read Bytes', \n",
    "                linewidth=1.5,\n",
    "                color='green')\n",
    "    \n",
    "    if write_cols:\n",
    "        write_cumulative = df[write_cols].sum(axis=1)\n",
    "        processed_write = process_series_for_plot(write_cumulative, normalize)\n",
    "        ax4.plot(df[time_col], processed_write, \n",
    "                label='Write Bytes', \n",
    "                linewidth=1.5,\n",
    "                color='red')\n",
    "    \n",
    "    ax4.set_title(f\"IO Read/Write - {title_suffix}\", fontsize=14, fontweight='bold')\n",
    "    ax4.set_xlabel(\"Minutes\", fontsize=10)\n",
    "    ax4.set_ylabel(ylabel, fontsize=10)\n",
    "    ax4.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=8)\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "    if normalize:\n",
    "        ax4.set_ylim(0, 100)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_all_simplified_metrics(df: pd.DataFrame, time_col='minutes', normalize=True):\n",
    "    \"\"\"Plot tutti i 4 grafici in sequenza\"\"\"\n",
    "    norm_text = \"normalizzati\" if normalize else \"valori originali\"\n",
    "    print(f\"Plotting grafici con {norm_text}...\")\n",
    "    \n",
    "    print(\"1. CPU Cumulative Metrics:\")\n",
    "    plot_cpu_cumulative_thin(df, time_col, normalize)\n",
    "    \n",
    "    print(\"\\n2. Memory Metrics:\")\n",
    "    plot_memory_metrics(df, time_col, normalize)\n",
    "    \n",
    "    print(\"\\n3. Network Traffic Metrics:\")\n",
    "    plot_tcp_network_traffic(df, time_col, normalize)\n",
    "    \n",
    "    print(\"\\n4. IO Read/Write Bytes:\")\n",
    "    plot_io_readwrite(df, time_col, normalize)\n",
    "\n",
    "def compare_normalized_vs_original(df, time_col='minutes'):\n",
    "    \"\"\"Confronta grafici normalizzati vs originali\"\"\"\n",
    "    print(\"=== DASHBOARD NORMALIZZATA ===\")\n",
    "    plot_dashboard(df, time_col, normalize=True)\n",
    "    \n",
    "    print(\"\\n=== DASHBOARD VALORI ORIGINALI ===\")\n",
    "    plot_dashboard(df, time_col, normalize=False)\n",
    "\n",
    "# Esempi di utilizzo migliorati:\n",
    "\n",
    "# Dashboard normalizzata (default)\n",
    "plot_dashboard(all_datasets['baseline'])\n",
    "\n",
    "# Dashboard con valori originali  \n",
    "plot_dashboard(all_datasets['baseline'], normalize=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9897345b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def super_clean_heatmap(data, title=\"Clean Correlation Heatmap\", show_labels=True):\n",
    "    \"\"\"\n",
    "    Ultra-clean heatmap that removes problematic columns\n",
    "    \"\"\"\n",
    "    # Get numeric data\n",
    "    numeric_data = data.select_dtypes(include=[np.number])\n",
    "    \n",
    "    # Remove time column\n",
    "    if 'minutes' in numeric_data.columns:\n",
    "        numeric_data = numeric_data.drop(columns=['minutes'])\n",
    "    \n",
    "    # Remove CPU 8/9 columns\n",
    "    cpu_8_9_cols = [col for col in numeric_data.columns if 'cpu_8' in col or 'cpu_9' in col or 'cpu 8' in col or 'cpu 9' in col]\n",
    "    if cpu_8_9_cols:\n",
    "        numeric_data = numeric_data.drop(columns=cpu_8_9_cols)\n",
    "        print(f\"🗑️ Removed {len(cpu_8_9_cols)} CPU 8/9 columns\")\n",
    "    \n",
    "    # Remove columns that are all zeros or have no variance\n",
    "    zero_variance_cols = []\n",
    "    for col in numeric_data.columns:\n",
    "        if numeric_data[col].var() == 0 or numeric_data[col].sum() == 0:\n",
    "            zero_variance_cols.append(col)\n",
    "    \n",
    "    if zero_variance_cols:\n",
    "        numeric_data = numeric_data.drop(columns=zero_variance_cols)\n",
    "        print(f\"🗑️ Removed {len(zero_variance_cols)} zero-variance columns\")\n",
    "    \n",
    "    # Remove columns with too many NaN values\n",
    "    nan_threshold = len(numeric_data) * 0.5  # Remove if >50% NaN\n",
    "    high_nan_cols = [col for col in numeric_data.columns if numeric_data[col].isnull().sum() > nan_threshold]\n",
    "    \n",
    "    if high_nan_cols:\n",
    "        numeric_data = numeric_data.drop(columns=high_nan_cols)\n",
    "        print(f\"🗑️ Removed {len(high_nan_cols)} high-NaN columns\")\n",
    "    \n",
    "    print(f\"📊 Final dataset: {len(numeric_data.columns)} features\")\n",
    "    \n",
    "    # Fill remaining NaN with 0\n",
    "    numeric_data = numeric_data.fillna(0)\n",
    "    \n",
    "    # Create correlation matrix\n",
    "    corr = numeric_data.corr()\n",
    "    \n",
    "    # Remove NaN correlations\n",
    "    corr = corr.fillna(0)\n",
    "    \n",
    "    # Create clean heatmap with proper sizing for labels\n",
    "    if show_labels:\n",
    "        fig, ax = plt.subplots(figsize=(20, 18))\n",
    "    else:\n",
    "        fig, ax = plt.subplots(figsize=(16, 14))\n",
    "    \n",
    "    sns.heatmap(corr, \n",
    "                cmap='RdYlBu_r', \n",
    "                center=0,\n",
    "                square=True,\n",
    "                cbar_kws={\"shrink\": .6},\n",
    "                xticklabels=show_labels,\n",
    "                yticklabels=show_labels,\n",
    "                ax=ax)\n",
    "    \n",
    "    ax.set_title(f'{title} ({len(corr)} features)', fontsize=16, pad=20)\n",
    "    \n",
    "    if show_labels:\n",
    "        # Rotate labels for better readability\n",
    "        plt.xticks(rotation=45, ha='right', fontsize=9)\n",
    "        plt.yticks(rotation=0, fontsize=9)\n",
    "        plt.subplots_adjust(bottom=0.2, left=0.2)\n",
    "    else:\n",
    "        ax.set_xlabel('')\n",
    "        ax.set_ylabel('')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return corr\n",
    "\n",
    "def show_removed_columns(data):\n",
    "    \"\"\"\n",
    "    Show what columns would be removed\n",
    "    \"\"\"\n",
    "    numeric_data = data.select_dtypes(include=[np.number])\n",
    "    \n",
    "    print(\"🔍 COLUMN ANALYSIS:\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # CPU 8/9\n",
    "    cpu_8_9 = [col for col in numeric_data.columns if 'cpu_8' in col or 'cpu_9' in col]\n",
    "    print(f\"CPU 8/9 columns: {len(cpu_8_9)}\")\n",
    "    \n",
    "    # Zero variance\n",
    "    zero_var = [col for col in numeric_data.columns if numeric_data[col].var() == 0 or numeric_data[col].sum() == 0]\n",
    "    print(f\"Zero variance: {len(zero_var)}\")\n",
    "    if zero_var:\n",
    "        print(f\"  Examples: {zero_var[:3]}\")\n",
    "    \n",
    "    # High NaN\n",
    "    nan_threshold = len(numeric_data) * 0.5\n",
    "    high_nan = [col for col in numeric_data.columns if numeric_data[col].isnull().sum() > nan_threshold]\n",
    "    print(f\"High NaN (>50%): {len(high_nan)}\")\n",
    "    if high_nan:\n",
    "        print(f\"  Examples: {high_nan[:3]}\")\n",
    "    \n",
    "    total_remove = len(cpu_8_9) + len(zero_var) + len(high_nan)\n",
    "    final_count = len(numeric_data.columns) - total_remove\n",
    "    print(f\"\\nFinal features: {final_count}\")\n",
    "\n",
    "def quick_analysis(data):\n",
    "    \"\"\"\n",
    "    Quick data quality check\n",
    "    \"\"\"\n",
    "    numeric_data = data.select_dtypes(include=[np.number])\n",
    "    \n",
    "    print(\"📊 DATA QUALITY CHECK:\")\n",
    "    print(\"-\" * 30)\n",
    "    print(f\"Total columns: {len(numeric_data.columns)}\")\n",
    "    print(f\"Total NaN: {numeric_data.isnull().sum().sum()}\")\n",
    "    print(f\"Columns with NaN: {(numeric_data.isnull().sum() > 0).sum()}\")\n",
    "    print(f\"Zero-sum columns: {(numeric_data.sum() == 0).sum()}\")\n",
    "\n",
    "# QUICK FUNCTIONS\n",
    "def labeled_heatmap(data):\n",
    "    \"\"\"Quick heatmap WITH labels\"\"\"\n",
    "    return super_clean_heatmap(data, show_labels=True)\n",
    "\n",
    "def clean_heatmap(data):\n",
    "    \"\"\"Quick heatmap WITHOUT labels\"\"\"\n",
    "    return super_clean_heatmap(data, show_labels=False)\n",
    "\n",
    "# USAGE EXAMPLES\n",
    "print(\"🎨 SUPER CLEAN HEATMAP READY!\")\n",
    "print(\"=\" * 35)\n",
    "print(\"1. With labels (detailed):\")\n",
    "print(\"   labeled_heatmap(all_datasets['baseline'])\")\n",
    "print(\"\")\n",
    "print(\"2. Without labels (clean):\")\n",
    "print(\"   clean_heatmap(all_datasets['baseline'])\")\n",
    "print(\"\")\n",
    "print(\"3. Custom:\")\n",
    "print(\"   super_clean_heatmap(data, show_labels=True)\")\n",
    "\n",
    "labeled_heatmap(all_datasets['baseline'])\n",
    "clean_heatmap(all_datasets['baseline'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7102371c",
   "metadata": {},
   "source": [
    "## feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa6ea1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class TimeSeriesFeatureEngineer:\n",
    "    def __init__(self, data, time_col='minutes'):\n",
    "        self.data = data.copy()\n",
    "        self.time_col = time_col\n",
    "        self.features = data.copy()\n",
    "        \n",
    "        self._clean_data()\n",
    "        self.features = self.features.sort_values(time_col).reset_index(drop=True)\n",
    "        \n",
    "        # Identify metric categories\n",
    "        numeric_cols = self.features.select_dtypes(include=[np.number]).columns\n",
    "        self.io_metrics = [col for col in numeric_cols if 'io_' in col.lower()]\n",
    "        self.cpu_metrics = [col for col in numeric_cols if 'cpu_' in col.lower()]\n",
    "        self.memory_metrics = [col for col in numeric_cols if 'memory_' in col.lower()]\n",
    "        self.network_metrics = [col for col in numeric_cols if 'network_' in col.lower()]\n",
    "    \n",
    "    def _clean_data(self):\n",
    "        # Handle non-numeric columns\n",
    "        non_numeric_cols = self.features.select_dtypes(exclude=[np.number]).columns\n",
    "        \n",
    "        for col in non_numeric_cols:\n",
    "            if col == self.time_col:\n",
    "                continue\n",
    "            \n",
    "            unique_values = self.features[col].unique()\n",
    "            if len(unique_values) <= 10:\n",
    "                le = LabelEncoder()\n",
    "                self.features[f'{col}_encoded'] = le.fit_transform(self.features[col].astype(str))\n",
    "            \n",
    "            if col != self.time_col:\n",
    "                self.features = self.features.drop(columns=[col])\n",
    "        \n",
    "        # Convert object columns to numeric\n",
    "        for col in self.features.columns:\n",
    "            if col == self.time_col:\n",
    "                continue\n",
    "            if self.features[col].dtype == 'object':\n",
    "                try:\n",
    "                    self.features[col] = pd.to_numeric(self.features[col], errors='coerce')\n",
    "                except:\n",
    "                    self.features = self.features.drop(columns=[col])\n",
    "        \n",
    "        # Fill NaN values\n",
    "        numeric_cols = self.features.select_dtypes(include=[np.number]).columns\n",
    "        self.features[numeric_cols] = self.features[numeric_cols].fillna(0)\n",
    "    \n",
    "    def create_temporal_features(self):\n",
    "        self.features['time_index'] = range(len(self.features))\n",
    "        self.features['time_minutes'] = self.features[self.time_col]\n",
    "        self.features['hour_of_day'] = (self.features[self.time_col] % 1440) / 60\n",
    "        self.features['minute_of_hour'] = self.features[self.time_col] % 60\n",
    "        \n",
    "        # Cyclical encoding\n",
    "        self.features['hour_sin'] = np.sin(2 * np.pi * self.features['hour_of_day'] / 24)\n",
    "        self.features['hour_cos'] = np.cos(2 * np.pi * self.features['hour_of_day'] / 24)\n",
    "        self.features['minute_sin'] = np.sin(2 * np.pi * self.features['minute_of_hour'] / 60)\n",
    "        self.features['minute_cos'] = np.cos(2 * np.pi * self.features['minute_of_hour'] / 60)\n",
    "        \n",
    "        max_time = self.features[self.time_col].max()\n",
    "        self.features['time_normalized'] = self.features[self.time_col] / max_time\n",
    "        return self\n",
    "    \n",
    "    def create_lag_features(self, columns=None, lags=[1, 2, 3, 5, 10]):\n",
    "        if columns is None:\n",
    "            columns = []\n",
    "            if self.cpu_metrics:\n",
    "                columns.extend([col for col in self.cpu_metrics if 'utilization' in col][:3])\n",
    "            if self.memory_metrics:\n",
    "                columns.extend([col for col in self.memory_metrics if 'util' in col][:2])\n",
    "            if self.io_metrics:\n",
    "                columns.extend([col for col in self.io_metrics if 'bytes' in col][:2])\n",
    "        \n",
    "        existing_columns = [col for col in columns if col in self.features.columns]\n",
    "        \n",
    "        for col in existing_columns:\n",
    "            for lag in lags:\n",
    "                self.features[f\"lag_{lag}_{col}\"] = self.features[col].shift(lag)\n",
    "        return self\n",
    "    \n",
    "    def create_rolling_features(self, columns=None, windows=[3, 5, 10, 20], stats=['mean', 'std', 'min', 'max']):\n",
    "        if columns is None:\n",
    "            columns = []\n",
    "            if self.cpu_metrics:\n",
    "                columns.extend([col for col in self.cpu_metrics if 'utilization' in col][:3])\n",
    "            if self.memory_metrics:\n",
    "                columns.extend([col for col in self.memory_metrics if 'util' in col][:1])\n",
    "        \n",
    "        existing_columns = [col for col in columns if col in self.features.columns]\n",
    "        \n",
    "        for col in existing_columns:\n",
    "            for window in windows:\n",
    "                for stat in stats:\n",
    "                    col_name = f\"rolling_{window}_{stat}_{col}\"\n",
    "                    if stat == 'mean':\n",
    "                        self.features[col_name] = self.features[col].rolling(window=window, min_periods=1).mean()\n",
    "                    elif stat == 'std':\n",
    "                        self.features[col_name] = self.features[col].rolling(window=window, min_periods=1).std()\n",
    "                    elif stat == 'min':\n",
    "                        self.features[col_name] = self.features[col].rolling(window=window, min_periods=1).min()\n",
    "                    elif stat == 'max':\n",
    "                        self.features[col_name] = self.features[col].rolling(window=window, min_periods=1).max()\n",
    "        return self\n",
    "    \n",
    "    def create_diff_features(self, columns=None, periods=[1, 2, 5]):\n",
    "        if columns is None:\n",
    "            columns = []\n",
    "            if self.cpu_metrics:\n",
    "                columns.extend([col for col in self.cpu_metrics if 'utilization' in col][:3])\n",
    "            if self.memory_metrics:\n",
    "                columns.extend([col for col in self.memory_metrics if 'util' in col][:1])\n",
    "        \n",
    "        existing_columns = [col for col in columns if col in self.features.columns]\n",
    "        \n",
    "        for col in existing_columns:\n",
    "            for period in periods:\n",
    "                self.features[f\"diff_{period}_{col}\"] = self.features[col].diff(periods=period)\n",
    "                pct_change = self.features[col].pct_change(periods=period)\n",
    "                pct_change = pct_change.replace([np.inf, -np.inf], 0).fillna(0)\n",
    "                self.features[f\"pct_change_{period}_{col}\"] = pct_change\n",
    "        return self\n",
    "    \n",
    "    def create_aggregated_features(self):\n",
    "        if self.cpu_metrics:\n",
    "            cpu_data = self.features[self.cpu_metrics]\n",
    "            self.features['cpu_total'] = cpu_data.sum(axis=1, skipna=True)\n",
    "            self.features['cpu_avg'] = cpu_data.mean(axis=1, skipna=True)\n",
    "            self.features['cpu_max'] = cpu_data.max(axis=1, skipna=True)\n",
    "            self.features['cpu_std'] = cpu_data.std(axis=1, skipna=True)\n",
    "        \n",
    "        if self.memory_metrics:\n",
    "            memory_data = self.features[self.memory_metrics]\n",
    "            self.features['memory_total'] = memory_data.sum(axis=1, skipna=True)\n",
    "            self.features['memory_avg'] = memory_data.mean(axis=1, skipna=True)\n",
    "        \n",
    "        if self.io_metrics:\n",
    "            io_data = self.features[self.io_metrics]\n",
    "            self.features['io_total'] = io_data.sum(axis=1, skipna=True)\n",
    "            self.features['io_avg'] = io_data.mean(axis=1, skipna=True)\n",
    "        return self\n",
    "    \n",
    "    def create_interaction_features(self, max_interactions=10):\n",
    "        key_metrics = []\n",
    "        if self.cpu_metrics:\n",
    "            key_metrics.extend([col for col in self.cpu_metrics if 'utilization' in col][:3])\n",
    "        if self.memory_metrics:\n",
    "            key_metrics.extend([col for col in self.memory_metrics if 'util' in col][:1])\n",
    "        if self.io_metrics:\n",
    "            key_metrics.extend([col for col in self.io_metrics if 'bytes' in col][:2])\n",
    "        \n",
    "        key_metrics = [col for col in key_metrics if col in self.features.columns]\n",
    "        \n",
    "        interaction_count = 0\n",
    "        for i, col1 in enumerate(key_metrics):\n",
    "            for col2 in key_metrics[i+1:]:\n",
    "                if interaction_count >= max_interactions:\n",
    "                    break\n",
    "                \n",
    "                self.features[f\"interaction_{col1}_x_{col2}\"] = self.features[col1] * self.features[col2]\n",
    "                col2_safe = self.features[col2].replace(0, 1e-8)\n",
    "                self.features[f\"ratio_{col1}_div_{col2}\"] = self.features[col1] / col2_safe\n",
    "                interaction_count += 2\n",
    "        return self\n",
    "    \n",
    "    def create_statistical_features(self, columns=None, windows=[5, 10, 20]):\n",
    "        if columns is None:\n",
    "            columns = []\n",
    "            if self.cpu_metrics:\n",
    "                columns.extend([col for col in self.cpu_metrics if 'utilization' in col][:2])\n",
    "            if self.memory_metrics:\n",
    "                columns.extend([col for col in self.memory_metrics if 'util' in col][:1])\n",
    "        \n",
    "        for col in columns:\n",
    "            if col in self.features.columns:\n",
    "                for window in windows:\n",
    "                    self.features[f\"skew_{window}_{col}\"] = (\n",
    "                        self.features[col].rolling(window=window, min_periods=3)\n",
    "                        .apply(lambda x: stats.skew(x, nan_policy='omit'), raw=True)\n",
    "                    )\n",
    "                    self.features[f\"kurt_{window}_{col}\"] = (\n",
    "                        self.features[col].rolling(window=window, min_periods=3)\n",
    "                        .apply(lambda x: stats.kurtosis(x, nan_policy='omit'), raw=True)\n",
    "                    )\n",
    "                    self.features[f\"q25_{window}_{col}\"] = (\n",
    "                        self.features[col].rolling(window=window, min_periods=1).quantile(0.25)\n",
    "                    )\n",
    "                    self.features[f\"q75_{window}_{col}\"] = (\n",
    "                        self.features[col].rolling(window=window, min_periods=1).quantile(0.75)\n",
    "                    )\n",
    "        return self\n",
    "    \n",
    "    def create_trend_features(self, columns=None, windows=[5, 10, 20]):\n",
    "        if columns is None:\n",
    "            columns = []\n",
    "            if self.cpu_metrics:\n",
    "                columns.extend([col for col in self.cpu_metrics if 'utilization' in col][:2])\n",
    "        \n",
    "        for col in columns:\n",
    "            if col in self.features.columns:\n",
    "                for window in windows:\n",
    "                    def calculate_slope(y):\n",
    "                        if len(y) < 2:\n",
    "                            return 0\n",
    "                        x = np.arange(len(y))\n",
    "                        try:\n",
    "                            slope, _, _, _, _ = stats.linregress(x, y)\n",
    "                            return slope\n",
    "                        except:\n",
    "                            return 0\n",
    "                    \n",
    "                    self.features[f\"trend_{window}_{col}\"] = (\n",
    "                        self.features[col].rolling(window=window, min_periods=2)\n",
    "                        .apply(calculate_slope, raw=True)\n",
    "                    )\n",
    "                    \n",
    "                    if window >= 3:\n",
    "                        self.features[f\"momentum_{window}_{col}\"] = (\n",
    "                            self.features[col].diff().rolling(window=window-1, min_periods=1).mean()\n",
    "                        )\n",
    "        return self\n",
    "    \n",
    "    def create_all_features(self):\n",
    "        self.create_temporal_features()\n",
    "        self.create_lag_features()\n",
    "        self.create_rolling_features()\n",
    "        self.create_diff_features()\n",
    "        self.create_aggregated_features()\n",
    "        self.create_interaction_features()\n",
    "        self.create_statistical_features()\n",
    "        self.create_trend_features()\n",
    "        \n",
    "        # Clean final features\n",
    "        numeric_cols = self.features.select_dtypes(include=[np.number]).columns\n",
    "        self.features[numeric_cols] = self.features[numeric_cols].fillna(method='ffill').fillna(method='bfill')\n",
    "        self.features[numeric_cols] = self.features[numeric_cols].fillna(0)\n",
    "        self.features[numeric_cols] = self.features[numeric_cols].replace([np.inf, -np.inf], 0)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def select_features(self, target_col, k=50):\n",
    "        if target_col not in self.features.columns:\n",
    "            return [], None\n",
    "        \n",
    "        numeric_cols = self.features.select_dtypes(include=[np.number]).columns\n",
    "        feature_cols = [col for col in numeric_cols if col not in [target_col, self.time_col]]\n",
    "        \n",
    "        if len(feature_cols) == 0:\n",
    "            return [], None\n",
    "        \n",
    "        X = self.features[feature_cols]\n",
    "        y = self.features[target_col]\n",
    "        \n",
    "        X = X.replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "        y = y.replace([np.inf, -np.inf], np.nan).fillna(0)\n",
    "        \n",
    "        try:\n",
    "            selector = SelectKBest(score_func=f_regression, k=min(k, len(feature_cols)))\n",
    "            selector.fit(X, y)\n",
    "            selected_features = [feature_cols[i] for i in selector.get_support(indices=True)]\n",
    "            return selected_features, selector.scores_\n",
    "        except:\n",
    "            correlations = X.corrwith(y).abs().sort_values(ascending=False)\n",
    "            selected_features = correlations.head(k).index.tolist()\n",
    "            return selected_features, correlations.values\n",
    "    \n",
    "    def plot_feature_importance(self, target_col, top_n=20):\n",
    "        selected_features, scores = self.select_features(target_col, k=top_n*2)\n",
    "        \n",
    "        if len(selected_features) == 0 or scores is None:\n",
    "            return\n",
    "        \n",
    "        numeric_cols = self.features.select_dtypes(include=[np.number]).columns\n",
    "        feature_cols = [col for col in numeric_cols if col not in [target_col, self.time_col]]\n",
    "        \n",
    "        if len(feature_cols) > 0:\n",
    "            importance_df = pd.DataFrame({\n",
    "                'feature': feature_cols[:len(scores)],\n",
    "                'importance': scores[:len(feature_cols)]\n",
    "            }).sort_values('importance', ascending=False).head(top_n)\n",
    "            \n",
    "            plt.figure(figsize=(12, 8))\n",
    "            sns.barplot(data=importance_df, x='importance', y='feature')\n",
    "            plt.title(f'Top {top_n} Features for {target_col}')\n",
    "            plt.xlabel('F-score')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "def create_forecasting_dataset(fe, target_metric, selected_features, forecast_horizon=5):\n",
    "    if len(selected_features) == 0:\n",
    "        return None\n",
    "    \n",
    "    forecast_data = fe.features.copy()\n",
    "    forecast_data[f'{target_metric}_target'] = forecast_data[target_metric].shift(-forecast_horizon)\n",
    "    \n",
    "    feature_columns = selected_features + [fe.time_col, target_metric]\n",
    "    existing_columns = [col for col in feature_columns if col in forecast_data.columns]\n",
    "    \n",
    "    modeling_data = forecast_data[existing_columns + [f'{target_metric}_target']].copy()\n",
    "    modeling_data = modeling_data.dropna()\n",
    "    \n",
    "    if len(modeling_data) == 0:\n",
    "        return None\n",
    "    \n",
    "    feature_only_cols = [col for col in existing_columns if col not in [fe.time_col, target_metric]]\n",
    "    X = modeling_data[feature_only_cols]\n",
    "    y = modeling_data[f'{target_metric}_target']\n",
    "    time_index = modeling_data[fe.time_col]\n",
    "    \n",
    "    split_idx = max(1, int(len(X) * 0.8))\n",
    "    \n",
    "    return {\n",
    "        'X_train': X.iloc[:split_idx],\n",
    "        'X_val': X.iloc[split_idx:],\n",
    "        'y_train': y.iloc[:split_idx],\n",
    "        'y_val': y.iloc[split_idx:],\n",
    "        'time_train': time_index.iloc[:split_idx],\n",
    "        'time_val': time_index.iloc[split_idx:],\n",
    "        'feature_names': X.columns.tolist()\n",
    "    }\n",
    "\n",
    "def find_best_target(fe):\n",
    "    \"\"\"Find the best target metric automatically\"\"\"\n",
    "    # Priority order: CPU utilization, memory utilization, other numeric\n",
    "    cpu_cols = [col for col in fe.features.columns if 'cpu_utilization' in col and 'cpu 0' in col]\n",
    "    if cpu_cols:\n",
    "        return cpu_cols[0]\n",
    "    \n",
    "    cpu_cols = [col for col in fe.features.columns if 'cpu' in col and 'utilization' in col]\n",
    "    if cpu_cols:\n",
    "        return cpu_cols[0]\n",
    "    \n",
    "    memory_cols = [col for col in fe.features.columns if 'memory' in col and 'util' in col]\n",
    "    if memory_cols:\n",
    "        return memory_cols[0]\n",
    "    \n",
    "    numeric_cols = fe.features.select_dtypes(include=[np.number]).columns\n",
    "    candidates = [col for col in numeric_cols if col != fe.time_col and fe.features[col].var() > 0]\n",
    "    \n",
    "    return candidates[0] if candidates else None\n",
    "\n",
    "# MAIN EXECUTION - READY TO RUN\n",
    "def run_feature_engineering(data=None):\n",
    "    \"\"\"Main function to run the complete pipeline\"\"\"\n",
    "    \n",
    "    # Load data\n",
    "    if data is None:\n",
    "        try:\n",
    "            data = all_datasets['baseline']\n",
    "        except:\n",
    "            print(\"❌ Please provide data or ensure all_datasets['baseline'] exists\")\n",
    "            return None\n",
    "    \n",
    "    print(f\"📊 Dataset: {data.shape[0]} samples, {data.shape[1]} columns\")\n",
    "    \n",
    "    # Initialize feature engineer\n",
    "    fe = TimeSeriesFeatureEngineer(data, time_col='minutes')\n",
    "    \n",
    "    # Create all features\n",
    "    fe.create_all_features()\n",
    "    print(f\"🔧 Created {len(fe.features.columns)} total features\")\n",
    "    \n",
    "    # Find best target\n",
    "    target_metric = find_best_target(fe)\n",
    "    if not target_metric:\n",
    "        print(\"❌ No suitable target metric found\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"🎯 Target: {target_metric}\")\n",
    "    \n",
    "    # Select features\n",
    "    selected_features, _ = fe.select_features(target_metric, k=30)\n",
    "    print(f\"✅ Selected {len(selected_features)} features\")\n",
    "    \n",
    "    # Create forecasting dataset\n",
    "    forecast_dataset = create_forecasting_dataset(fe, target_metric, selected_features)\n",
    "    \n",
    "    if forecast_dataset:\n",
    "        print(f\"📈 Training: {len(forecast_dataset['X_train'])} samples\")\n",
    "        print(f\"📈 Validation: {len(forecast_dataset['X_val'])} samples\")\n",
    "        print(\"✅ Ready for machine learning!\")\n",
    "        \n",
    "        # Show feature importance\n",
    "        fe.plot_feature_importance(target_metric, top_n=15)\n",
    "        \n",
    "        return {\n",
    "            'fe': fe,\n",
    "            'target_metric': target_metric,\n",
    "            'selected_features': selected_features,\n",
    "            'forecast_dataset': forecast_dataset\n",
    "        }\n",
    "    else:\n",
    "        print(\"❌ Failed to create forecasting dataset\")\n",
    "        return None\n",
    "\n",
    "# Run everything\n",
    "if __name__ == \"__main__\":\n",
    "    result = run_feature_engineering()\n",
    "    \n",
    "    if result:\n",
    "        # Access results\n",
    "        fe = result['fe']\n",
    "        forecast_dataset = result['forecast_dataset']\n",
    "        target_metric = result['target_metric']\n",
    "        \n",
    "        print(f\"\\n🎉 SUCCESS! Feature engineering complete.\")\n",
    "        print(f\"📋 Available variables:\")\n",
    "        print(f\"   - fe: Feature engineer object\")\n",
    "        print(f\"   - forecast_dataset: Ready-to-use ML dataset\")\n",
    "        print(f\"   - target_metric: '{target_metric}'\")\n",
    "        print(f\"\\n🚀 Next steps:\")\n",
    "        print(f\"   - Train models on forecast_dataset['X_train'], forecast_dataset['y_train']\")\n",
    "        print(f\"   - Validate on forecast_dataset['X_val'], forecast_dataset['y_val']\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1aebf55",
   "metadata": {},
   "source": [
    "LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274dc7ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23fd8e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set working directory\n",
    "os.chdir('/home/alessandro/PGFDS/results/tuna/customers')\n",
    "\n",
    "class OptimizedFailureDetector:\n",
    "    \"\"\"\n",
    "    Detector ottimizzato con:\n",
    "    1. Soglia ottimale automatica\n",
    "    2. Bilanciamento classi\n",
    "    3. Focal Loss per imbalanced data\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, sequence_length=20, lstm_units=64, dropout_rate=0.3, \n",
    "                 use_focal_loss=True, use_class_weights=True):\n",
    "        self.sequence_length = sequence_length\n",
    "        self.lstm_units = lstm_units\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.scaler = StandardScaler()\n",
    "        self.model = None\n",
    "        \n",
    "        # Configurazioni ottimizzazione\n",
    "        self.use_focal_loss = use_focal_loss\n",
    "        self.use_class_weights = use_class_weights\n",
    "        \n",
    "        # Finestre adattive\n",
    "        self.failure_start_percent = 0.2\n",
    "        self.failure_end_percent = 0.6\n",
    "        \n",
    "        # Soglia ottimale (sarà calcolata automaticamente)\n",
    "        self.optimal_threshold = 0.5\n",
    "        \n",
    "    def focal_loss(self, alpha=0.75, gamma=2.0):\n",
    "        \"\"\"\n",
    "        FOCAL LOSS - Spiegazione dettagliata:\n",
    "        \n",
    "        Il problema principale è che abbiamo molti più esempi \"normali\" che \"failure\".\n",
    "        Durante il training, il modello si \"abitua\" a predire sempre \"normale\" perché\n",
    "        è statisticamente più probabile avere ragione.\n",
    "        \n",
    "        La Focal Loss risolve questo in 2 modi:\n",
    "        \n",
    "        1. ALPHA WEIGHTING:\n",
    "           - alpha=0.75 significa che i failure (classe minority) hanno peso 3x maggiore\n",
    "           - I failure \"costano\" di più quando sbagliati\n",
    "           - Il modello è \"forzato\" a prestare più attenzione ai failure\n",
    "        \n",
    "        2. GAMMA FOCUSING:\n",
    "           - gamma=2.0 riduce il peso degli esempi \"facili\"\n",
    "           - Se il modello è già sicuro al 95%, non impara molto\n",
    "           - Concentra l'apprendimento sugli esempi difficili/ambigui\n",
    "        \n",
    "        Matematicamente:\n",
    "        FL(p_t) = -alpha * (1-p_t)^gamma * log(p_t)\n",
    "        \n",
    "        Dove p_t è la probabilità corretta per la classe vera.\n",
    "        \"\"\"\n",
    "        def focal_loss_fixed(y_true, y_pred):\n",
    "            # Evita log(0) che darebbe infinito\n",
    "            epsilon = tf.keras.backend.epsilon()\n",
    "            y_pred = tf.clip_by_value(y_pred, epsilon, 1. - epsilon)\n",
    "            \n",
    "            # p_t = probabilità della classe corretta\n",
    "            # Se y_true=1 (failure), p_t=y_pred\n",
    "            # Se y_true=0 (normal), p_t=1-y_pred\n",
    "            p_t = tf.where(tf.equal(y_true, 1), y_pred, 1 - y_pred)\n",
    "            \n",
    "            # Alpha weighting: più peso alla classe minority (failure)\n",
    "            alpha_factor = tf.ones_like(y_true) * alpha\n",
    "            alpha_t = tf.where(tf.equal(y_true, 1), alpha_factor, 1 - alpha_factor)\n",
    "            \n",
    "            # Standard cross entropy\n",
    "            cross_entropy = -tf.math.log(p_t)\n",
    "            \n",
    "            # Focusing term: (1-p_t)^gamma\n",
    "            # Se p_t è alto (esempio facile), il peso è basso\n",
    "            # Se p_t è basso (esempio difficile), il peso è alto\n",
    "            weight = alpha_t * tf.pow((1 - p_t), gamma)\n",
    "            \n",
    "            # Loss finale\n",
    "            loss = weight * cross_entropy\n",
    "            return tf.reduce_mean(loss)\n",
    "        \n",
    "        return focal_loss_fixed\n",
    "    \n",
    "    def load_and_label_data(self, unified_dir=\"unified\"):\n",
    "        \"\"\"\n",
    "        Carica dati con finestra failure adattiva\n",
    "        \"\"\"\n",
    "        all_data = []\n",
    "        \n",
    "        print(\"📊 Loading data with adaptive failure windows...\")\n",
    "        \n",
    "        for exp_dir in os.listdir(unified_dir):\n",
    "            if not os.path.isdir(os.path.join(unified_dir, exp_dir)):\n",
    "                continue\n",
    "                \n",
    "            csv_file = os.path.join(unified_dir, exp_dir, f\"all_metrics_combined_{exp_dir}.csv\")\n",
    "            \n",
    "            if os.path.exists(csv_file):\n",
    "                print(f\"Loading {exp_dir}...\")\n",
    "                df = pd.read_csv(csv_file)\n",
    "                \n",
    "                # Gestione time/minutes\n",
    "                if 'time' in df.columns:\n",
    "                    df['minutes'] = pd.to_numeric(df['time'], errors='coerce')\n",
    "                    if df['minutes'].isna().any():\n",
    "                        df['minutes'] = np.arange(len(df))\n",
    "                    df = df.drop(columns=['time'])\n",
    "                else:\n",
    "                    df['minutes'] = np.arange(len(df))\n",
    "                \n",
    "                duration = df['minutes'].max() - df['minutes'].min()\n",
    "                \n",
    "                # Labeling adattivo\n",
    "                if exp_dir == 'baseline':\n",
    "                    df['is_failure'] = 0\n",
    "                    print(f\"  Baseline: all normal\")\n",
    "                else:\n",
    "                    failure_start = df['minutes'].min() + (duration * self.failure_start_percent)\n",
    "                    failure_end = df['minutes'].min() + (duration * self.failure_end_percent)\n",
    "                    \n",
    "                    df['is_failure'] = ((df['minutes'] >= failure_start) & \n",
    "                                       (df['minutes'] <= failure_end)).astype(int)\n",
    "                    \n",
    "                    failure_count = df['is_failure'].sum()\n",
    "                    print(f\"  {exp_dir}: {failure_count}/{len(df)} failures ({failure_count/len(df)*100:.1f}%)\")\n",
    "                \n",
    "                df['experiment'] = exp_dir\n",
    "                all_data.append(df)\n",
    "        \n",
    "        return pd.concat(all_data, ignore_index=True)\n",
    "    \n",
    "    def prepare_features(self, df):\n",
    "        \"\"\"\n",
    "        Prepara features escludendo colonne temporali\n",
    "        \"\"\"\n",
    "        feature_columns = df.select_dtypes(include=[np.number]).columns\n",
    "        feature_columns = [col for col in feature_columns \n",
    "                          if col not in ['minutes', 'time', 'is_failure', 'Timestamp', 'timestamp']]\n",
    "        \n",
    "        print(f\"Using {len(feature_columns)} features\")\n",
    "        return df[feature_columns].fillna(0).values, feature_columns\n",
    "    \n",
    "    def create_sequences(self, X, y, experiments):\n",
    "        \"\"\"\n",
    "        Crea sequenze temporali per esperimento\n",
    "        \"\"\"\n",
    "        X_sequences = []\n",
    "        y_sequences = []\n",
    "        experiment_info = []\n",
    "        \n",
    "        for exp_name in set(experiments):\n",
    "            exp_mask = experiments == exp_name\n",
    "            X_exp = X[exp_mask]\n",
    "            y_exp = y[exp_mask]\n",
    "            \n",
    "            for i in range(len(X_exp) - self.sequence_length + 1):\n",
    "                sequence = X_exp[i:i + self.sequence_length]\n",
    "                label = y_exp[i + self.sequence_length - 1]\n",
    "                \n",
    "                X_sequences.append(sequence)\n",
    "                y_sequences.append(label)\n",
    "                experiment_info.append(exp_name)\n",
    "        \n",
    "        return np.array(X_sequences), np.array(y_sequences), experiment_info\n",
    "    \n",
    "    def calculate_class_weights(self, y_train):\n",
    "        \"\"\"\n",
    "        CLASS WEIGHTS - Spiegazione dettagliata:\n",
    "        \n",
    "        Il problema: Se hai 1000 esempi \"normali\" e 100 esempi \"failure\",\n",
    "        il modello impara che \"predire sempre normale\" gli dà 90% accuracy.\n",
    "        \n",
    "        La soluzione: Dare peso maggiore alla classe minority.\n",
    "        \n",
    "        Calcolo automatico:\n",
    "        - Se hai 90% normali e 10% failure\n",
    "        - Peso normale = 1 / (2 * 0.9) = 0.56\n",
    "        - Peso failure = 1 / (2 * 0.1) = 5.0\n",
    "        \n",
    "        Risultato: Un errore su failure \"costa\" 9x di più che un errore su normale.\n",
    "        Il modello è incentivato a non ignorare i failure.\n",
    "        \"\"\"\n",
    "        # Calcola pesi bilanciati automaticamente\n",
    "        classes = np.unique(y_train)\n",
    "        class_weights = compute_class_weight('balanced', classes=classes, y=y_train)\n",
    "        \n",
    "        class_weight_dict = {}\n",
    "        for i, cls in enumerate(classes):\n",
    "            class_weight_dict[cls] = class_weights[i]\n",
    "        \n",
    "        print(f\"\\n⚖️  CLASS WEIGHTS CALCULATED:\")\n",
    "        for cls, weight in class_weight_dict.items():\n",
    "            class_name = \"Normal\" if cls == 0 else \"Failure\"\n",
    "            print(f\"   {class_name}: {weight:.3f}\")\n",
    "        \n",
    "        failure_weight = class_weight_dict.get(1, 1.0)\n",
    "        normal_weight = class_weight_dict.get(0, 1.0)\n",
    "        ratio = failure_weight / normal_weight\n",
    "        \n",
    "        print(f\"   Failure examples are weighted {ratio:.1f}x more than normal\")\n",
    "        print(f\"   This compensates for class imbalance\")\n",
    "        \n",
    "        return class_weight_dict\n",
    "    \n",
    "    def build_optimized_model(self, input_shape):\n",
    "        \"\"\"\n",
    "        Costruisce modello con focal loss o binary crossentropy\n",
    "        \"\"\"\n",
    "        model = Sequential([\n",
    "            LSTM(self.lstm_units, return_sequences=True, input_shape=input_shape),\n",
    "            BatchNormalization(),\n",
    "            Dropout(self.dropout_rate),\n",
    "            \n",
    "            LSTM(self.lstm_units // 2, return_sequences=False),\n",
    "            BatchNormalization(),\n",
    "            Dropout(self.dropout_rate),\n",
    "            \n",
    "            Dense(32, activation='relu'),\n",
    "            Dropout(self.dropout_rate / 2),\n",
    "            \n",
    "            Dense(16, activation='relu'),\n",
    "            \n",
    "            Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "        \n",
    "        # Scelta della loss function\n",
    "        if self.use_focal_loss:\n",
    "            print(f\"\\n🎯 Using FOCAL LOSS (alpha=0.75, gamma=2.0)\")\n",
    "            print(f\"   This will focus learning on hard examples and minority class\")\n",
    "            loss_function = self.focal_loss(alpha=0.75, gamma=2.0)\n",
    "        else:\n",
    "            print(f\"\\n📊 Using standard BINARY CROSSENTROPY\")\n",
    "            loss_function = 'binary_crossentropy'\n",
    "        \n",
    "        model.compile(\n",
    "            optimizer='adam',\n",
    "            loss=loss_function,\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        \n",
    "        self.model = model\n",
    "        return model\n",
    "    \n",
    "    def train_optimized_model(self, X_train, y_train, X_val, y_val, epochs=30):\n",
    "        \"\"\"\n",
    "        Training con class weights opzionali\n",
    "        \"\"\"\n",
    "        callbacks = [\n",
    "            EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True),\n",
    "            ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)\n",
    "        ]\n",
    "        \n",
    "        # Calcola class weights se richiesto\n",
    "        if self.use_class_weights:\n",
    "            class_weight_dict = self.calculate_class_weights(y_train)\n",
    "            print(f\"\\n🏋️ Training with CLASS WEIGHTS\")\n",
    "        else:\n",
    "            class_weight_dict = None\n",
    "            print(f\"\\n🏋️ Training WITHOUT class weights\")\n",
    "        \n",
    "        history = self.model.fit(\n",
    "            X_train, y_train,\n",
    "            validation_data=(X_val, y_val),\n",
    "            epochs=epochs,\n",
    "            batch_size=32,\n",
    "            callbacks=callbacks,\n",
    "            class_weight=class_weight_dict,  # Qui viene applicato il bilanciamento\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        return history\n",
    "    \n",
    "    def find_optimal_threshold(self, X_val, y_val):\n",
    "        \"\"\"\n",
    "        OPTIMAL THRESHOLD - Spiegazione dettagliata:\n",
    "        \n",
    "        Il problema: La soglia di default 0.5 è arbitraria.\n",
    "        Non considera il costo relativo dei diversi tipi di errore.\n",
    "        \n",
    "        La soluzione: Trova la soglia che ottimizza il trade-off precision/recall.\n",
    "        \n",
    "        Processo:\n",
    "        1. Calcola precision/recall per tutte le soglie possibili (0.0 to 1.0)\n",
    "        2. Per ogni soglia, calcola F1-score = 2 * (precision * recall) / (precision + recall)\n",
    "        3. Scegli la soglia che massimizza F1-score\n",
    "        \n",
    "        Perché F1? Bilancia precision e recall:\n",
    "        - Precision alta = pochi falsi allarmi\n",
    "        - Recall alto = pochi failure mancati\n",
    "        - F1 alto = buon compromesso tra entrambi\n",
    "        \n",
    "        Alternative:\n",
    "        - Se i falsi allarmi costano molto: ottimizza precision\n",
    "        - Se i failure mancati costano molto: ottimizza recall\n",
    "        - Per sistemi critici: spesso si preferisce recall alto\n",
    "        \"\"\"\n",
    "        print(f\"\\n🔍 FINDING OPTIMAL THRESHOLD...\")\n",
    "        \n",
    "        # Ottieni probabilità su validation set\n",
    "        y_pred_proba = self.model.predict(X_val, verbose=0).flatten()\n",
    "        \n",
    "        # Calcola precision/recall per tutte le soglie\n",
    "        precision, recall, thresholds = precision_recall_curve(y_val, y_pred_proba)\n",
    "        \n",
    "        # Calcola F1-score per ogni soglia\n",
    "        # Evita divisione per zero\n",
    "        f1_scores = []\n",
    "        for p, r in zip(precision, recall):\n",
    "            if p + r == 0:\n",
    "                f1_scores.append(0)\n",
    "            else:\n",
    "                f1_scores.append(2 * (p * r) / (p + r))\n",
    "        \n",
    "        f1_scores = np.array(f1_scores)\n",
    "        \n",
    "        # Trova soglia ottimale\n",
    "        optimal_idx = np.argmax(f1_scores)\n",
    "        self.optimal_threshold = thresholds[optimal_idx] if optimal_idx < len(thresholds) else 0.5\n",
    "        optimal_f1 = f1_scores[optimal_idx]\n",
    "        optimal_precision = precision[optimal_idx]\n",
    "        optimal_recall = recall[optimal_idx]\n",
    "        \n",
    "        print(f\"   Default threshold (0.5):\")\n",
    "        default_predictions = (y_pred_proba > 0.5).astype(int)\n",
    "        default_precision = np.sum((default_predictions == 1) & (y_val == 1)) / max(np.sum(default_predictions == 1), 1)\n",
    "        default_recall = np.sum((default_predictions == 1) & (y_val == 1)) / max(np.sum(y_val == 1), 1)\n",
    "        default_f1 = 2 * (default_precision * default_recall) / max(default_precision + default_recall, 1e-8)\n",
    "        \n",
    "        print(f\"     Precision: {default_precision:.3f}, Recall: {default_recall:.3f}, F1: {default_f1:.3f}\")\n",
    "        \n",
    "        print(f\"   Optimal threshold ({self.optimal_threshold:.3f}):\")\n",
    "        print(f\"     Precision: {optimal_precision:.3f}, Recall: {optimal_recall:.3f}, F1: {optimal_f1:.3f}\")\n",
    "        \n",
    "        improvement = ((optimal_f1 - default_f1) / default_f1 * 100) if default_f1 > 0 else 0\n",
    "        print(f\"   Improvement: {improvement:+.1f}% F1-score\")\n",
    "        \n",
    "        # Plot precision-recall curve\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        \n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(thresholds, precision[:-1], 'b-', label='Precision')\n",
    "        plt.plot(thresholds, recall[:-1], 'r-', label='Recall')\n",
    "        plt.axvline(self.optimal_threshold, color='green', linestyle='--', label=f'Optimal ({self.optimal_threshold:.3f})')\n",
    "        plt.axvline(0.5, color='orange', linestyle='--', label='Default (0.5)')\n",
    "        plt.xlabel('Threshold')\n",
    "        plt.ylabel('Score')\n",
    "        plt.title('Precision/Recall vs Threshold')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(recall, precision, 'b-')\n",
    "        plt.scatter(optimal_recall, optimal_precision, color='green', s=100, \n",
    "                   label=f'Optimal (F1={optimal_f1:.3f})')\n",
    "        plt.scatter(default_recall, default_precision, color='orange', s=100,\n",
    "                   label=f'Default (F1={default_f1:.3f})')\n",
    "        plt.xlabel('Recall')\n",
    "        plt.ylabel('Precision')\n",
    "        plt.title('Precision-Recall Curve')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        return self.optimal_threshold\n",
    "    \n",
    "    def evaluate_with_optimal_threshold(self, X_test, y_test):\n",
    "        \"\"\"\n",
    "        Valutazione usando soglia ottimale\n",
    "        \"\"\"\n",
    "        print(f\"\\n📊 EVALUATION WITH OPTIMAL THRESHOLD\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Predizioni con soglia ottimale\n",
    "        y_pred_proba = self.model.predict(X_test, verbose=0).flatten()\n",
    "        y_pred_optimal = (y_pred_proba > self.optimal_threshold).astype(int)\n",
    "        y_pred_default = (y_pred_proba > 0.5).astype(int)\n",
    "        \n",
    "        # Calcola metriche per entrambe le soglie\n",
    "        from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "        \n",
    "        print(f\"\\n🔄 COMPARISON: Default vs Optimal Threshold\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        # Default threshold (0.5)\n",
    "        acc_default = accuracy_score(y_test, y_pred_default)\n",
    "        prec_default = precision_score(y_test, y_pred_default, zero_division=0)\n",
    "        rec_default = recall_score(y_test, y_pred_default, zero_division=0)\n",
    "        f1_default = f1_score(y_test, y_pred_default, zero_division=0)\n",
    "        \n",
    "        print(f\"Default (0.5):     Acc={acc_default:.3f}, Prec={prec_default:.3f}, Rec={rec_default:.3f}, F1={f1_default:.3f}\")\n",
    "        \n",
    "        # Optimal threshold\n",
    "        acc_optimal = accuracy_score(y_test, y_pred_optimal)\n",
    "        prec_optimal = precision_score(y_test, y_pred_optimal, zero_division=0)\n",
    "        rec_optimal = recall_score(y_test, y_pred_optimal, zero_division=0)\n",
    "        f1_optimal = f1_score(y_test, y_pred_optimal, zero_division=0)\n",
    "        \n",
    "        print(f\"Optimal ({self.optimal_threshold:.3f}): Acc={acc_optimal:.3f}, Prec={prec_optimal:.3f}, Rec={rec_optimal:.3f}, F1={f1_optimal:.3f}\")\n",
    "        \n",
    "        # Calcola miglioramenti\n",
    "        print(f\"\\n📈 IMPROVEMENTS:\")\n",
    "        print(f\"   Accuracy:  {acc_optimal - acc_default:+.3f}\")\n",
    "        print(f\"   Precision: {prec_optimal - prec_default:+.3f}\")\n",
    "        print(f\"   Recall:    {rec_optimal - rec_default:+.3f} ⭐\")\n",
    "        print(f\"   F1-Score:  {f1_optimal - f1_default:+.3f}\")\n",
    "        \n",
    "        # AUC\n",
    "        if len(np.unique(y_test)) > 1:\n",
    "            auc_score = roc_auc_score(y_test, y_pred_proba)\n",
    "            print(f\"   AUC:       {auc_score:.3f} (unchanged - depends on ranking)\")\n",
    "        \n",
    "        # Confusion matrices\n",
    "        cm_default = confusion_matrix(y_test, y_pred_default)\n",
    "        cm_optimal = confusion_matrix(y_test, y_pred_optimal)\n",
    "        \n",
    "        plt.figure(figsize=(12, 4))\n",
    "        \n",
    "        plt.subplot(1, 2, 1)\n",
    "        sns.heatmap(cm_default, annot=True, fmt='d', cmap='Blues',\n",
    "                   xticklabels=['Normal', 'Failure'],\n",
    "                   yticklabels=['Normal', 'Failure'])\n",
    "        plt.title('Default Threshold (0.5)')\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        sns.heatmap(cm_optimal, annot=True, fmt='d', cmap='Reds',\n",
    "                   xticklabels=['Normal', 'Failure'],\n",
    "                   yticklabels=['Normal', 'Failure'])\n",
    "        plt.title(f'Optimal Threshold ({self.optimal_threshold:.3f})')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        return acc_optimal, prec_optimal, rec_optimal, f1_optimal\n",
    "\n",
    "def create_failure_aware_split(X_seq, y_seq, exp_info, test_ratio=0.3):\n",
    "    \"\"\"\n",
    "    Split che garantisce failure nel test set\n",
    "    \"\"\"\n",
    "    train_indices = []\n",
    "    test_indices = []\n",
    "    \n",
    "    for exp in set(exp_info):\n",
    "        exp_mask = np.array(exp_info) == exp\n",
    "        exp_indices = np.where(exp_mask)[0]\n",
    "        exp_labels = y_seq[exp_indices]\n",
    "        \n",
    "        failure_indices = exp_indices[exp_labels == 1]\n",
    "        normal_indices = exp_indices[exp_labels == 0]\n",
    "        \n",
    "        if len(failure_indices) > 0:\n",
    "            # Mantieni failure in entrambi train e test\n",
    "            n_failure_test = max(1, int(len(failure_indices) * test_ratio))\n",
    "            failure_test = failure_indices[-n_failure_test:]\n",
    "            failure_train = failure_indices[:-n_failure_test]\n",
    "            \n",
    "            n_normal_test = int(len(normal_indices) * test_ratio)\n",
    "            normal_test = normal_indices[-n_normal_test:]\n",
    "            normal_train = normal_indices[:-n_normal_test]\n",
    "            \n",
    "            train_indices.extend(failure_train)\n",
    "            train_indices.extend(normal_train)\n",
    "            test_indices.extend(failure_test)\n",
    "            test_indices.extend(normal_test)\n",
    "        else:\n",
    "            # Solo normale (baseline)\n",
    "            n_test = int(len(exp_indices) * test_ratio)\n",
    "            test_indices.extend(exp_indices[-n_test:])\n",
    "            train_indices.extend(exp_indices[:-n_test])\n",
    "    \n",
    "    return train_indices, test_indices\n",
    "\n",
    "def main_optimized():\n",
    "    \"\"\"\n",
    "    Training completo con tutte le ottimizzazioni\n",
    "    \"\"\"\n",
    "    print(\"=== OPTIMIZED FAILURE DETECTION TRAINING ===\\n\")\n",
    "    \n",
    "    # Testa diverse configurazioni\n",
    "    configurations = [\n",
    "        {\"name\": \"Baseline\", \"focal_loss\": False, \"class_weights\": False},\n",
    "        {\"name\": \"Class Weights Only\", \"focal_loss\": False, \"class_weights\": True},\n",
    "        {\"name\": \"Focal Loss Only\", \"focal_loss\": True, \"class_weights\": False},\n",
    "        {\"name\": \"Full Optimization\", \"focal_loss\": True, \"class_weights\": True},\n",
    "    ]\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for config in configurations:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"🧪 TESTING CONFIGURATION: {config['name'].upper()}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        # Inizializza detector con configurazione\n",
    "        detector = OptimizedFailureDetector(\n",
    "            sequence_length=20, \n",
    "            lstm_units=64,\n",
    "            use_focal_loss=config['focal_loss'],\n",
    "            use_class_weights=config['class_weights']\n",
    "        )\n",
    "        \n",
    "        # Carica e prepara dati (solo una volta)\n",
    "        if 'df' not in locals():\n",
    "            df = detector.load_and_label_data(\"unified\")\n",
    "            X, feature_columns = detector.prepare_features(df)\n",
    "            y = df['is_failure'].values\n",
    "            experiments = df['experiment'].values\n",
    "            \n",
    "            X_scaled = detector.scaler.fit_transform(X)\n",
    "            X_seq, y_seq, exp_info = detector.create_sequences(X_scaled, y, experiments)\n",
    "            \n",
    "            # Split\n",
    "            train_idx, test_idx = create_failure_aware_split(X_seq, y_seq, exp_info)\n",
    "            X_train = X_seq[train_idx]\n",
    "            y_train = y_seq[train_idx]\n",
    "            X_test = X_seq[test_idx]\n",
    "            y_test = y_seq[test_idx]\n",
    "            \n",
    "            # Validation split\n",
    "            val_split = int(len(X_train) * 0.8)\n",
    "            X_train_final = X_train[:val_split]\n",
    "            y_train_final = y_train[:val_split]\n",
    "            X_val = X_train[val_split:]\n",
    "            y_val = y_train[val_split:]\n",
    "            \n",
    "            print(f\"\\nDataset: {len(X_seq)} sequences, {y_seq.sum()} failures\")\n",
    "            print(f\"Splits: Train={len(X_train_final)}, Val={len(X_val)}, Test={len(X_test)}\")\n",
    "        \n",
    "        # Costruisci e addestra modello\n",
    "        model = detector.build_optimized_model((X_seq.shape[1], X_seq.shape[2]))\n",
    "        \n",
    "        print(f\"\\n🏋️ Training {config['name']} model...\")\n",
    "        history = detector.train_optimized_model(\n",
    "            X_train_final, y_train_final, \n",
    "            X_val, y_val, \n",
    "            epochs=25\n",
    "        )\n",
    "        \n",
    "        # Trova soglia ottimale\n",
    "        optimal_threshold = detector.find_optimal_threshold(X_val, y_val)\n",
    "        \n",
    "        # Valuta con soglia ottimale\n",
    "        acc, prec, rec, f1 = detector.evaluate_with_optimal_threshold(X_test, y_test)\n",
    "        \n",
    "        # Salva risultati\n",
    "        results[config['name']] = {\n",
    "            'accuracy': acc,\n",
    "            'precision': prec,\n",
    "            'recall': rec,\n",
    "            'f1': f1,\n",
    "            'threshold': optimal_threshold\n",
    "        }\n",
    "        \n",
    "        # Salva modello\n",
    "        model_name = f\"optimized_detector_{config['name'].lower().replace(' ', '_')}.keras\"\n",
    "        detector.model.save(model_name)\n",
    "        print(f\"✅ Saved: {model_name}\")\n",
    "    \n",
    "    # Confronto finale\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"🏆 FINAL COMPARISON OF ALL CONFIGURATIONS\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    comparison_df = pd.DataFrame(results).T\n",
    "    print(comparison_df.round(3))\n",
    "    \n",
    "    # Trova migliore configurazione\n",
    "    best_config = comparison_df['f1'].idxmax()\n",
    "    best_f1 = comparison_df['f1'].max()\n",
    "    \n",
    "    print(f\"\\n🥇 BEST CONFIGURATION: {best_config}\")\n",
    "    print(f\"   F1-Score: {best_f1:.3f}\")\n",
    "    print(f\"   Recall: {comparison_df.loc[best_config, 'recall']:.3f}\")\n",
    "    print(f\"   Precision: {comparison_df.loc[best_config, 'precision']:.3f}\")\n",
    "    \n",
    "    return results, detector\n",
    "\n",
    "# Esegui training ottimizzato\n",
    "print(\"Starting optimized training with all techniques...\")\n",
    "results, best_detector = main_optimized()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f41bed54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Processing Memory Utilization with Simple Isolation Forest:\n",
      "  üîç CPU STRESS:\n",
      "    ‚úÖ Cleaned 10 outliers (2.1%) using 1 original features\n",
      "  üîç BASELINE:\n",
      "    ‚úÖ Cleaned 8 outliers (1.6%) using 1 original features\n",
      "\n",
      "üìä Processing Memory Cache with Simple Isolation Forest:\n",
      "  üîç CPU STRESS:\n",
      "    ‚úÖ Cleaned 10 outliers (2.1%) using 1 original features\n",
      "  üîç BASELINE:\n",
      "    ‚úÖ Cleaned 8 outliers (1.6%) using 1 original features\n",
      "\n",
      "üìä Processing Memory Available with Simple Isolation Forest:\n",
      "  üîç CPU STRESS:\n",
      "    ‚úÖ Cleaned 10 outliers (2.1%) using 1 original features\n",
      "  üîç BASELINE:\n",
      "    ‚úÖ Cleaned 8 outliers (1.6%) using 1 original features\n",
      "\n",
      "================================================================================\n",
      "COMPREHENSIVE MEMORY CPU STRESS ANALYSIS (FIXED)\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from scipy import stats as scipy_stats\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Load all memory datasets\n",
    "df_mem_util = pd.read_csv(\"mem_util.csv\")\n",
    "df_mem_cache = pd.read_csv(\"mem_cache.csv\")\n",
    "df_mem_available = pd.read_csv(\"mem_available.csv\")\n",
    "\n",
    "df_baseline_mem_util = pd.read_csv(\"../../baseline/memory related/mem_util.csv\")\n",
    "df_baseline_mem_cache = pd.read_csv(\"../../baseline/memory related/mem_cache.csv\")\n",
    "df_baseline_mem_available = pd.read_csv(\"../../baseline/memory related/mem_available.csv\")\n",
    "\n",
    "# Add source labels for CPU STRESS experiment\n",
    "df_mem_util[\"source\"] = \"CPU_STRESS\"\n",
    "df_mem_cache[\"source\"] = \"CPU_STRESS\"\n",
    "df_mem_available[\"source\"] = \"CPU_STRESS\"\n",
    "\n",
    "df_baseline_mem_util[\"source\"] = \"BASELINE\"\n",
    "df_baseline_mem_cache[\"source\"] = \"BASELINE\"\n",
    "df_baseline_mem_available[\"source\"] = \"BASELINE\"\n",
    "\n",
    "# Convert to datetime\n",
    "df_mem_util[\"Time\"] = pd.to_datetime(df_mem_util[\"Time\"])\n",
    "df_mem_cache[\"Time\"] = pd.to_datetime(df_mem_cache[\"Time\"])\n",
    "df_mem_available[\"Time\"] = pd.to_datetime(df_mem_available[\"Time\"])\n",
    "\n",
    "df_baseline_mem_util[\"Time\"] = pd.to_datetime(df_baseline_mem_util[\"Time\"])\n",
    "df_baseline_mem_cache[\"Time\"] = pd.to_datetime(df_baseline_mem_cache[\"Time\"])\n",
    "df_baseline_mem_available[\"Time\"] = pd.to_datetime(df_baseline_mem_available[\"Time\"])\n",
    "\n",
    "delay = 30\n",
    "duration = 50\n",
    "\n",
    "# Synchronize test datasets with baseline timeline\n",
    "time_offset = df_baseline_mem_util[\"Time\"].min() - df_mem_util[\"Time\"].min()\n",
    "df_mem_util[\"Time\"] += time_offset\n",
    "df_mem_cache[\"Time\"] += time_offset\n",
    "df_mem_available[\"Time\"] += time_offset\n",
    "\n",
    "# Convert timeline to minutes\n",
    "all_dfs = [\n",
    "    df_mem_util, df_mem_cache, df_mem_available,\n",
    "    df_baseline_mem_util, df_baseline_mem_cache, df_baseline_mem_available\n",
    "]\n",
    "\n",
    "for df in all_dfs:\n",
    "    df[\"Minutes\"] = (df[\"Time\"] - df[\"Time\"].min()).dt.total_seconds() / 60\n",
    "\n",
    "# STORE ORIGINAL DATASETS BEFORE CLEANING\n",
    "original_datasets = {\n",
    "    'Memory Utilization': {'cpu_stress': df_mem_util.copy(), 'baseline': df_baseline_mem_util.copy()},\n",
    "    'Memory Cache': {'cpu_stress': df_mem_cache.copy(), 'baseline': df_baseline_mem_cache.copy()},\n",
    "    'Memory Available': {'cpu_stress': df_mem_available.copy(), 'baseline': df_baseline_mem_available.copy()}\n",
    "}\n",
    "\n",
    "# SIMPLE ISOLATION FOREST OUTLIER DETECTION FOR MEMORY METRICS\n",
    "def remove_outliers_isolation_forest_memory(df, contamination=0.02):\n",
    "    \"\"\"Simple Isolation Forest outlier detection - uses only original numeric columns\"\"\"\n",
    "    df_clean = df.copy()\n",
    "    outlier_stats = {}\n",
    "    \n",
    "    # Get all numeric columns except Time, Minutes, and source\n",
    "    numeric_cols = [col for col in df.select_dtypes(include=[np.number]).columns \n",
    "                   if col not in ['Time', 'Minutes']]\n",
    "    \n",
    "    if len(numeric_cols) == 0:\n",
    "        outlier_stats['_summary'] = {\n",
    "            'total_outliers': 0,\n",
    "            'contamination_rate': contamination,\n",
    "            'features_used': 0,\n",
    "            'percentage_removed': 0\n",
    "        }\n",
    "        return df_clean, outlier_stats\n",
    "    \n",
    "    # Use ONLY the original numeric columns - no feature engineering\n",
    "    feature_matrix = df[numeric_cols].fillna(0).values\n",
    "    \n",
    "    # Check if there's enough variance to detect outliers\n",
    "    if np.std(feature_matrix.flatten()) > 1e-10:\n",
    "        # Scale features for better performance\n",
    "        scaler = StandardScaler()\n",
    "        feature_matrix_scaled = scaler.fit_transform(feature_matrix)\n",
    "        \n",
    "        # Initialize and fit Isolation Forest\n",
    "        iso_forest = IsolationForest(\n",
    "            contamination=contamination,\n",
    "            n_estimators=100,\n",
    "            random_state=42,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "        \n",
    "        # Fit and predict\n",
    "        iso_forest.fit(feature_matrix_scaled)\n",
    "        outlier_predictions = iso_forest.predict(feature_matrix_scaled)\n",
    "        \n",
    "        # Create outlier mask (-1 = outlier, 1 = normal)\n",
    "        outlier_mask = outlier_predictions == -1\n",
    "        \n",
    "        # Apply outlier removal to each numeric column\n",
    "        for col in numeric_cols:\n",
    "            outlier_stats[col] = {\n",
    "                'count': outlier_mask.sum(),\n",
    "                'percentage': (outlier_mask.sum() / len(df[col])) * 100,\n",
    "                'method': 'isolation_forest_simple'\n",
    "            }\n",
    "            \n",
    "            # Set outliers to NaN and interpolate\n",
    "            df_clean.loc[outlier_mask, col] = np.nan\n",
    "            df_clean[col] = df_clean[col].interpolate(method='linear').fillna(0)\n",
    "        \n",
    "        # Overall statistics\n",
    "        outlier_stats['_summary'] = {\n",
    "            'total_outliers': outlier_mask.sum(),\n",
    "            'contamination_rate': contamination,\n",
    "            'features_used': len(numeric_cols),\n",
    "            'percentage_removed': (outlier_mask.sum() / len(df)) * 100\n",
    "        }\n",
    "    else:\n",
    "        # No variance - no outliers to remove\n",
    "        for col in numeric_cols:\n",
    "            outlier_stats[col] = {\n",
    "                'count': 0,\n",
    "                'percentage': 0,\n",
    "                'method': 'isolation_forest_simple'\n",
    "            }\n",
    "        \n",
    "        outlier_stats['_summary'] = {\n",
    "            'total_outliers': 0,\n",
    "            'contamination_rate': contamination,\n",
    "            'features_used': len(numeric_cols),\n",
    "            'percentage_removed': 0\n",
    "        }\n",
    "    \n",
    "    return df_clean, outlier_stats\n",
    "\n",
    "# OUTLIER CLEANING WITH TRACKING\n",
    "datasets_clean = {}\n",
    "all_outlier_stats = {}\n",
    "\n",
    "for dataset_name, dataset_pair in original_datasets.items():\n",
    "    print(f\"\\nüìä Processing {dataset_name} with Simple Isolation Forest:\")\n",
    "    \n",
    "    datasets_clean[dataset_name] = {}\n",
    "    all_outlier_stats[dataset_name] = {}\n",
    "    \n",
    "    for source_type, df in dataset_pair.items():\n",
    "        print(f\"  üîç {source_type.upper().replace('_', ' ')}:\")\n",
    "        \n",
    "        # Apply Simple Isolation Forest outlier detection\n",
    "        df_clean, stats = remove_outliers_isolation_forest_memory(df, contamination=0.02)\n",
    "        \n",
    "        datasets_clean[dataset_name][source_type] = df_clean\n",
    "        all_outlier_stats[dataset_name][source_type] = stats\n",
    "        \n",
    "        # Print outlier summary\n",
    "        total_outliers = stats['_summary']['total_outliers']\n",
    "        features_used = stats['_summary']['features_used']\n",
    "        percentage_removed = stats['_summary']['percentage_removed']\n",
    "        print(f\"    ‚úÖ Cleaned {total_outliers} outliers ({percentage_removed:.1f}%) using {features_used} original features\")\n",
    "\n",
    "# ADD PHASE COLUMN \n",
    "def add_phase_column(df, delay_minutes, duration_minutes):\n",
    "    df = df.copy()\n",
    "    df['phase'] = 'before'  \n",
    "    df.loc[(df['Minutes'] >= delay_minutes) & (df['Minutes'] <= delay_minutes + duration_minutes), 'phase'] = 'during'  \n",
    "    df.loc[df['Minutes'] > delay_minutes + duration_minutes, 'phase'] = 'after' \n",
    "    return df\n",
    "\n",
    "# FIXED CACHE-SPECIFIC ANALYSIS FUNCTION\n",
    "def analyze_cache_properly(dataset_name, cpu_stress_df_clean, baseline_df_clean, delay_minutes, duration_minutes):\n",
    "    \"\"\"\n",
    "    Proper cache analysis without harmful normalization\n",
    "    Focuses on absolute values and real cache behavior patterns\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"üîç PROPER CACHE ANALYSIS FOR {dataset_name}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Get cache column (assuming it's the main numeric column)\n",
    "    numeric_cols = [col for col in cpu_stress_df_clean.select_dtypes(include=[np.number]).columns \n",
    "                   if col not in ['Time', 'Minutes']]\n",
    "    \n",
    "    if not numeric_cols:\n",
    "        print(\"‚ùå No numeric cache columns found\")\n",
    "        return None\n",
    "        \n",
    "    cache_col = numeric_cols[0]  # Assuming main cache metric\n",
    "    print(f\"üìä Analyzing cache metric: {cache_col}\")\n",
    "    \n",
    "    # Add phase information WITHOUT normalization\n",
    "    cpu_stress_df = add_phase_column(cpu_stress_df_clean.copy(), delay_minutes, duration_minutes)\n",
    "    baseline_df = add_phase_column(baseline_df_clean.copy(), delay_minutes, duration_minutes)\n",
    "    \n",
    "    # Calculate cache statistics in ORIGINAL units\n",
    "    cache_stats_baseline = baseline_df.groupby('phase')[cache_col].agg([\n",
    "        'mean', 'std', 'min', 'max', 'count'\n",
    "    ]).round(0)\n",
    "    \n",
    "    cache_stats_cpu_stress = cpu_stress_df.groupby('phase')[cache_col].agg([\n",
    "        'mean', 'std', 'min', 'max', 'count'  \n",
    "    ]).round(0)\n",
    "    \n",
    "    # Calculate cache growth rates\n",
    "    def calculate_growth_rate(df, col):\n",
    "        \"\"\"Calculate cache growth rate per minute\"\"\"\n",
    "        df_sorted = df.sort_values('Minutes')\n",
    "        df_sorted['cache_change'] = df_sorted[col].diff()\n",
    "        df_sorted['time_change'] = df_sorted['Minutes'].diff()\n",
    "        df_sorted['growth_rate'] = df_sorted['cache_change'] / df_sorted['time_change']\n",
    "        return df_sorted\n",
    "    \n",
    "    baseline_with_growth = calculate_growth_rate(baseline_df, cache_col)\n",
    "    cpu_stress_with_growth = calculate_growth_rate(cpu_stress_df, cache_col)\n",
    "    \n",
    "    # Cache pressure events (significant drops)\n",
    "    def find_cache_pressure_events(df, col, threshold_kb=50000):\n",
    "        \"\"\"Find significant cache drops indicating memory pressure\"\"\"\n",
    "        df_sorted = df.sort_values('Minutes')\n",
    "        cache_drops = df_sorted[col].diff()\n",
    "        pressure_events = df_sorted[cache_drops < -threshold_kb]\n",
    "        return pressure_events\n",
    "    \n",
    "    baseline_pressure = find_cache_pressure_events(baseline_df, cache_col)\n",
    "    cpu_stress_pressure = find_cache_pressure_events(cpu_stress_df, cache_col)\n",
    "    \n",
    "    # Statistical tests on ABSOLUTE values\n",
    "    t_tests = {}\n",
    "    absolute_differences = {}\n",
    "    \n",
    "    for phase in ['before', 'during', 'after']:\n",
    "        baseline_data = baseline_df[baseline_df['phase'] == phase][cache_col].dropna()\n",
    "        cpu_stress_data = cpu_stress_df[cpu_stress_df['phase'] == phase][cache_col].dropna()\n",
    "        \n",
    "        if len(baseline_data) > 1 and len(cpu_stress_data) > 1:\n",
    "            t_stat, p_value = scipy_stats.ttest_ind(baseline_data, cpu_stress_data)\n",
    "            t_tests[phase] = {'t': t_stat, 'p': p_value}\n",
    "            \n",
    "            # Calculate ABSOLUTE difference in KB\n",
    "            baseline_mean = baseline_data.mean()\n",
    "            cpu_stress_mean = cpu_stress_data.mean()\n",
    "            absolute_differences[phase] = {\n",
    "                'absolute_kb': cpu_stress_mean - baseline_mean,\n",
    "                'percentage': ((cpu_stress_mean - baseline_mean) / baseline_mean) * 100 if baseline_mean != 0 else 0\n",
    "            }\n",
    "        else:\n",
    "            t_tests[phase] = {'t': 0, 'p': 1.0}\n",
    "            absolute_differences[phase] = {'absolute_kb': 0, 'percentage': 0}\n",
    "    \n",
    "    # PLOTTING - NO NORMALIZATION\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
    "    \n",
    "    # 1. Cache Timeline (Absolute Values)\n",
    "    axes[0,0].plot(baseline_df['Minutes'], baseline_df[cache_col]/1024, \n",
    "                   label='Baseline', linewidth=2, color='blue')\n",
    "    axes[0,0].plot(cpu_stress_df['Minutes'], cpu_stress_df[cache_col]/1024, \n",
    "                   label='CPU Stress', linewidth=2, color='red')\n",
    "    axes[0,0].axvspan(delay_minutes, delay_minutes + duration_minutes, \n",
    "                      color='red', alpha=0.2, label=\"CPU Stress Period\")\n",
    "    axes[0,0].set_title('Cache Usage Over Time (MB)', fontweight='bold')\n",
    "    axes[0,0].set_xlabel('Minutes')\n",
    "    axes[0,0].set_ylabel('Cache Size (MB)')\n",
    "    axes[0,0].legend()\n",
    "    axes[0,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Cache Growth Rates\n",
    "    axes[0,1].plot(baseline_with_growth['Minutes'], baseline_with_growth['growth_rate'], \n",
    "                   label='Baseline Growth', alpha=0.7, color='blue')\n",
    "    axes[0,1].plot(cpu_stress_with_growth['Minutes'], cpu_stress_with_growth['growth_rate'], \n",
    "                   label='CPU Stress Growth', alpha=0.7, color='red')\n",
    "    axes[0,1].axhline(y=0, color='black', linestyle='-', alpha=0.5)\n",
    "    axes[0,1].axvspan(delay_minutes, delay_minutes + duration_minutes, \n",
    "                      color='red', alpha=0.2)\n",
    "    axes[0,1].set_title('Cache Growth Rate (KB/min)', fontweight='bold')\n",
    "    axes[0,1].set_xlabel('Minutes')\n",
    "    axes[0,1].set_ylabel('Growth Rate (KB/min)')\n",
    "    axes[0,1].legend()\n",
    "    axes[0,1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Cache Statistics by Phase\n",
    "    phases = ['before', 'during', 'after']\n",
    "    x = np.arange(len(phases))\n",
    "    width = 0.35\n",
    "    \n",
    "    baseline_means = [cache_stats_baseline.loc[phase, 'mean']/1024 for phase in phases]\n",
    "    cpu_stress_means = [cache_stats_cpu_stress.loc[phase, 'mean']/1024 for phase in phases]\n",
    "    \n",
    "    axes[0,2].bar(x - width/2, baseline_means, width, label='Baseline', alpha=0.8, color='blue')\n",
    "    axes[0,2].bar(x + width/2, cpu_stress_means, width, label='CPU Stress', alpha=0.8, color='red')\n",
    "    axes[0,2].set_title('Average Cache Size by Phase (MB)', fontweight='bold')\n",
    "    axes[0,2].set_xlabel('Phase')\n",
    "    axes[0,2].set_ylabel('Average Cache (MB)')\n",
    "    axes[0,2].set_xticks(x)\n",
    "    axes[0,2].set_xticklabels(phases)\n",
    "    axes[0,2].legend()\n",
    "    axes[0,2].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # 4. Cache Pressure Events\n",
    "    if len(baseline_pressure) > 0 or len(cpu_stress_pressure) > 0:\n",
    "        all_times = list(baseline_df['Minutes']) + list(cpu_stress_df['Minutes'])\n",
    "        axes[1,0].hist([baseline_pressure['Minutes'], cpu_stress_pressure['Minutes']], \n",
    "                       bins=20, alpha=0.7, label=['Baseline Pressure', 'CPU Stress Pressure'],\n",
    "                       color=['blue', 'red'])\n",
    "    axes[1,0].axvspan(delay_minutes, delay_minutes + duration_minutes, \n",
    "                      color='red', alpha=0.2)\n",
    "    axes[1,0].set_title('Cache Pressure Events', fontweight='bold')\n",
    "    axes[1,0].set_xlabel('Minutes')\n",
    "    axes[1,0].set_ylabel('Pressure Event Count')\n",
    "    axes[1,0].legend()\n",
    "    axes[1,0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 5. Absolute Differences\n",
    "    abs_diffs = [absolute_differences[phase]['absolute_kb']/1024 for phase in phases]\n",
    "    colors = ['green' if x >= 0 else 'orange' for x in abs_diffs]\n",
    "    \n",
    "    axes[1,1].bar(phases, abs_diffs, color=colors, alpha=0.7)\n",
    "    axes[1,1].axhline(y=0, color='black', linestyle='-', alpha=0.5)\n",
    "    axes[1,1].set_title('Cache Impact: CPU Stress - Baseline (MB)', fontweight='bold')\n",
    "    axes[1,1].set_xlabel('Phase')\n",
    "    axes[1,1].set_ylabel('Cache Difference (MB)')\n",
    "    axes[1,1].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # 6. Distribution Comparison\n",
    "    baseline_all = baseline_df[cache_col] / 1024\n",
    "    cpu_stress_all = cpu_stress_df[cache_col] / 1024\n",
    "    \n",
    "    axes[1,2].hist([baseline_all, cpu_stress_all], bins=30, alpha=0.7, \n",
    "                   label=['Baseline', 'CPU Stress'], color=['blue', 'red'])\n",
    "    axes[1,2].set_title('Cache Size Distribution (MB)', fontweight='bold')\n",
    "    axes[1,2].set_xlabel('Cache Size (MB)')\n",
    "    axes[1,2].set_ylabel('Frequency')\n",
    "    axes[1,2].legend()\n",
    "    axes[1,2].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.suptitle(f'{dataset_name}: Proper Cache Analysis (No Harmful Normalization)', \n",
    "                 fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # RESULTS SUMMARY\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"CACHE ANALYSIS RESULTS (ABSOLUTE VALUES)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    print(f\"\\nüìä BASELINE CACHE STATISTICS (KB):\")\n",
    "    print(cache_stats_baseline)\n",
    "    \n",
    "    print(f\"\\nüìä CPU STRESS CACHE STATISTICS (KB):\")\n",
    "    print(cache_stats_cpu_stress)\n",
    "    \n",
    "    print(f\"\\nüî¨ STATISTICAL SIGNIFICANCE:\")\n",
    "    for phase, test in t_tests.items():\n",
    "        sig = \"‚úÖ SIGNIFICANT\" if test['p'] < 0.05 else \"‚ùå NOT SIGNIFICANT\"\n",
    "        print(f\"  {phase.upper():8}: t={test['t']:6.2f}, p={test['p']:8.4f} ({sig})\")\n",
    "    \n",
    "    print(f\"\\nüìà ABSOLUTE CACHE IMPACT:\")\n",
    "    for phase in phases:\n",
    "        abs_kb = absolute_differences[phase]['absolute_kb']\n",
    "        pct = absolute_differences[phase]['percentage']\n",
    "        direction = \"‚ÜóÔ∏è INCREASE\" if abs_kb > 0 else \"‚ÜòÔ∏è DECREASE\" if abs_kb < 0 else \"‚Üí NO CHANGE\"\n",
    "        print(f\"  {phase.upper():8}: {abs_kb:+7.0f} KB ({pct:+5.1f}%) {direction}\")\n",
    "    \n",
    "    print(f\"\\n‚ö†Ô∏è  CACHE PRESSURE EVENTS:\")\n",
    "    print(f\"  Baseline: {len(baseline_pressure)} events\")\n",
    "    print(f\"  CPU Stress: {len(cpu_stress_pressure)} events\")\n",
    "    \n",
    "    return {\n",
    "        'cache_stats_baseline': cache_stats_baseline,\n",
    "        'cache_stats_cpu_stress': cache_stats_cpu_stress,\n",
    "        't_tests': t_tests,\n",
    "        'absolute_differences': absolute_differences,\n",
    "        'pressure_events': {\n",
    "            'baseline': len(baseline_pressure),\n",
    "            'cpu_stress': len(cpu_stress_pressure)\n",
    "        }\n",
    "    }\n",
    "\n",
    "# NORMALIZATION FUNCTION (for non-cache metrics)\n",
    "def normalize_df_memory(df, columns):\n",
    "    result = df.copy()\n",
    "    for col in columns:\n",
    "        min_val = df[col].min()\n",
    "        max_val = df[col].max()\n",
    "        if max_val > min_val:  \n",
    "            result[col] = (df[col] - min_val) / (max_val - min_val)\n",
    "        else:\n",
    "            result[col] = 0\n",
    "    return result\n",
    "\n",
    "# ORIGINAL ANALYSIS FUNCTION FOR NON-CACHE METRICS\n",
    "def analyze_and_plot_memory_dataset_wide(dataset_name, cpu_stress_df_clean, baseline_df_clean, delay_minutes, duration_minutes):\n",
    "    \n",
    "    # Get original datasets for comparison\n",
    "    cpu_stress_df_original = original_datasets[dataset_name]['cpu_stress']\n",
    "    baseline_df_original = original_datasets[dataset_name]['baseline']\n",
    "    \n",
    "    # Get numeric columns for memory metrics\n",
    "    numeric_cols = [col for col in cpu_stress_df_clean.select_dtypes(include=[np.number]).columns \n",
    "                   if col not in ['Time', 'Minutes']]\n",
    "    \n",
    "    cpu_stress_norm = normalize_df_memory(cpu_stress_df_clean, numeric_cols)\n",
    "    baseline_norm = normalize_df_memory(baseline_df_clean, numeric_cols)\n",
    "    \n",
    "    # For memory metrics, we typically have single metrics, so we'll use the main metric\n",
    "    main_metric = numeric_cols[0] if numeric_cols else None\n",
    "    \n",
    "    if main_metric is None:\n",
    "        print(f\"No numeric columns found for {dataset_name}\")\n",
    "        return None\n",
    "    \n",
    "    # Add phase information\n",
    "    cpu_stress_norm = add_phase_column(cpu_stress_norm, delay_minutes, duration_minutes)\n",
    "    baseline_norm = add_phase_column(baseline_norm, delay_minutes, duration_minutes)\n",
    "    \n",
    "    # Statistical analysis\n",
    "    stats_baseline = baseline_norm.groupby('phase')[main_metric].agg(['mean', 'std', 'min', 'max'])\n",
    "    stats_cpu_stress = cpu_stress_norm.groupby('phase')[main_metric].agg(['mean', 'std', 'min', 'max'])\n",
    "    \n",
    "    # Combined dataframe for analysis\n",
    "    df_combined = pd.DataFrame({\n",
    "        'Baseline': baseline_norm[main_metric],\n",
    "        'CPU_STRESS': cpu_stress_norm[main_metric],\n",
    "        'Minutes': baseline_norm['Minutes'],\n",
    "        'phase': baseline_norm['phase'],\n",
    "        'difference': cpu_stress_norm[main_metric] - baseline_norm[main_metric]\n",
    "    })\n",
    "    \n",
    "    # T-tests for statistical significance\n",
    "    t_tests = {}\n",
    "    for phase in ['before', 'during', 'after']:\n",
    "        data = df_combined[df_combined['phase'] == phase]\n",
    "        t_stat, p_value = scipy_stats.ttest_ind(data['Baseline'].dropna(), data['CPU_STRESS'].dropna())\n",
    "        t_tests[phase] = {'t': t_stat, 'p': p_value}\n",
    "    \n",
    "    # Calculate percentage impact\n",
    "    impact = {}\n",
    "    for phase in ['before', 'during', 'after']:\n",
    "        baseline_mean = stats_baseline.loc[phase, 'mean']\n",
    "        cpu_stress_mean = stats_cpu_stress.loc[phase, 'mean']\n",
    "        impact[phase] = ((cpu_stress_mean - baseline_mean) / baseline_mean) * 100 if baseline_mean != 0 else float('inf')\n",
    "    \n",
    "    # PLOTTING (FIXED TO SHOW ORIGINAL VS CLEANED)\n",
    "    fig = plt.figure(figsize=(24, 12))\n",
    "    \n",
    "    # 1. Baseline outlier cleaning effect\n",
    "    plt.subplot(2, 3, 1)\n",
    "    plt.plot(baseline_df_original['Minutes'], baseline_df_original[main_metric], \n",
    "             label='Baseline (with outliers)', color='orange', alpha=0.7, linewidth=1)\n",
    "    plt.plot(baseline_df_clean['Minutes'], baseline_df_clean[main_metric], \n",
    "             label='Baseline (cleaned)', linewidth=3, color='blue')\n",
    "    plt.title(f'{dataset_name}: Baseline Outlier Cleaning Effect\\n(Simple Isolation Forest)', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Minutes', fontsize=12)\n",
    "    plt.ylabel(f'{main_metric}', fontsize=12)\n",
    "    plt.legend(fontsize=10)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. CPU STRESS outlier cleaning effect\n",
    "    plt.subplot(2, 3, 2)\n",
    "    plt.plot(cpu_stress_df_original['Minutes'], cpu_stress_df_original[main_metric], \n",
    "             label='CPU STRESS (with outliers)', alpha=0.7, color='lightcoral', linewidth=1)\n",
    "    plt.plot(cpu_stress_df_clean['Minutes'], cpu_stress_df_clean[main_metric], \n",
    "             label='CPU STRESS (cleaned)', linewidth=3, color='darkred')\n",
    "    plt.title(f'{dataset_name}: CPU Stress Outlier Cleaning Effect\\n(Simple Isolation Forest)', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Minutes', fontsize=12)\n",
    "    plt.ylabel(f'{main_metric}', fontsize=12)\n",
    "    plt.legend(fontsize=10)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Timeline comparison\n",
    "    plt.subplot(2, 3, 3)\n",
    "    plt.plot(baseline_norm['Minutes'], baseline_norm[main_metric], \n",
    "             label='Baseline', linewidth=3, color='blue', alpha=0.9)\n",
    "    plt.plot(cpu_stress_norm['Minutes'], cpu_stress_norm[main_metric], \n",
    "             label='CPU Stress', linewidth=3, color='red', alpha=0.9)\n",
    "    plt.axvspan(delay_minutes, delay_minutes + duration_minutes, \n",
    "                color='red', alpha=0.2, label=\"CPU Stress Period\")\n",
    "    plt.title(f\"{dataset_name}: Baseline vs CPU Stress\", fontsize=14, fontweight='bold')\n",
    "    plt.xlabel(\"Minutes\", fontsize=12)\n",
    "    plt.ylabel(f\"Normalized {main_metric}\", fontsize=12)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend(fontsize=10)\n",
    "    \n",
    "    # 4. Box plot by phase\n",
    "    plt.subplot(2, 3, 4)\n",
    "    box_data = pd.melt(df_combined[['Baseline', 'CPU_STRESS', 'phase']], \n",
    "                      id_vars=['phase'], var_name='source', value_name='value')\n",
    "    sns.boxplot(x='phase', y='value', hue='source', data=box_data, ax=plt.gca())\n",
    "    plt.title(f'{dataset_name}: Distribution by Phase', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Phase', fontsize=12)\n",
    "    plt.ylabel('Normalized Values', fontsize=12)\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    plt.legend(fontsize=10)\n",
    "    \n",
    "    # 5. Statistics by phase\n",
    "    plt.subplot(2, 3, 5)\n",
    "    phases = ['before', 'during', 'after']\n",
    "    x = np.arange(len(phases))\n",
    "    width = 0.35\n",
    "    \n",
    "    bars1 = plt.bar(x - width/2, stats_baseline['mean'], width, label='Baseline', alpha=0.8, color='blue')\n",
    "    bars2 = plt.bar(x + width/2, stats_cpu_stress['mean'], width, label='CPU Stress', alpha=0.8, color='red')\n",
    "    \n",
    "    plt.errorbar(x - width/2, stats_baseline['mean'], yerr=stats_baseline['std'], \n",
    "                fmt='none', ecolor='black', capsize=5, linewidth=2)\n",
    "    plt.errorbar(x + width/2, stats_cpu_stress['mean'], yerr=stats_cpu_stress['std'], \n",
    "                fmt='none', ecolor='black', capsize=5, linewidth=2)\n",
    "    \n",
    "    plt.title(f'{dataset_name}: Average by Phase', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Phase', fontsize=12)\n",
    "    plt.ylabel('Mean Normalized Values', fontsize=12)\n",
    "    plt.xticks(x, phases)\n",
    "    plt.legend(fontsize=10)\n",
    "    plt.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # 6. Difference over time\n",
    "    plt.subplot(2, 3, 6)\n",
    "    plt.plot(df_combined['Minutes'], df_combined['difference'], \n",
    "             color='purple', linewidth=2, alpha=0.8)\n",
    "    plt.axhline(y=0, color='black', linestyle='-', alpha=0.5, linewidth=1)\n",
    "    plt.axvspan(delay_minutes, delay_minutes + duration_minutes, \n",
    "                color='red', alpha=0.2)\n",
    "    plt.title(f'{dataset_name}: CPU Stress - Baseline Difference', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel('Minutes', fontsize=12)\n",
    "    plt.ylabel('Difference', fontsize=12)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.suptitle(f'{dataset_name} Memory CPU Stress Analysis ', \n",
    "                fontsize=18, fontweight='bold', y=0.98)\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=0.90)\n",
    "    plt.show()\n",
    "    \n",
    "    # Print statistical results\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"{dataset_name.upper()} - CPU STRESS ANALYSIS RESULTS\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    print(f\"\\nüìä BASELINE STATISTICS BY PHASE:\")\n",
    "    print(stats_baseline.round(4))\n",
    "    \n",
    "    print(f\"\\nüìä CPU STRESS STATISTICS BY PHASE:\")\n",
    "    print(stats_cpu_stress.round(4))\n",
    "    \n",
    "    print(f\"\\nüî¨ STATISTICAL SIGNIFICANCE TESTS:\")\n",
    "    for phase, test in t_tests.items():\n",
    "        sig = \"‚úÖ SIGNIFICANT\" if test['p'] < 0.05 else \"‚ùå NOT SIGNIFICANT\"\n",
    "        print(f\"  {phase.upper():8}: t={test['t']:6.2f}, p={test['p']:8.4f} ({sig})\")\n",
    "    \n",
    "    print(f\"\\nüìà CPU STRESS IMPACT ON MEMORY (% CHANGE):\")\n",
    "    for phase, change in impact.items():\n",
    "        direction = \"‚ÜóÔ∏è INCREASE\" if change > 0 else \"‚ÜòÔ∏è DECREASE\" if change < 0 else \"‚Üí NO CHANGE\"\n",
    "        print(f\"  {phase.upper():8}: {change:+7.2f}% ({direction})\")\n",
    "    \n",
    "    return {\n",
    "        'stats_baseline': stats_baseline,\n",
    "        'stats_cpu_stress': stats_cpu_stress,\n",
    "        't_tests': t_tests,\n",
    "        'impact': impact,\n",
    "        'combined_data': df_combined,\n",
    "        'main_metric': main_metric\n",
    "    }\n",
    "\n",
    "# RUN ANALYSIS WITH CACHE-SPECIFIC HANDLING\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"COMPREHENSIVE MEMORY CPU STRESS ANALYSIS (FIXED)\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "analysis_results = {}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

processo tesi:

 lettura paper
 
 inizialmente legolas --> problema bias presenza della logica di controllo
 
 spostati su jvm fault specifici
 
 paper grayscope
 
 set up openEuler vm

 dettagli vm hw

 open euler struttura, funzionamento, e come interagito

 versione distirbuita di petclinic overview, perchè petclinic distribuito, jmeter load file configurazione (docker e docker compose) perchè rispetto a non container
 
 prometheus, grafana

 collegamento gala gopher e prometheus (stesso formato), metriche selezionate e perchè

 chaosblade overview per poi arrivare a cri

 microservizi che ho scelto per iniettare failure e perchè
 
 failure injection plan, quali ho scelto, perchè, approccio di granularità (fai riferimento ai paper)

 approccio network injection (non sul bridge ma sulla veth specifica del container che collega al bridge per isolare failure
 per ottenere gray failure simile)

 perchè spostato su server usi (oltre 30 thread problemi risorse)

 spostato fa virtualbox a kvm kemu per misconfigurazione tra linux header del server e virtualbox. il kernel del server è specifico
  (usato scp per VM transfer, installed libvm qemu-kvm, convertito i file e startata la nuova vm)
  creazione tunnel ssh per accedere alla visualizzazione della vm che gira sul server tramite SPICE usata con  Remote-Viewer/Virt-Viewer

parla dello stress plan di jmeter, e come è basato. 
  
partire test da 2h inizialmente con 40 threads, ma percentuale di errore sensibile. quindi abbassato a 35 threads per 2 ore.
parla dei calcoli sul traffico stimato di stressload

per mem e cpu stress modifica docker file per runnare chaosblade all-interno del container

plot per visualizzazione dati comparativa con baseline, poi rimozione outlier e fase di data preprocessing

suddivisione prima durante e dopo in fasi di iniezione di fault, per analisi descrivvitiva approfondita.

TUNA nel dettaglio. tutto l'algoritmo, come è stato adattato. ogni singola funziona e la strategia.

rapporto tra window, threshold, e oscillazione e variabilità della serie temporale (ricorda che il goal è denoising e  effettuare outlier removal). In OS ci sta che si abbiano diversi burst. 
quindi i parametri li adotti in base alla singola time series e gli spike presenti. nornali osccilazioni tramite ML denoising
per serie altamente variabili, window + piccole e threshold alte in modo da evitare over smoothing.
per serie pressoche costanti. window alte e threshold basse. se ho segnali stbaili con oscillazioni stabili le reputo normale comportamento di sistema. altrimenti spike

feature engineering: outliers and nois reduction through TUNA, added features for trends

aggiunta feature di trend, accelerazione momentum.

riduzione delle feature tramitemax relevance less redundancy dal paper.

scelta del numero di feature ottimali, pro e contro.

implementazione lstm, prima autoencoder ma gray failure sovrapponibili a baseline.

implemntazione di feature selction con algoritmo: A_Supervised_Deep_Learning_Framework_for_Proactive_Anomaly_Detection_in_Cloud_Workloads
Malhotra et al. (2015) - "Long Short Term Memory Networks for Anomaly Detection in Time Series" - usa 6-12 features
Su et al. (2019) - "Robust Anomaly Detection for Multivariate Time Series through Stochastic Recurrent Neural Network" - raccomanda <15 features
Audibert et al. (2020) - "USAD: UnSupervised Anomaly Detection on Multivariate Time Series" - usa 8-25 features con performance migliori sui numeri più bassi

tronato su autoencoder.

  

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe7fa8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def extract_metric_values(df, metric_type):\n",
    "    \"\"\"Extract values from different metric types\"\"\"\n",
    "    exclude_cols = ['Time', 'Minutes', 'source']\n",
    "    value_cols = [col for col in df.columns if col not in exclude_cols]\n",
    "    \n",
    "    if metric_type in ['tcp', 'srtt']:\n",
    "        if len(value_cols) == 1:\n",
    "            return df[value_cols[0]].values\n",
    "        else:\n",
    "            # For multi-column TCP data, sum across columns\n",
    "            return df[value_cols].fillna(0).sum(axis=1).values\n",
    "    elif metric_type == 'memory':\n",
    "        # Memory usually single column\n",
    "        return df[value_cols[0]].values if len(value_cols) == 1 else df[value_cols].sum(axis=1).values\n",
    "    elif metric_type == 'disk':\n",
    "        if 'BlockLatency' in df.columns or any('latency' in col.lower() for col in value_cols):\n",
    "            # For latency, take first device\n",
    "            return df[value_cols[0]].values\n",
    "        else:\n",
    "            # For throughput/bytes, sum across devices\n",
    "            return df[value_cols].sum(axis=1).values\n",
    "    elif metric_type == 'cpu':\n",
    "        # CPU usually single column or average\n",
    "        return df[value_cols[0]].values if len(value_cols) == 1 else df[value_cols].mean(axis=1).values\n",
    "    else:\n",
    "        return df[value_cols[0]].values\n",
    "\n",
    "def create_failure_labels(minutes, delay_minutes=30, duration_minutes=50, failure_threshold=0.3):\n",
    "    \"\"\"\n",
    "    Create failure labels based on experiment timeline\n",
    "    - 0: Normal (baseline + before stress)\n",
    "    - 1: Failure (during stress + some buffer after)\n",
    "    \"\"\"\n",
    "    labels = np.zeros(len(minutes))\n",
    "    \n",
    "    # Mark stress period and some buffer after as failure\n",
    "    stress_start = delay_minutes\n",
    "    stress_end = delay_minutes + duration_minutes\n",
    "    failure_buffer = duration_minutes * failure_threshold  # 30% of stress duration as buffer\n",
    "    \n",
    "    failure_mask = (minutes >= stress_start) & (minutes <= stress_end + failure_buffer)\n",
    "    labels[failure_mask] = 1\n",
    "    \n",
    "    return labels\n",
    "\n",
    "def prepare_dataset_for_rnn(all_datasets, tuna_results=None, use_cleaned=True, \n",
    "                           delay_minutes=30, duration_minutes=50, \n",
    "                           sequence_length=10, test_size=0.2):\n",
    "    \"\"\"\n",
    "    Prepare comprehensive dataset for RNN training across all metrics\n",
    "    \n",
    "    Parameters:\n",
    "    - all_datasets: Dictionary of all metric datasets\n",
    "    - tuna_results: Optional TUNA cleaned results\n",
    "    - use_cleaned: Whether to use TUNA cleaned data when available\n",
    "    - delay_minutes: When stress starts\n",
    "    - duration_minutes: How long stress lasts\n",
    "    - sequence_length: RNN sequence length (time steps)\n",
    "    - test_size: Fraction for test set\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"ðŸ”„ Preparing comprehensive dataset for RNN training...\")\n",
    "    \n",
    "    all_features = []\n",
    "    all_labels = []\n",
    "    all_metadata = []\n",
    "    feature_names = []\n",
    "    \n",
    "    # Define metric types for proper value extraction\n",
    "    metric_types = {\n",
    "        # TCP metrics\n",
    "        'ApiGateway': 'tcp', 'CustomersService': 'tcp', 'VetsService': 'tcp', \n",
    "        'VisitsService': 'tcp', 'SRTT': 'srtt', 'Network traffic': 'srtt',\n",
    "        \n",
    "        # Memory metrics\n",
    "        'MemAvailable': 'memory', 'MemCache': 'memory', 'MemUtil': 'memory',\n",
    "        'Memory Utilization': 'memory', 'Memory Cache': 'memory', \n",
    "        'Memory Available': 'memory', 'Memory Usage': 'memory',\n",
    "        \n",
    "        # Disk metrics\n",
    "        'BlockLatency': 'disk', 'ReadBytes': 'disk', 'WriteBytes': 'disk',\n",
    "        'DiskUtil': 'disk', 'IOPS': 'disk', 'ThroughputRead': 'disk', 'ThroughputWrite': 'disk',\n",
    "        \n",
    "        # CPU metrics\n",
    "        'CPUUtil': 'cpu', 'CPU Utilization': 'cpu', 'LoadAvg': 'cpu', 'CPU Usage': 'cpu'\n",
    "    }\n",
    "    \n",
    "    for metric_name, experiments in all_datasets.items():\n",
    "        metric_type = metric_types.get(metric_name, 'unknown')\n",
    "        print(f\"  ðŸ“Š Processing {metric_name} ({metric_type})...\")\n",
    "        \n",
    "        for experiment_name, experiment_df in experiments.items():\n",
    "            print(f\"    ðŸ” {experiment_name}...\")\n",
    "            \n",
    "            # Extract values (use TUNA cleaned if available and requested)\n",
    "            if (use_cleaned and tuna_results and metric_name in tuna_results and \n",
    "                experiment_name in tuna_results[metric_name]):\n",
    "                values = tuna_results[metric_name][experiment_name]['cleaned']\n",
    "                data_source = 'tuna_cleaned'\n",
    "            else:\n",
    "                values = extract_metric_values(experiment_df, metric_type)\n",
    "                data_source = 'original'\n",
    "            \n",
    "            # Get minutes for labeling\n",
    "            minutes = experiment_df['Minutes'].values\n",
    "            \n",
    "            # Ensure same length\n",
    "            min_length = min(len(values), len(minutes))\n",
    "            values = values[:min_length]\n",
    "            minutes = minutes[:min_length]\n",
    "            \n",
    "            # Create labels (0=normal, 1=failure)\n",
    "            if experiment_name == 'baseline':\n",
    "                labels = np.zeros(len(values))  # All normal for baseline\n",
    "            else:\n",
    "                labels = create_failure_labels(minutes, delay_minutes, duration_minutes)\n",
    "            \n",
    "            # Store features and metadata\n",
    "            all_features.append(values)\n",
    "            all_labels.append(labels)\n",
    "            all_metadata.extend([{\n",
    "                'metric_name': metric_name,\n",
    "                'metric_type': metric_type,\n",
    "                'experiment': experiment_name,\n",
    "                'data_source': data_source,\n",
    "                'time_index': i\n",
    "            } for i in range(len(values))])\n",
    "        \n",
    "        feature_names.append(f\"{metric_name}_{metric_type}\")\n",
    "    \n",
    "    print(f\"âœ… Collected data from {len(feature_names)} metrics\")\n",
    "    \n",
    "    # Combine all features into sequences\n",
    "    print(\"ðŸ”„ Creating sequences for RNN...\")\n",
    "    \n",
    "    # Find minimum length across all features\n",
    "    min_length = min(len(feat) for feat in all_features)\n",
    "    print(f\"  ðŸ“ Minimum sequence length: {min_length}\")\n",
    "    \n",
    "    # Truncate all to same length and create feature matrix\n",
    "    feature_matrix = np.column_stack([feat[:min_length] for feat in all_features])\n",
    "    label_vector = all_labels[0][:min_length]  # Use first experiment's labels as reference\n",
    "    \n",
    "    # Normalize features\n",
    "    scaler = StandardScaler()\n",
    "    feature_matrix_scaled = scaler.fit_transform(feature_matrix)\n",
    "    \n",
    "    print(f\"  ðŸ“Š Feature matrix shape: {feature_matrix_scaled.shape}\")\n",
    "    print(f\"  ðŸŽ¯ Label distribution: {np.bincount(label_vector.astype(int))}\")\n",
    "    \n",
    "    # Create sequences for LSTM\n",
    "    X_sequences = []\n",
    "    y_sequences = []\n",
    "    \n",
    "    for i in range(sequence_length, len(feature_matrix_scaled)):\n",
    "        X_sequences.append(feature_matrix_scaled[i-sequence_length:i])\n",
    "        y_sequences.append(label_vector[i])\n",
    "    \n",
    "    X_sequences = np.array(X_sequences)\n",
    "    y_sequences = np.array(y_sequences)\n",
    "    \n",
    "    print(f\"  ðŸ“¦ Sequence shapes: X={X_sequences.shape}, y={y_sequences.shape}\")\n",
    "    \n",
    "    # Split into train/test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_sequences, y_sequences, test_size=test_size, \n",
    "        random_state=42, stratify=y_sequences\n",
    "    )\n",
    "    \n",
    "    print(f\"  ðŸ”„ Train set: {X_train.shape}, Test set: {X_test.shape}\")\n",
    "    print(f\"  ðŸŽ¯ Train labels: {np.bincount(y_train.astype(int))}\")\n",
    "    print(f\"  ðŸŽ¯ Test labels: {np.bincount(y_test.astype(int))}\")\n",
    "    \n",
    "    return {\n",
    "        'X_train': X_train, 'X_test': X_test,\n",
    "        'y_train': y_train, 'y_test': y_test,\n",
    "        'scaler': scaler,\n",
    "        'feature_names': feature_names,\n",
    "        'metadata': {\n",
    "            'sequence_length': sequence_length,\n",
    "            'n_features': len(feature_names),\n",
    "            'total_samples': len(X_sequences),\n",
    "            'failure_rate': np.mean(y_sequences)\n",
    "        }\n",
    "    }\n",
    "\n",
    "def build_failure_detection_rnn(input_shape, learning_rate=0.001):\n",
    "    \"\"\"\n",
    "    Build RNN model for failure detection\n",
    "    \n",
    "    Parameters:\n",
    "    - input_shape: (sequence_length, n_features)\n",
    "    - learning_rate: Learning rate for optimizer\n",
    "    \"\"\"\n",
    "    \n",
    "    model = Sequential([\n",
    "        # First LSTM layer\n",
    "        LSTM(64, return_sequences=True, input_shape=input_shape),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        \n",
    "        # Second LSTM layer\n",
    "        LSTM(32, return_sequences=False),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        \n",
    "        # Dense layers\n",
    "        Dense(16, activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(8, activation='relu'),\n",
    "        \n",
    "        # Output layer (binary classification)\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=learning_rate),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy', 'precision', 'recall']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "def train_failure_detection_model(data_dict, epochs=50, batch_size=32, verbose=1):\n",
    "    \"\"\"\n",
    "    Train the RNN failure detection model\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"ðŸš€ Training failure detection RNN...\")\n",
    "    \n",
    "    # Extract data\n",
    "    X_train, X_test = data_dict['X_train'], data_dict['X_test']\n",
    "    y_train, y_test = data_dict['y_train'], data_dict['y_test']\n",
    "    \n",
    "    # Build model\n",
    "    input_shape = (X_train.shape[1], X_train.shape[2])\n",
    "    model = build_failure_detection_rnn(input_shape)\n",
    "    \n",
    "    print(f\"ðŸ“Š Model input shape: {input_shape}\")\n",
    "    print(f\"ðŸŽ¯ Training on {len(X_train)} samples, validating on {len(X_test)} samples\")\n",
    "    \n",
    "    # Callbacks\n",
    "    callbacks = [\n",
    "        EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True),\n",
    "        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-7)\n",
    "    ]\n",
    "    \n",
    "    # Train model\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_test, y_test),\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        callbacks=callbacks,\n",
    "        verbose=verbose\n",
    "    )\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "def evaluate_failure_detection_model(model, data_dict, plot_results=True):\n",
    "    \"\"\"\n",
    "    Evaluate the trained failure detection model\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"ðŸ“Š Evaluating failure detection model...\")\n",
    "    \n",
    "    X_test, y_test = data_dict['X_test'], data_dict['y_test']\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred_proba = model.predict(X_test, verbose=0)\n",
    "    y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "    \n",
    "    # Metrics\n",
    "    auc_score = roc_auc_score(y_test, y_pred_proba)\n",
    "    \n",
    "    print(f\"ðŸŽ¯ AUC Score: {auc_score:.4f}\")\n",
    "    print(\"\\nðŸ“ˆ Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred, target_names=['Normal', 'Failure']))\n",
    "    \n",
    "    if plot_results:\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "        \n",
    "        # Confusion Matrix\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        sns.heatmap(cm, annot=True, fmt='d', ax=axes[0,0], cmap='Blues')\n",
    "        axes[0,0].set_title('Confusion Matrix')\n",
    "        axes[0,0].set_xlabel('Predicted')\n",
    "        axes[0,0].set_ylabel('Actual')\n",
    "        \n",
    "        # ROC Curve\n",
    "        fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "        axes[0,1].plot(fpr, tpr, label=f'ROC Curve (AUC = {auc_score:.4f})')\n",
    "        axes[0,1].plot([0, 1], [0, 1], 'k--')\n",
    "        axes[0,1].set_xlabel('False Positive Rate')\n",
    "        axes[0,1].set_ylabel('True Positive Rate')\n",
    "        axes[0,1].set_title('ROC Curve')\n",
    "        axes[0,1].legend()\n",
    "        \n",
    "        # Prediction Distribution\n",
    "        axes[1,0].hist(y_pred_proba[y_test==0], alpha=0.7, label='Normal', bins=20)\n",
    "        axes[1,0].hist(y_pred_proba[y_test==1], alpha=0.7, label='Failure', bins=20)\n",
    "        axes[1,0].set_xlabel('Prediction Probability')\n",
    "        axes[1,0].set_ylabel('Frequency')\n",
    "        axes[1,0].set_title('Prediction Distribution')\n",
    "        axes[1,0].legend()\n",
    "        \n",
    "        # Feature Importance (simplified)\n",
    "        feature_names = data_dict['feature_names']\n",
    "        feature_importance = np.random.rand(len(feature_names))  # Placeholder\n",
    "        axes[1,1].barh(range(len(feature_names)), feature_importance)\n",
    "        axes[1,1].set_yticks(range(len(feature_names)))\n",
    "        axes[1,1].set_yticklabels(feature_names, fontsize=8)\n",
    "        axes[1,1].set_xlabel('Relative Importance')\n",
    "        axes[1,1].set_title('Feature Importance (Placeholder)')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.suptitle('Failure Detection Model Evaluation', fontsize=16, y=1.02)\n",
    "        plt.show()\n",
    "    \n",
    "    return {\n",
    "        'auc_score': auc_score,\n",
    "        'predictions': y_pred,\n",
    "        'probabilities': y_pred_proba,\n",
    "        'true_labels': y_test\n",
    "    }\n",
    "\n",
    "def predict_failures_realtime(model, scaler, new_data, sequence_length=10, feature_names=None):\n",
    "    \"\"\"\n",
    "    Use trained model to predict failures on new data\n",
    "    \n",
    "    Parameters:\n",
    "    - model: Trained RNN model\n",
    "    - scaler: Fitted StandardScaler\n",
    "    - new_data: New metric data (DataFrame or array)\n",
    "    - sequence_length: Sequence length used in training\n",
    "    - feature_names: Names of features in order\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"ðŸ”® Predicting failures on new data...\")\n",
    "    \n",
    "    # Prepare new data\n",
    "    if isinstance(new_data, pd.DataFrame):\n",
    "        # Extract values similar to training\n",
    "        exclude_cols = ['Time', 'Minutes', 'source']\n",
    "        value_cols = [col for col in new_data.columns if col not in exclude_cols]\n",
    "        new_features = new_data[value_cols].values\n",
    "    else:\n",
    "        new_features = new_data\n",
    "    \n",
    "    # Scale features\n",
    "    new_features_scaled = scaler.transform(new_features)\n",
    "    \n",
    "    # Create sequences\n",
    "    predictions = []\n",
    "    probabilities = []\n",
    "    \n",
    "    for i in range(sequence_length, len(new_features_scaled)):\n",
    "        sequence = new_features_scaled[i-sequence_length:i].reshape(1, sequence_length, -1)\n",
    "        prob = model.predict(sequence, verbose=0)[0][0]\n",
    "        pred = 1 if prob > 0.5 else 0\n",
    "        \n",
    "        predictions.append(pred)\n",
    "        probabilities.append(prob)\n",
    "    \n",
    "    return np.array(predictions), np.array(probabilities)\n",
    "\n",
    "def run_comprehensive_failure_detection(all_datasets, tuna_results=None):\n",
    "    \"\"\"\n",
    "    Run complete failure detection pipeline\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"ðŸŽ¯ COMPREHENSIVE FAILURE DETECTION SYSTEM\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Prepare dataset\n",
    "    data_dict = prepare_dataset_for_rnn(all_datasets, tuna_results)\n",
    "    \n",
    "    # Train model\n",
    "    model, history = train_failure_detection_model(data_dict)\n",
    "    \n",
    "    # Evaluate model\n",
    "    results = evaluate_failure_detection_model(model, data_dict)\n",
    "    \n",
    "    print(f\"\\nâœ… Model training complete!\")\n",
    "    print(f\"ðŸ“Š Final AUC Score: {results['auc_score']:.4f}\")\n",
    "    print(f\"ðŸŽ¯ Failure detection rate: {data_dict['metadata']['failure_rate']:.2%}\")\n",
    "    \n",
    "    return {\n",
    "        'model': model,\n",
    "        'data_dict': data_dict,\n",
    "        'results': results,\n",
    "        'history': history\n",
    "    }\n",
    "\n",
    "# Execute the failure detection pipeline\n",
    "failure_detection_system = run_comprehensive_failure_detection(all_datasets, tuna_results)\n",
    "\n",
    "# Save the model for later use\n",
    "# failure_detection_system['model'].save('failure_detection_rnn.h5')\n",
    "\n",
    "print(\"ðŸŽ‰ Failure detection system ready!\")\n",
    "print(\"ðŸ’¡ Use predict_failures_realtime() for real-time predictions on new data\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
